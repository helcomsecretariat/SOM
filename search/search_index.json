{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sufficiency of Measures (SOM)","text":"<p>Methodology developed at HELCOM. Tool available on GitHub.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>python main.py</code> - Run tool.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>src/                # Project source code.\nUI/                 # User interface source code.\npyproject.toml      # Project build requirements file.\nmkdocs.yml          # Documentation configuration file.\ndocs/               # Documentation resources.\nLICENSE             # Project license.\n</code></pre>"},{"location":"license/","title":"License","text":"<p>Please see the project license for further details.</p>"},{"location":"development/documentation/","title":"Documentation","text":"<p>The documentation has been built using Material for MkDocs as a static website, and is hosted on the GitHub Pages service.</p> <p>The doc files are located in the <code>docs/</code> directory and configured in the <code>mkdocs.yml</code> file.</p>"},{"location":"development/documentation/#editing","title":"Editing","text":"<p>To edit existing or add new pages to the documentation, navigate to the <code>docs/</code> directory.</p> <p>Each page is its own markdown file (.md), and the files have been ordered in subdirectories for a clearer distinction between sections. However, the actual structure of these pages in the documentation is set in the <code>mkdocs.yml</code> file under the <code>nav</code> keyword.</p> <p>To add a new page, create your markdown file and link to it in <code>mkdocs.yml</code>.</p> <p>To edit an existing file, simply open it and make your changes. It is recommended to familiarize yourself with the markdown format beforehand. </p>"},{"location":"development/documentation/#updating-on-github-pages","title":"Updating on GitHub Pages","text":"<p>The file <code>.github/workflows/ci.yml</code> has been setup so that upon each push to the repository, the docs pages are updated also on GitHub Pages, without further action required from the developer.</p>"},{"location":"development/documentation/#viewing-locally","title":"Viewing locally","text":"<p>To edit and view changes locally, you will need to install the required modules using pip:</p> <pre><code>pip install mkdocs-material\npip install mkdocstrings-python\npip install mkdocs-git-revision-date-localized-plugin\npip install mkdocs-git-authors-plugin\n</code></pre> <p>The docs can then be launched on a local server using </p> <pre><code>cd path/to/SOM\npython -m mkdocs serve\n</code></pre> <p>and opened in a web browser at http://127.0.0.1:8000.</p>"},{"location":"development/further-development/","title":"Further development","text":"<p>The original framework developed in the ACTION project was further expanded on in the BLUES project. The figure below highlights the additions. </p> <p></p> <p>Figure 1. Expanded SOM model, as developed in HELCOM BLUES.</p> <p>The main components added are:</p> <ul> <li>Drivers</li> <li>Pressure-Pressure and State-State interactions</li> <li>Ecosystem services and benefits</li> <li>Impacts on human well-being</li> <li>Incentives</li> </ul> <p>In order to include these in future versions of the tool, care has to be taken in order to implement them in a logical way in accordance with the already existing logic. The sections below suggest potential ways of implementing these components, given that the data on them is available. </p>"},{"location":"development/further-development/#drivers","title":"Drivers","text":"<p>The drivers have two main vectors into the existing framework:</p> <ul> <li>Driver-Pressure interaction</li> <li>Driver-State interaction</li> </ul> <p>These could be implemented as two tables, linking driver and pressure/state IDs together along with a multiplier affecting the reduction on pressure/TPL levels.</p>"},{"location":"development/further-development/#som_toolspy","title":"<code>som_tools.py</code>","text":"<p>If the data is not already as described above, add a method to handle the preprocessing of the input data:</p> <pre><code>def read_drivers(src):\n    Arguments:\n        src: source of input data\n    Returns:\n        driver_pressure_interactions (DataFrame): \n            driver (Any): driver ID\n            pressure (Any): pressure ID\n            area_id (Any): area ID, if the drivers are area specific\n            multiplier (float): multiplier applied to pressure contributions\n        driver_state_interaction (DataFrame):\n            driver (Any): driver ID\n            state (Any): state ID\n            area_id (Any): area ID, if the drivers are area specific\n            multiplier (float): multiplier applied to total pressure load reductions\n</code></pre>"},{"location":"development/further-development/#som_apppy","title":"<code>som_app.py</code>","text":"<p>Add driver-pressure interactions in <code>build_changes()</code>:</p> <pre><code># after pressure reductions section, before total pressure load reductions section\nfor each area:\n    - adjust each relevant pressure level\n    - adjust pressure contributions to reflect changes\n</code></pre> <p>Add driver-state interactions in <code>build_changes()</code>:</p> <pre><code># after total pressure load reductions section\nfor each area:\n    - adjust each relevant total pressure load level\n    - adjust each relevant state pressure level\n</code></pre>"},{"location":"development/further-development/#pressure-pressure-and-state-state-interaction","title":"Pressure-Pressure and State-State interaction","text":"<p>The pressure-pressure and state-state interactions work similarly to the measure overlaps, represented as tables linking IDs if the same type together.</p> <p>While the subpressures data reflect how a pressure makes up for a portion of another pressure, pressure-pressure interactions describe how the change in one pressure affects another pressure. </p>"},{"location":"development/further-development/#som_toolspy_1","title":"<code>som_tools.py</code>","text":"<p>If the data is not already as described above, add two methods to handle the preprocessing of the input data:</p> <pre><code>def read_pressure_overlaps(src):\n    Arguments:\n        src: source of input data\n    Returns:\n        pressure_overlaps (DataFrame): \n            overlapping (Any): overlapping pressure ID\n            overlapped (Any): overlapped pressure ID\n            state (Any): state ID\n            area_id (Any): area ID, if the overlaps are area specific\n            multiplier (float): multiplier applied to overlapped pressure\n\ndef read_state_overlaps(src):\n    Arguments:\n        src: source of input data\n    Returns:\n        state_overlaps (DataFrame): \n            overlapping (Any): overlapping state ID\n            overlapped (Any): overlapped state ID\n            area_id (Any): area ID, if the overlaps are area specific\n            multiplier (float): multiplier applied to overlapped state\n</code></pre>"},{"location":"development/further-development/#som_apppy_1","title":"<code>som_app.py</code>","text":"<p>Add interactions in <code>build_changes()</code>:</p> <pre><code># in total pressure load reductions section, in pressure contributions sub-section, after subpressures sub-sub-section\n- adjust overlapped pressure level change by relevant overlapping pressure multipliers\n\n# after total pressure load reductions section\n- adjust total pressure load level changes by relevant overlapping state multipliers\n</code></pre>"},{"location":"development/further-development/#ecosystem-services-and-benefits","title":"Ecosystem services and benefits","text":"<p>The ecosystem services share similarities with the states, and could also be implemented as a table, linking the states to the ecosystem services by ID along with a multiplier to reflect the ecosystem service levels. These levels would then be affected by the total pressure load reductions accordingly. From the ecosystem services, the benefits and their produced value could then be determined.</p> <p>However, the inclusion of these have not been completely mapped out, and as such a solution on their complete implementation is not produced here. </p> <p>Once identified, it is likely that these calculations can be added after those in <code>build_changes()</code> in <code>som_app.py</code>.</p>"},{"location":"development/further-development/#impacts-on-human-well-being","title":"Impacts on human well-being","text":"<p>The impacts on human well-being are dependent on the Value properties calculated from the Ecosystem benefits, as well as the Cost property derived from the measures. To achieve this, the measures data needs to be expanded to include the cost of each measure. Given the input data, the impacts should act as a transformation function, using the values and costs to produce the impact-incentive and impact-measure interaction data. This data could likely take the form as similar multipliers detailed previously on this page. However, as the transformation functions were not mapped out at the time of writing, a solution on their implementation is not produced here. </p>"},{"location":"development/further-development/#incentives","title":"Incentives","text":"<p>The incentives once again work as multipliers, this time on the coverage and implementation properties of the measures in the cases data, and thus requires linking of the incentives to either the cases or measures data. However, due to not being mapped out, a solution is not included here. </p>"},{"location":"development/tool/","title":"Tool","text":"<p>As outlined in the File structure page, the tool consist of a few separate sub-modules that together comprise the complete SOM module. Each sub-module targets a specific part of the tools functionality, efficiently separating tasks into distinct sections.</p>"},{"location":"development/tool/#mainpy","title":"<code>main.py</code>","text":"<p>This file manages the user interactions and calls to the core <code>som_app.py</code> module. If run directly, it will read the <code>config.toml</code> file and parse user specified arguments to change the config settings accordingly. Once done, it will run the <code>run()</code> method, which acts as the main SOM wrapper method. Here, the following occurs:</p> <ol> <li>A directory for the indivudal simulation run logs is created in the same directory as the main script.</li> <li>Output directories for the results are created.</li> <li>The input data is loaded and processed.</li> <li>The simulations are run by calling <code>run_sim()</code> for each individual simulation run. If the <code>parallell_processing</code> setting is set to <code>true</code>, the calculations will be run in parallell for faster processing. Each simulation's results are saved to a pickle file to conserve data structures.</li> <li>The results are calculated from the congregation of all simulation round results.</li> <li>The results are exported to the specified directory.</li> <li>Plots are created to visualize the results.</li> </ol>"},{"location":"development/tool/#som_apppy","title":"<code>som_app.py</code>","text":"<p>This file acts as the core module of the package, containing the main calculations of the framework. The calculations called from <code>main.run_sim()</code> follow these steps:</p> <ol> <li> <p><code>build_links()</code></p> <ol> <li>Measure reductions for the current simulation run are calculated from the input data probability distributions.</li> <li>Activity contributions for the current simulation run are calculated from the input data probability distributions.</li> <li>Pressure contributions for the current simulation run are calculated from the input data probability distributions.</li> <li>Thresholds for the current simulation run are calculated from the input data probability distributions.</li> </ol> </li> <li> <p><code>build_scenario()</code> if <code>use_scenario</code> setting is set to <code>true</code></p> <ol> <li>Activity contributions are adjusted by scenario multiplier</li> <li>Activity contributions are normalized so that for each pressure, the changes in contributions is reflected in the total sum of the contributions</li> </ol> </li> <li> <p><code>build_cases()</code></p> <ol> <li>Cases are filtered to only include measures that have a documented effect in the input data</li> <li>Cases are exploded, replacing the placeholder '0' value with all relevant activities, pressures and states</li> <li>Cases are filtered to exclude activity-pressure-state combinations without associated measure reduction</li> <li>Duplicate measure-activity-pressure-state combinations are removed, leaving only rows with the highest coverage and implementation</li> </ol> </li> <li> <p><code>build_changes()</code></p> <ol> <li>Activity contributions are normalized to make sure they do not exceed 100 %<ul> <li>Below 100 % is allowed as it is not guaranteed all contributions are known</li> </ul> </li> <li>Pressure contributions are normalized to make sure they do not exceed 100 %<ul> <li>Below 100 % is allowed as it is not guaranteed all contributions are known</li> </ul> </li> <li>Pressure level reductions<ol> <li>Measure reductions are modified by coverage and implementation multipliers</li> <li>Measure reductions are modififed by overlapping measures</li> <li>Pressure levels are adjusted by activity contributions multiplied with measure reductions</li> <li>Activity contributions are adjusted by reductions and normalized</li> </ol> </li> <li>Total pressure load reductions<ol> <li>Straight to state measures<ol> <li>Measure reductions are modified by coverage and implementation multipliers</li> <li>Measure reductions are modified by overlapping measures</li> <li>Total pressure load levels are adjusted by reductions</li> </ol> </li> <li>Pressure level reductions<ol> <li>Reduction is determined as 1 - pressure level reductions</li> <li>Reduction is adjusted by changes also in current pressure's subpressures</li> <li>Total pressure load levels are adjusted by calculated reductions</li> <li>Pressure contributions are adjusted by reductions and normalized</li> </ol> </li> </ol> </li> </ol> </li> <li> <p><code>build_results()</code></p> <p>Individual simulation run results are allocated into arrays, from which means and standard errors are calculated. </p> </li> </ol>"},{"location":"development/tool/#som_toolspy","title":"<code>som_tools.py</code>","text":"<p>This file handles the pre-processing of the legacy input data, to make sure it follows the format accepted by the methods in <code>som_app.py</code>. Each method in the file addresses one component of the input data. The accepted format of the legacy input data is detailed in Input  data (legacy).</p>"},{"location":"development/tool/#utilitiespy","title":"<code>utilities.py</code>","text":"<p>This file contains small utility methods that are used throughout the several scripts in the SOM package. The main target functions of these fall into three categories:</p> <ul> <li>exception handling methods</li> <li>progress displaying methods</li> <li>probability distribution methods</li> </ul>"},{"location":"development/tool/#som_plotspy","title":"<code>som_plots.py</code>","text":"<p>This file handles the optional output of the simulation results into plots to visualize them. The <code>build_display()</code> method makes the calls to the other methods to create each different type of plot. The following types of plots are created:</p> <ul> <li>Total pressure load levels (TPL level over state) for each case area</li> <li>Pressure levels for each case area</li> <li>Target threshold reductions compared to actual TPL reductions</li> <li>Total pressure load levels for each separate state for each case area (State pressures)</li> <li>Activity contributions for each case area</li> <li>Pressure contributions for each case area</li> <li>Measure reduction effects</li> </ul>"},{"location":"development/user-interface/","title":"User interface","text":"<p>The user interface code is separated from the main tool and located in the <code>UI</code> directory.</p> <p>The code uses electron to run a standalone app, running Node.js as its backend. This allows for both the frontend and backend to be written in JavaScript, and makes for quick development and updates. </p> <p>Most of the files UI files are located in the sub-directory <code>assets</code>.</p>"},{"location":"development/user-interface/#server-configuration","title":"Server configuration","text":"<p>The Node.js server configuration is defined in the <code>package.json</code> file, with additional electron-forge configuration defined in <code>forge.config.js</code>. </p>"},{"location":"development/user-interface/#mainjs","title":"main.js","text":"<p>Running the server with <code>npm run start</code> will launch the main.js file. This file creates the application window, as well as handles the server-side logic. The window will start by displaying <code>assets/index.html</code>. </p>"},{"location":"development/user-interface/#indexhtml","title":"index.html","text":"<p>This file acts as the main page displayed in the application, which holds the main hierarchy of elements on the page.</p>"},{"location":"development/user-interface/#rendererjs","title":"renderer.js","text":"<p>Handles the user-side logic, and creates the dynamic interface as defined in <code>parameters.json</code>. </p>"},{"location":"development/user-interface/#parametersjson","title":"parameters.json","text":"<p>Defines the parameter input options to be included in the interface and their order. </p>"},{"location":"development/user-interface/#preloadjs","title":"preload.js","text":"<p>Acts as a bridge between main.js and renderer.js, preventing exposure of server-side methods and control to the user. </p>"},{"location":"development/user-interface/#stylecss","title":"style.css","text":"<p>Style sheet for the interface elements. </p>"},{"location":"development/welcome/","title":"Welcome","text":"<p>Welcome to the SOM tool development section!</p> <p>These pages detail how to update the code or add functionalities. Knowledge of the following is highly recommended before making changes:</p> <ul> <li>Python</li> <li>web dev (only for the UI)</li> <li>git</li> </ul> <p>Filepaths and directories always start in the main <code>SOM</code> directory unless stated otherwise.</p>"},{"location":"guide/configuration/","title":"Configuration","text":"<p>The <code>config.toml</code> file can be edited in a text editor and follows the TOML format. Please note that the options are case sensitive. </p> <ul> <li> <p><code>export_path</code>: Path of the output excel file</p> </li> <li> <p><code>use_scenario</code>: Should an activity development scenario be applied? (true/false)</p> </li> <li> <p><code>scenario</code>: Which activity development scenario to use</p> </li> <li> <p><code>use_random_seed</code>: Should a custom seed be used for random values? (true/false)</p> </li> <li> <p><code>random_seed</code>: Custom random seed to use</p> </li> <li> <p><code>simulations</code>: Number of simulations to run</p> </li> <li> <p><code>use_parallel_processing</code>: Should multiprocessing be used for faster calculations? (true/false)</p> </li> <li> <p><code>use_legacy_input_data</code>: Choose between legacy/new input data (true/false), see Input data</p> </li> <li><code>input_data:path</code>: Path of the new input data excel file</li> <li><code>input_data_legacy:general_input</code>: Path to legacy input data general data excel file</li> <li><code>input_data_legacy:measure_effect_input</code>: Path to legacy input data measure effects data excel file</li> <li><code>input_data_legacy:pressure_state_input</code>: Path to legacy input data pressure-state links and thresholds data excel file</li> <li> <p><code>input_data_legacy:general_input_sheets</code>: Links to the sheets in <code>input_data_legacy:general_input</code> in case of custom naming</p> </li> <li> <p><code>link_mpas_to_subbasins</code>: Choose to link areas with measures to the areas in the input data using polygon layer overlaps (true/false)</p> </li> <li><code>layers:subbasin:url</code>: url to subbasin polygon layer</li> <li><code>layers:subbasin:path</code>: path to subbasin polygon layer, overrides url</li> <li><code>layers:subbasin:id_attr</code>: id attribute field of subbasin polygon layer, must correspond to <code>input_data:area:ID</code> values</li> <li><code>layers:mpa:url</code>: url to MPA polygon layer</li> <li><code>layers:mpa:path</code>: path to MPA polygon layer, overrides url</li> <li><code>layers:mpa:measure_attr</code>: measure attribute field, all measures of the MPA joined together by <code>layers:mpa:measure_delimiter</code></li> <li><code>layers:mpa:id_attr</code>: id attribute field of MPA polygon layer</li> <li><code>layers:mpa:name_attr</code>: name attribute field of MPA polygon layer</li> <li><code>layers:mpa:measure_delimiter</code>: delimiter used to separate measures in <code>layers:mpa:measure_attr</code></li> </ul>"},{"location":"guide/file-structure/","title":"File structure","text":"<p>Model file structure (in <code>src/</code> directory):</p> <ul> <li><code>main.py</code>: Runs the tool</li> <li><code>config.toml</code>: Configuration settings</li> <li><code>som_app.py</code>: Main calculations are performed here</li> <li><code>som_tools.py</code>: Input data loading functions</li> <li><code>som_plots.py</code>: Plot functions for results</li> <li><code>utilities.py</code>: Small utility functions</li> <li><code>api_tools.py</code>: Functions for linking areas</li> </ul>"},{"location":"guide/input-data-legacy/","title":"Input data (legacy)","text":""},{"location":"guide/input-data-legacy/#intro","title":"Intro","text":"<p>(Note! This page details the previous format of the input data, that can be used with the SOM model by pre-processing it. To do so, make sure to set the <code>use_legacy_input_data</code> option to <code>true</code> in the configuration file)</p> <p>The input data consists of three files:</p> <ul> <li>exampleData.xlsx</li> <li>exampleEffect.xlsx</li> <li>examplePressureState.xlsx</li> </ul> <p>Example data has been provided in the <code>data</code> directory.</p> <p>Please note that most column names are case sensitive.</p>"},{"location":"guide/input-data-legacy/#general-input","title":"General Input","text":"<p><code>exampleData.xlsx</code> contains descriptions of the model domain:</p>"},{"location":"guide/input-data-legacy/#id-sheets","title":"ID sheets","text":""},{"location":"guide/input-data-legacy/#sheetmeasure-id","title":"<code>sheet:Measure ID</code>","text":"<ul> <li>Unique identifiers for measures</li> </ul>"},{"location":"guide/input-data-legacy/#sheetactivity-id","title":"<code>sheet:Activity ID</code>","text":"<ul> <li>Unique identifiers for activities</li> </ul>"},{"location":"guide/input-data-legacy/#sheetpressure-id","title":"<code>sheet:Pressure ID</code>","text":"<ul> <li>Unique identifiers for pressures</li> </ul>"},{"location":"guide/input-data-legacy/#sheetstate-id","title":"<code>sheet:State ID</code>","text":"<ul> <li>Unique identifiers for states</li> </ul>"},{"location":"guide/input-data-legacy/#sheetarea-id","title":"<code>sheet:Area ID</code>","text":"<ul> <li>Unique identifiers for areas</li> </ul>"},{"location":"guide/input-data-legacy/#sheetcase-id","title":"<code>sheet:Case ID</code>","text":"<ul> <li>Unique identifiers for cases</li> </ul>"},{"location":"guide/input-data-legacy/#sheetactmeas","title":"<code>sheet:ActMeas</code>","text":"<ul> <li>Implemented measure cases, all rows are independent, multiple IDs can be joined by a semi-colon.<ul> <li><code>column:ID</code>: Unique case id, linked to <code>sheet:Case ID</code></li> <li><code>column:measure</code>: Measure type ID, linked to <code>sheet:Measure ID</code></li> <li><code>column:activity</code>: Relevant Activities, linked to <code>sheet:Activity ID</code>, the value 0 (zero) means all relevant activities affected by the measure</li> <li><code>column:pressure</code>: Relevant Pressures, linked to <code>sheet:Pressure ID</code>, the value 0 (zero) means all relevant pressures affected by the measure</li> <li><code>column:state</code>: Relevant States, linked to <code>sheet:State ID</code>, the value 0 (zero) means all relevant states affected by the measure</li> <li><code>column:coverage</code>: Multiplier (fraction), represents how much of the area is covered by the measure</li> <li><code>column:implementation</code>: Multiplier (fraction), represents how much of the measure is implemented</li> <li><code>column:area_id</code>: Area ID, linked to <code>sheet:Area ID</code></li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#sheetactpres","title":"<code>sheet:ActPres</code>","text":"<ul> <li>Activity-Pressure links, how much the individual activities contribute to the pressures<ul> <li><code>column:Activity</code>: Activity ID, linked to <code>sheet:Activity ID</code></li> <li><code>column:Pressure</code>: Pressure ID, linked to <code>sheet:Pressure ID</code></li> <li><code>column:area_id</code>: Area ID, linked to <code>sheet:Area ID</code>, multiple IDs can be joined by a semi-colon</li> <li><code>column:Ml#</code>: Most likely contribution (%)</li> <li><code>column:Min#</code>: Lowest potential contribution (%)</li> <li><code>column:Max#</code>: Highest potential contribution (%)</li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#sheetdev_scenarios","title":"<code>sheet:DEV_scenarios</code>","text":"<ul> <li>Activity development scenarios<ul> <li><code>column:Activity</code>: Activity ID, linked to <code>sheet:Activity ID</code></li> <li><code>column:###</code>: Subsequent columns are treated as the change / scenarios (fraction)</li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#sheetoverlaps","title":"<code>sheet:Overlaps</code>","text":"<ul> <li>Interaction between separate measures, how joint implementation affects measure efficiency<ul> <li><code>column:Overlap</code>: Overlap ID</li> <li><code>column:Pressure</code>: Pressure ID, linked to <code>sheet:Pressure ID</code></li> <li><code>column:Activity</code>: Activity ID, linked to <code>sheet:Activity ID</code></li> <li><code>column:Overlapping</code>: Overlapping measure ID, linked to <code>sheet:Measure ID</code></li> <li><code>column:Overlapped</code>: Overlapped measure ID, linked to <code>sheet:Measure ID</code></li> <li><code>column:Multiplier</code>: Multiplier (fraction), how much of the <code>column:Overlapped</code> measure's effect will be observed if <code>column:Overlapping</code> is also implemented</li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#sheetsubpres","title":"<code>sheet:SubPres</code>","text":"<ul> <li>Links between separate pressures, where subpressures make up part of state pressures<ul> <li><code>column:Reduced pressure</code>: Subpressure ID, linked to <code>sheet:Pressure ID</code></li> <li><code>column:State pressure</code>: State pressure ID, linked to <code>sheet:Pressure ID</code></li> <li><code>column:Equivalence</code>: Equivalence between <code>column:Reduced pressure</code> and <code>column:State pressure</code>, i.e. how much of the state pressure is made up of the subpressure, where values between 0 and 1 are treated as fractions, and other values as either no quantified equivalence or no reduction from pressures</li> <li><code>column:State</code>: State ID, linked to <code>sheet:State ID</code></li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#measure-efficiencies","title":"Measure efficiencies","text":"<p><code>exampleEffect.xslx</code> contains survey data on the effects of measures on activity-pressure pairs as surved by expert panels:</p>"},{"location":"guide/input-data-legacy/#sheetmteq","title":"<code>sheet:MTEQ</code>","text":"<ul> <li>General information on the survey questions, each row corresponds to a unique activity-pressure pair, the value 0 (zero) for the Activity, Pressure and State columns is used to denote no value, used for direct to pressure / direct to state measures<ul> <li><code>column:Survey ID</code>: Survey ID, each unique id corresponds to a specific sheet in <code>exampleEffect.xslx</code></li> <li><code>column:Activity</code>: Activity ID, linked to <code>exampleData.xlsx:Activity ID</code></li> <li><code>column:Pressure</code>: Pressure ID, linked to <code>exampleData.xlsx:Pressure ID</code></li> <li><code>column:State</code>: State ID, linked to <code>exampleData.xlsx:State ID</code></li> <li><code>column:AMT</code>: Amount of measures linked to the activity-pressure pair in the corresponding survey sheet</li> <li><code>column:Exp#</code>: Expert columns, details the number of experts that gave each answer, used for weighting</li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#sheetsurveys","title":"<code>sheet:Surveys</code>","text":"<ul> <li>Survey sheets detailing the effects of the measures on the activity-pressure pairs in <code>sheet:MTEQ</code><ul> <li><code>column:expert ID</code>: Expert ID, linked to the corresponding expert columns in <code>sheet:MTEQ</code></li> <li><code>column:#</code>: Measure IDs as columns, linked to <code>exampleData.xlsx:Measure ID</code>, each measure takes two columns<ul> <li>the first column describes the most likely reduction (%) of the measure on the activity-pressure pair</li> <li>the second column describes the potential uncertainty range (%) regarding the reduction</li> </ul> </li> <li><code>column:ME</code>: The actual effect of the most effective measure for the current activity-pressure pair</li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#pressure-contributions-and-ges-thresholds","title":"Pressure contributions and GES thresholds","text":"<p><code>examplePressureState.xlsx</code> contains survey data on pressure contributions to states and total pressure load reduction targets:</p>"},{"location":"guide/input-data-legacy/#sheetpsq","title":"<code>sheet:PSQ</code>","text":"<ul> <li>General information on the survey questions, each row corresponds to a unique state-area pair<ul> <li><code>column:State</code>: State ID, linked to <code>exampleData.xlsx:State ID</code></li> <li><code>column:area_id</code>: Area ID, linked to <code>exampleData.xlsx:Area ID</code>, multiple IDs can be joined by a semi-colon</li> <li><code>column:GES known</code>: Is the GES threshold known, 0 for no, 1 for yes</li> <li><code>column:Exp#</code>: Expert columns, details the number of experts that gave each answer, used for weighting</li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#sheetsurveys_1","title":"<code>sheet:Surveys</code>","text":"<ul> <li>Survey sheets detailing the contributions of individual pressures to states and the total pressure load reduction targets for the state, the targets are for PR (=GES), 10 %, 25 % and 50 % improvement in state<ul> <li><code>column:Expert</code>: Expert ID, linked to the corresponding expert columns in <code>sheet:PSQ</code>, each expert's answers comprise a block of rows corresponding to the state-area pair rows in <code>sheet:PSQ</code></li> <li><code>column:P#</code>: Pressure IDs, linked to <code>exampleData.xlsx:Pressure ID</code></li> <li><code>column:S#</code>: Significance of corresponding <code>column:P#</code>, used when weighing contributions of each pressure</li> <li><code>column:MIN#</code>: Lowest potential threshold value (%)</li> <li><code>column:MAX#</code>: Highest potential threshold value (%)</li> <li><code>column:ML#</code>: Most likely threshold value (%)</li> </ul> </li> </ul>"},{"location":"guide/input-data/","title":"Input data","text":""},{"location":"guide/input-data/#intro","title":"Intro","text":"<p>The new input data consists of one file:</p> <ul> <li>exampleInputData.xlsx</li> </ul> <p>Example input data has been provided in the <code>data</code> directory.</p> <p>For the previous version of the input data, see Input data (legacy).</p> <p>Please note that most column names are case sensitive.</p> <p>NOTE! Certain characters are not allowed in the name columns of the ID sheets due to OS restrictions,  and may lead to crashes or unexpected behavior. To avoid this, try to not use special characters or letters  in the names. </p>"},{"location":"guide/input-data/#probability-distributions","title":"Probability Distributions","text":"<p>When the input data contains probability distributions, they will follow this format:</p> <ul> <li>The distribution is represented as a string</li> <li>The string is enclosed by square brackets [ ]</li> <li>Entries are space-separated</li> <li>Each entry in the distribution is the probability for a random pick to be within that discrete interval within the range [0, 1], where the distance is determined by the total number of entries in the distribution, such that the first and last entries represent 0 % and 100 % and each step in between is 100 / (N - 1).</li> </ul>"},{"location":"guide/input-data/#data-structure","title":"Data Structure","text":""},{"location":"guide/input-data/#sheetmeasure","title":"<code>sheet:measure</code>","text":"<ul> <li>Unique identifiers for measures</li> </ul>"},{"location":"guide/input-data/#sheetactivity","title":"<code>sheet:activity</code>","text":"<ul> <li>Unique identifiers for activities</li> </ul>"},{"location":"guide/input-data/#sheetpressure","title":"<code>sheet:pressure</code>","text":"<ul> <li>Unique identifiers for pressures</li> </ul>"},{"location":"guide/input-data/#sheetstate","title":"<code>sheet:state</code>","text":"<ul> <li>Unique identifiers for states</li> </ul>"},{"location":"guide/input-data/#sheetarea","title":"<code>sheet:area</code>","text":"<ul> <li>Unique identifiers for areas</li> </ul>"},{"location":"guide/input-data/#sheetcases","title":"<code>sheet:cases</code>","text":"<ul> <li>Implemented measure cases, all rows are independent<ul> <li><code>column:ID</code>: Unique case id</li> <li><code>column:measure</code>: Measure type ID, linked to <code>sheet:measure</code></li> <li><code>column:activity</code>: Relevant Activities, linked to <code>sheet:activity</code>, the value 0 (zero) means all relevant activities affected by the measure</li> <li><code>column:pressure</code>: Relevant Pressures, linked to <code>sheet:pressure</code>, the value 0 (zero) means all relevant pressures affected by the measure</li> <li><code>column:state</code>: Relevant States, linked to <code>sheet:state</code>, the value 0 (zero) means all relevant states affected by the measure</li> <li><code>column:coverage</code>: Multiplier (fraction), represents how much of the area is covered by the measure</li> <li><code>column:implementation</code>: Multiplier (fraction), represents how much of the measure is implemented</li> <li><code>column:area_id</code>: Area ID, linked to <code>sheet:Area ID</code></li> </ul> </li> </ul>"},{"location":"guide/input-data/#sheetmeasure_effects","title":"<code>sheet:measure_effects</code>","text":"<ul> <li>Activity-Pressure links, how much the individual activities contribute to the pressures<ul> <li><code>column:activity</code>: Activity ID, linked to <code>sheet:activity</code></li> <li><code>column:pressure</code>: Pressure ID, linked to <code>sheet:pressure</code></li> <li><code>column:area_id</code>: Area ID, linked to <code>sheet:area</code></li> <li><code>column:contribution</code>: Measure reduction effect, probability distribution, see Probability Distributions</li> </ul> </li> </ul>"},{"location":"guide/input-data/#sheetactivity_contributions","title":"<code>sheet:activity_contributions</code>","text":"<ul> <li>Measure reduction effects on activity-pressure pairs<ul> <li><code>column:measure</code>: Activity ID, linked to <code>sheet:measure</code></li> <li><code>column:activity</code>: Activity ID, linked to <code>sheet:activity</code></li> <li><code>column:pressure</code>: Pressure ID, linked to <code>sheet:pressure</code></li> <li><code>column:state</code>: Pressure ID, linked to <code>sheet:state</code></li> <li><code>column:probability</code>: Activity contribution, probability distribution, see Probability Distributions</li> </ul> </li> </ul>"},{"location":"guide/input-data/#sheetpressure_contributions","title":"<code>sheet:pressure_contributions</code>","text":"<ul> <li>Pressure-State links, how much the individual pressures contribute to the states<ul> <li><code>column:state</code>: State ID, linked to <code>sheet:state</code></li> <li><code>column:pressure</code>: Pressure ID, linked to <code>sheet:pressure</code></li> <li><code>column:area_id</code>: Area ID, linked to <code>sheet:area</code></li> <li><code>column:contribution</code>: Pressure contribution, probability distribution, see Probability Distributions</li> </ul> </li> </ul>"},{"location":"guide/input-data/#sheetthresholds","title":"<code>sheet:thresholds</code>","text":"<ul> <li>Environmental target thresholds, how much the individual states need to be reduced to reach the set targets<ul> <li><code>column:state</code>: State ID, linked to <code>sheet:state</code></li> <li><code>column:area_id</code>: Area ID, linked to <code>sheet:area</code></li> <li><code>column:PR/10/25/50</code>: Probability distributions for GES (PR) and 10/25/50 % reduction thresholds, see Probability Distributions</li> </ul> </li> </ul>"},{"location":"guide/input-data/#sheetoverlaps","title":"<code>sheet:overlaps</code>","text":"<ul> <li>Interaction between separate measures, how joint implementation affects measure efficiency<ul> <li><code>column:overlap</code>: Overlap ID</li> <li><code>column:pressure</code>: Pressure ID, linked to <code>sheet:pressure</code></li> <li><code>column:activity</code>: Activity ID, linked to <code>sheet:activity</code></li> <li><code>column:overlapping</code>: Overlapping measure ID, linked to <code>sheet:measure</code></li> <li><code>column:overlapped</code>: Overlapped measure ID, linked to <code>sheet:measure</code></li> <li><code>column:multiplier</code>: Multiplier (fraction), how much of the <code>column:overlapped</code> measure's effect will be observed if <code>column:overlapping</code> is also implemented</li> </ul> </li> </ul>"},{"location":"guide/input-data/#sheetdevelopment_scenarios","title":"<code>sheet:development_scenarios</code>","text":"<ul> <li>Activity development scenarios, how much each activity is expected to change during various scenarios, each value is a multiplier<ul> <li><code>column:activity</code>: Activity ID, linked to <code>sheet:activity</code></li> <li><code>column:###</code>: Subsequent columns are treated as the change / scenarios (fraction)</li> </ul> </li> </ul>"},{"location":"guide/input-data/#sheetsubpressures","title":"<code>sheet:subpressures</code>","text":"<ul> <li>Links between separate pressures, where subpressures make up part of state pressures<ul> <li><code>column:reduced pressure</code>: Subpressure ID, linked to <code>sheet:pressure</code></li> <li><code>column:state pressure</code>: State pressure ID, linked to <code>sheet:pressure</code></li> <li><code>column:equivalence</code>: Equivalence between <code>column:reduced pressure</code> and <code>column:state pressure</code>, i.e. how much of the state pressure is made up of the subpressure, where values between 0 and 1 are treated as fractions, and other values as either no quantified equivalence or no reduction from pressures</li> <li><code>column:state</code>: State ID, linked to <code>sheet:state</code></li> <li><code>column:multiplier</code>: Multiplier (fraction), how much of the reduction in <code>column:reduced pressure</code> should be applied to <code>column:state pressure</code>, determined from <code>column:equivalence</code></li> </ul> </li> </ul>"},{"location":"guide/installation/","title":"Installation","text":""},{"location":"guide/installation/#download-the-model","title":"Download the model","text":"<p>The code can be downloaded from the repository (steps highlighted in red). If you are using the graphical user interface (GUI), instead download the compiled tool from releases.</p> <p> </p> <p>Once downloaded, unzip the archive into your project directory.</p>"},{"location":"guide/installation/#installing-requirements","title":"Installing requirements","text":""},{"location":"guide/installation/#python","title":"Python","text":"<p>Running the SOM tool requires Python version 3.12 or above, which can be downloaded here. Follow the installer instructions to set it up. </p>"},{"location":"guide/installation/#nodejs","title":"Node.js","text":"<p>To run the script using the GUI from the terminal, Node.js is required, which can be downloaded here. Follow the installer instructions to set it up. Alternatively, download the standalone GUI from here to skip this step.</p>"},{"location":"guide/installation/#setting-up-the-environment","title":"Setting up the environment","text":"<p>Open up a terminal and enter the following:</p> <ol> <li> <p>Navigate to your directory</p> <p><code>cd \"/path/to/SOM\"</code></p> </li> <li> <p>Create a new python environment (optional, not if using the GUI):</p> <p><code>python -m venv . source bin/activate</code></p> </li> <li> <p>Install dependencies:</p> <p><code>python -m pip install .</code></p> </li> </ol> <p>If Node.js was installed to run the GUI from the terminal, additionally install the required node modules:</p> <ol> <li> <p>Navigate to UI directory</p> <p><code>cd \"/path/to/SOM/ui\"</code></p> </li> <li> <p>Install dependencies:</p> <p><code>npm install</code></p> </li> </ol>"},{"location":"guide/installation/#running-the-tool","title":"Running the tool","text":"<p>To run the tool from the terminal:</p> <pre><code>python \"/path/to/SOM/src/main.py\"\n</code></pre> <p>To run the UI from the terminal:</p> <pre><code>cd \"/path/to/SOM/ui\"\nnpm run start\n</code></pre> <p>Alternatively, see Using the tool.</p>"},{"location":"guide/interpreting-the-results/","title":"Interpreting the results","text":"<p>The results are saved to the excel file specified in <code>config.toml</code>. The plots contain svisualizations of these results for an easier analysis. </p>"},{"location":"guide/interpreting-the-results/#sheetpressuremean","title":"<code>sheet:PressureMean</code>","text":"<ul> <li><code>column:ID</code>: Pressure ID</li> <li><code>columns:area_name...</code>: Pressure level (mean) in area relative to before applying measures (fraction)</li> </ul>"},{"location":"guide/interpreting-the-results/#sheetpressureerror","title":"<code>sheet:PressureError</code>","text":"<ul> <li><code>column:ID</code>: Pressure ID</li> <li><code>columns:area_name...</code>: Standard error to value in <code>sheet:PressureMean</code> for area</li> </ul>"},{"location":"guide/interpreting-the-results/#sheettplmean","title":"<code>sheet:TPLMean</code>","text":"<ul> <li><code>column:ID</code>:State ID</li> <li><code>columns:area_name...</code>: Total Pressure Load on state level (mean) in area relative to before applying measures (fraction)</li> </ul>"},{"location":"guide/interpreting-the-results/#sheettplerror","title":"<code>sheet:TPLError</code>","text":"<ul> <li><code>column:ID</code>: State ID</li> <li><code>columns:area_name...</code>: Standard error to value in <code>sheet:TPLMean</code> for area</li> </ul>"},{"location":"guide/interpreting-the-results/#sheettplredmean","title":"<code>sheet:TPLRedMean</code>","text":"<ul> <li><code>column:ID</code>:State ID</li> <li><code>columns:area_name...</code>: Reduction on Total Pressure Load on state (mean) in area relative to before applying measures (fraction), equals 1 - value in <code>sheet:TPLMean</code></li> </ul>"},{"location":"guide/interpreting-the-results/#sheettplrederror","title":"<code>sheet:TPLRedError</code>","text":"<ul> <li><code>column:ID</code>: State ID</li> <li><code>columns:area_name...</code>: Standard error to value in <code>sheet:TPLRedMean</code> for area</li> </ul>"},{"location":"guide/interpreting-the-results/#sheetthresholdsmean","title":"<code>sheet:ThresholdsMean</code>","text":"<ul> <li><code>column:ID</code>:State ID</li> <li><code>columns:area_name...</code>: Target reduction on Total Pressure Load on state (mean) in area relative to before applying measures (fraction)</li> </ul>"},{"location":"guide/interpreting-the-results/#sheetthresholdserror","title":"<code>sheet:ThresholdsError</code>","text":"<ul> <li><code>column:ID</code>: State ID</li> <li><code>columns:area_name...</code>: Standard error to value in <code>sheet:ThresholdsMean</code> for area</li> </ul>"},{"location":"guide/interpreting-the-results/#sheetmeasureeffectsmean","title":"<code>sheet:MeasureEffectsMean</code>","text":"<ul> <li><code>column:measure</code>: Measure ID</li> <li><code>column:activity</code>: Activity ID</li> <li><code>column:pressure</code>: Pressure ID</li> <li><code>column:state</code>: State ID</li> <li><code>columns:reduction</code>: Average measure reduction effect (fraction)</li> </ul>"},{"location":"guide/interpreting-the-results/#sheetmeasureeffectserror","title":"<code>sheet:MeasureEffectsError</code>","text":"<ul> <li><code>column:measure</code>: Measure ID</li> <li><code>column:activity</code>: Activity ID</li> <li><code>column:pressure</code>: Pressure ID</li> <li><code>column:state</code>: State ID</li> <li><code>columns:reduction</code>: Measure reduction effect standard error (fraction)</li> </ul>"},{"location":"guide/interpreting-the-results/#sheetactivitycontributionsmean","title":"<code>sheet:ActivityContributionsMean</code>","text":"<ul> <li><code>column:activity</code>: Activity ID</li> <li><code>column:pressure</code>: Pressure ID</li> <li><code>column:area_id</code>: Area ID</li> <li><code>columns:contribution</code>: Average activity contribution to pressure (fraction)</li> </ul>"},{"location":"guide/interpreting-the-results/#sheetactivitycontributionserror","title":"<code>sheet:ActivityContributionsError</code>","text":"<ul> <li><code>column:activity</code>: Activity ID</li> <li><code>column:pressure</code>: Pressure ID</li> <li><code>column:area_id</code>: Area ID</li> <li><code>columns:contribution</code>: Activity contribution to pressure standard error (fraction)</li> </ul>"},{"location":"guide/interpreting-the-results/#sheetpressurecontributionsmean","title":"<code>sheet:PressureContributionsMean</code>","text":"<ul> <li><code>column:state</code>: State ID</li> <li><code>column:pressure</code>: Pressure ID</li> <li><code>column:area_id</code>: Area ID</li> <li><code>columns:contribution</code>: Average pressure contribution to total pressure load on state (fraction)</li> </ul>"},{"location":"guide/interpreting-the-results/#sheetpressurecontributionserror","title":"<code>sheet:PressureContributionsError</code>","text":"<ul> <li><code>column:state</code>: State ID</li> <li><code>column:pressure</code>: Pressure ID</li> <li><code>column:area_id</code>: Area ID</li> <li><code>columns:contribution</code>: Pressure contribution to total pressure load on state standard error (fraction)</li> </ul>"},{"location":"guide/linking-areas/","title":"Linking areas","text":""},{"location":"guide/linking-areas/#intro","title":"Intro","text":"<p>The tool provides the possibility to link the cases of smaller areas (MPAs) to the activity/pressure contributions of larger areas (subbasins). Although this page calls the smaller areas MPAs and the larger subbasins, the feature should be applicable for any areas that follow these conditions:</p> <ul> <li>MPAs: Polygon layer. Should have data on the measures implemented.</li> <li>Subbasins: Polygon layer. The IDs of the polygons should correspond to the area IDs already in the input data.</li> </ul> <p>To activate this feature, set the <code>link_mpas_to_subbasins</code> option to <code>true</code>.</p>"},{"location":"guide/linking-areas/#mpa-layer","title":"MPA layer","text":"<p>Requires the following attributes:</p> <ul> <li>ID: unique identifier field</li> <li>Measures: string attribute separated by a delimiter</li> <li>Name: string attribute</li> </ul>"},{"location":"guide/linking-areas/#subbasin-layer","title":"Subbasin layer","text":"<p>Requires the following attributes:</p> <ul> <li>ID: unique identifier field</li> </ul>"},{"location":"guide/using-the-gui/","title":"Using the UI","text":""},{"location":"guide/using-the-gui/#preparation","title":"Preparation","text":"<p>The SOM graphical user interface can be downloaded from the releases, and only requires the Python dependencies to be installed (step 1 and 3 in Installation). Once downloaded, extract the compressed file into a chosen directory.</p>"},{"location":"guide/using-the-gui/#running-the-tool","title":"Running the tool","text":"<p>The tool can be executed by running the <code>SOM.exe</code> app. For explanations on the parameter options presented, see Configuration.</p>"},{"location":"guide/using-the-gui/#results","title":"Results","text":"<p>The results matrices are saved to the excel file set in <code>config.toml</code>. The individual simulation round results are saved to the <code>sim_res</code> folder. Plots are saved to the <code>output</code> folder. </p>"},{"location":"guide/using-the-tool/","title":"Using the tool","text":""},{"location":"guide/using-the-tool/#preparation","title":"Preparation","text":"<p>Before running the tool, make sure you have all dependencies installed (see Installation).</p> <p>If you are using the GUI, see the instructions here instead.</p> <p>Your input data should be inside the <code>data</code> directory (or alternatively, edit the <code>config.toml</code> file).</p> <p>Set the settings in the <code>config.toml</code> file to your preferences, see Configuration.</p>"},{"location":"guide/using-the-tool/#running-the-tool","title":"Running the tool","text":"<p>The tool can be executed by running the <code>SOM_Launcher.bat</code> app from the main directory. </p> <p>Alternatively, navigate to the <code>src</code> directory and run the <code>main.py</code> directly:</p> <pre><code>cd \"/path/to/SOM/src\"\npython main.py\n</code></pre>"},{"location":"guide/using-the-tool/#results","title":"Results","text":"<p>The results matrices are saved to the excel file set in <code>config.toml</code>. The individual simulation round results are saved to the <code>sim_res</code> folder. Plots are saved to the <code>output</code> folder. </p>"},{"location":"guide/welcome/","title":"Welcome","text":"<p>Welcome to the SOM tool user guide!</p> <p>Feel free to explore the various topics from the table of contents.</p>"},{"location":"modules/api_tools/","title":"<code>api_tools</code>","text":"<p>Methods for linking the data from one polygon layer to the SOM input data of another polygon layer.</p>"},{"location":"modules/api_tools/#src.api_tools.link_areas","title":"<code>link_areas(config, data)</code>","text":"<p>Links MPAs to subbasin data</p> Source code in <code>src/api_tools.py</code> <pre><code>def link_areas(config: dict, data: dict[str, pd.DataFrame]):\n    \"\"\"\n    Links MPAs to subbasin data\n    \"\"\"\n    mpa_id = config['layers']['mpa']['id_attr']\n    subbasin_id = config['layers']['subbasin']['id_attr']\n\n    #\n    # Get subbasin-country combinations\n    #\n\n    if 'path' in config['layers']['subbasin'] and config['layers']['subbasin']['path'] != \"\":\n        path = config['layers']['subbasin']['path']\n        if not os.path.isfile(path): path = os.path.join(os.path.dirname(os.path.realpath(__file__)), path)\n        if not os.path.exists(path): path = config['layers']['subbasin']['url']\n    else:\n        path = config['layers']['subbasin']['url']\n\n    # read from path\n    subbasins = gpd.read_file(path)\n\n    # fix geometries if needed\n    subbasins['geometry'] = subbasins.geometry.make_valid()\n\n    #\n    # Get areas from MPA layer\n    #\n\n    if 'path' in config['layers']['mpa'] and config['layers']['mpa']['path'] != \"\":\n        path = config['layers']['mpa']['path']\n        if not os.path.isfile(path): path = os.path.join(os.path.dirname(os.path.realpath(__file__)), path)\n        if not os.path.exists(path): path = config['layers']['mpa']['url']\n    else:\n        path = config['layers']['mpa']['url']\n\n    # read from path\n    mpa = gpd.read_file(path)\n\n    # fix geometries if needed\n    mpa['geometry'] = mpa.geometry.make_valid()\n\n    #\n    # Get measures in MPAs\n    #\n\n    # explode so there's only one measure per row\n    mpa[config['layers']['mpa']['measure_attr']] = mpa[config['layers']['mpa']['measure_attr']].apply(lambda x: x.split(config['layers']['mpa']['measure_delimiter']))\n    measures = mpa.explode(column=config['layers']['mpa']['measure_attr'])\n\n    #\n    # identify links between mpas and subbasins\n    #\n\n    # ensure both layers have the same crs\n    mpa = mpa.to_crs(subbasins.crs)\n\n    mpa = mpa.reset_index(drop=True)\n    subbasins = subbasins.reset_index(drop=True)\n\n    # perform spatial join on intersections\n    intersects = gpd.sjoin(\n        mpa[[mpa_id, 'geometry']], \n        subbasins[[subbasin_id, 'geometry']], \n        how='left', \n        predicate='intersects'\n    )\n    # remove rows without intersect\n    intersects = intersects.dropna(subset=[subbasin_id])\n    # create new columns for mpa and subbasin geometries\n    merged = intersects.merge(\n        mpa[[mpa_id, 'geometry']].rename(columns={'geometry': 'geom_mpa'}), \n        on=mpa_id\n    ).merge(\n        subbasins[[subbasin_id, 'geometry']].rename(columns={'geometry': 'geom_subbasins'}), \n        on=subbasin_id\n    )\n    # calculate intersect area between mpa and subbasin for each intersection (row)\n    merged['intersect_area'] = merged.apply(\n        lambda x: x['geom_mpa'].intersection(x['geom_subbasins']).area, axis=1\n    )\n    # find subbasin with the highest intersect area for each mpa\n    max_idx = (merged.loc[merged.groupby(mpa_id)['intersect_area'].idxmax()][[mpa_id, subbasin_id]].set_index(mpa_id))\n    # map the highest intersect area subbasin to the mpa geodataframe\n    mpa[subbasin_id] = mpa[mpa_id].map(max_idx[subbasin_id])\n    links = mpa.loc[:, [mpa_id, subbasin_id]]\n\n    #\n    # change area ids to match MPAs\n    #\n\n    # create the cases dataframe for the input data\n    cases = {\n        'ID': np.arange(len(measures)), \n        'measure': measures[config['layers']['mpa']['measure_attr']], \n        'activity': np.zeros(len(measures)), \n        'pressure': np.zeros(len(measures)), \n        'state': np.zeros(len(measures)), \n        'coverage': np.ones(len(measures)), \n        'implementation': np.ones(len(measures)), \n        'area_id': measures[config['layers']['mpa']['id_attr']]\n    }\n\n    data['cases'] = pd.DataFrame(cases)\n\n    # create the area dataframe\n    areas = pd.DataFrame({\n        'ID': mpa[mpa_id].unique()\n    })\n    areas['area'] = areas['ID'].apply(lambda x: mpa.loc[(mpa[mpa_id] == x), config['layers']['mpa']['name_attr']].values[0])\n    data['area'] = areas\n\n    for key in ['activity_contributions', 'pressure_contributions', 'thresholds']:\n        df = data[key]\n        df = df.rename(columns={'area_id': subbasin_id})\n        merged = df.merge(links, on=subbasin_id, how='inner')\n        merged = merged.rename(columns={mpa_id: 'area_id'})\n        merged = merged.drop(columns=[subbasin_id])\n        data[key] = merged\n\n    return data\n</code></pre>"},{"location":"modules/main/","title":"<code>main</code>","text":"<p>Main script calling methods to do SOM calculations.</p>"},{"location":"modules/main/#src.main.read_config","title":"<code>read_config(config_file='config.toml')</code>","text":"<p>Reads configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>path to configuration file, defaults to 'config.toml'.</p> <code>'config.toml'</code> <p>Returns:</p> Name Type Description <code>config</code> <code>dict</code> <p>configuration settings.</p> Source code in <code>src/main.py</code> <pre><code>def read_config(config_file: str = 'config.toml'):\n    \"\"\"\n    Reads configuration file.\n\n    Arguments:\n        config_file (str): path to configuration file, defaults to 'config.toml'.\n\n    Returns:\n        config (dict): configuration settings.\n    \"\"\"\n    print('Reading configuration...')\n    try:\n        if not os.path.isfile(config_file): config_file = os.path.join(os.path.dirname(os.path.realpath(__file__)), config_file)\n        with open(config_file, 'r') as f:\n            config = toml.load(f)\n    except Exception as e:\n        fail_with_message('ERROR! Could not load config file!', e)\n    return config\n</code></pre>"},{"location":"modules/main/#src.main.run","title":"<code>run(config, skip_sim=False)</code>","text":"<p>Main function that loads input data and user configuration,  runs simulations and processes results.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>configuration settings.</p> required <code>skip_sim</code> <code>bool</code> <p>toggle to skip SOM calculations and only process results.</p> <code>False</code> Source code in <code>src/main.py</code> <pre><code>def run(config: dict, skip_sim: bool = False):\n    \"\"\"\n    Main function that loads input data and user configuration, \n    runs simulations and processes results.\n\n    Arguments:\n        config (dict): configuration settings.\n        skip_sim (bool): toggle to skip SOM calculations and only process results.\n    \"\"\"\n    # create log directory\n    # NOTE! Existing logs are not deleted before new runs, only overwritten\n    log_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'log')\n    if os.path.exists(log_dir): shutil.rmtree(log_dir)\n    os.makedirs(log_dir, exist_ok=True)\n\n    #\n    # setup paths\n    #\n\n    # main result directory\n    export_path = os.path.realpath(config['export_path'])\n    if not os.path.isdir(os.path.dirname(export_path)): export_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), config['export_path'])\n    # individual simulation results directory\n    sim_res_dir = os.path.join(os.path.dirname(export_path), 'sim_res')\n    if os.path.exists(sim_res_dir):\n        for f in [x for x in os.listdir(sim_res_dir) if x.endswith('.xlsx') or x.endswith('.pickle') and 'sim_res' in x]:\n            if not skip_sim:\n                os.remove(os.path.join(sim_res_dir, f))\n    os.makedirs(sim_res_dir, exist_ok=True)\n    # plot directory\n    out_dir = os.path.join(os.path.dirname(export_path), 'output')\n    if os.path.exists(out_dir): shutil.rmtree(out_dir)\n    os.makedirs(out_dir, exist_ok=True)\n\n    #\n    # run simulations\n    #\n\n    # controlled randomness\n    if config['use_random_seed']:\n        print(f'Using random seed: {config[\"random_seed\"]}')\n        np.random.seed(config['random_seed'])\n\n    # process survey data and read general input\n    print('Loading input data...')\n    try:\n        input_data = som_app.build_input(config)\n    except Exception as e:\n        fail_with_message(f'ERROR! Something went wrong while processing input data! Check traceback.', e)\n\n    # run simulations and do calculations\n    print('Running simulations...')\n    if not skip_sim:\n        cpu_count = multiprocessing.cpu_count()     # available cpu cores\n        with multiprocessing.Manager() as manager:\n            progress = manager.Namespace()\n            progress.current = 0\n            progress.total = config['simulations']\n            lock = manager.Lock()\n            display_progress(progress.current / progress.total, text='\\tProgress: ')\n            if config['use_parallel_processing']:   # parallell processing for faster computations\n                with multiprocessing.Pool(processes=(min(cpu_count - 2, config['simulations']))) as pool:\n                    jobs = [(i, input_data, config, os.path.join(sim_res_dir, f'sim_res_{i}.xlsx'), os.path.join(log_dir, f'log_{i}.txt'), progress, lock) for i in range(config['simulations'])]\n                    pool.starmap(run_sim, jobs)\n            else:   # single core solution\n                for i in range(config['simulations']):\n                    run_sim(i, input_data, config, os.path.join(sim_res_dir, f'sim_res_{i}.xlsx'), os.path.join(log_dir, f'log_{i}.txt'), progress, lock)\n            display_progress(progress.current / progress.total, text='\\tProgress: ')\n\n    #\n    # process results\n    #\n\n    print('\\nProcessing results...')\n    try:\n        print('\\tCalculating means and errors...')\n        res = som_app.build_results(sim_res_dir, input_data)    # get condensed results from the individual simulation runs\n        print('\\tExporting results to excel...')\n        som_app.export_results_to_excel(res, input_data, export_path)   # write results to file for human reading\n        if config['create_plots']:\n            print('\\tProducing plots...')\n            som_plots.build_display(res, input_data, out_dir, config['use_parallel_processing'], config['filter'])   # plots various results for visual interpretation\n    except Exception as e:\n        fail_with_message(f'ERROR! Something went wrong while processing results! Check traceback.', e)\n\n    return\n</code></pre>"},{"location":"modules/main/#src.main.run_sim","title":"<code>run_sim(id, input_data, config, out_path, log_path, progress, lock)</code>","text":"<p>Runs a single simulation round.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>Simulation round identifier.</p> required <code>input_data</code> <code>dict[str, DataFrame]</code> <p>Input data used for calculations.</p> required <code>config</code> <code>dict</code> <p>User configuration settings.</p> required <code>out_path</code> <code>str</code> <p>Output path for results.</p> required <code>log_path</code> <code>str</code> <p>Output path for log.</p> required <code>progress</code> <code>Namespace</code> <p>multiprocessing.Manager.Namespace:</p> <ul> <li>current (int): Current amount of finished simulations.</li> <li>total (int): Total amount of simulations to calculate.</li> </ul> required <code>lock</code> <code>Lock</code> <p>multiprocessing.Manager.Lock, used to manage concurrent processes updating progress.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>int</code> <p>0 (failure) | 1 (success)</p> Source code in <code>src/main.py</code> <pre><code>def run_sim(id: int, input_data: dict[str, pd.DataFrame], config: dict, out_path: str, log_path: str, progress, lock):\n    \"\"\"\n    Runs a single simulation round.\n\n    Arguments:\n        id (int): Simulation round identifier.\n        input_data (dict[str, DataFrame]): Input data used for calculations.\n        config (dict): User configuration settings.\n        out_path (str): Output path for results.\n        log_path (str): Output path for log.\n        progress (Namespace): multiprocessing.Manager.Namespace:\n\n            - current (int): Current amount of finished simulations.\n            - total (int): Total amount of simulations to calculate.\n\n        lock (Lock): multiprocessing.Manager.Lock, used to manage concurrent processes updating progress.\n\n    Returns:\n        out (int): 0 (failure) | 1 (success)\n    \"\"\"\n    log = open(log_path, 'w')\n\n    try:\n\n        print(f'sim = {id}', file=log)\n\n        # Create links between core components\n        print('\\tBuilding links between Measures, Activities, Pressures and States...', file=log)\n        data = som_app.build_links(copy.deepcopy(input_data))\n\n        if config['use_scenario']:\n            # Update activity contributions to scenario values\n            print('\\tApplying activity development scenario...', file=log)\n            data['activity_contributions'] = som_app.build_scenario(data, config['scenario'])\n\n        # Create cases\n        print('\\tBuilding cases...', file=log)\n        data = som_app.build_cases(data)\n\n        # Run model\n        print('\\tCalculating changes in environment...', file=log)\n        data = som_app.build_changes(data)\n\n        #\n        # export results\n        #\n        print('\\tExporting results...', file=log)\n        # export to pickle\n        with open(out_path.replace('xlsx', 'pickle'), 'wb') as f:\n            pickle.dump(data, f)\n\n        with lock:\n            progress.current += 1\n            display_progress(progress.current / progress.total, text='\\tProgress: ')\n\n    except Exception as e:\n        fail_with_message(f'ERROR! Something went wrong during simulation! Check traceback.', e, file=log, do_not_exit=True)\n        log.close()\n        return 0\n\n    log.close()\n    return 1\n</code></pre>"},{"location":"modules/som_app/","title":"<code>som_app</code>","text":"<p>Main SOM calculation methods.</p>"},{"location":"modules/som_app/#src.som_app.build_cases","title":"<code>build_cases(data)</code>","text":"<p>Builds cases.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>dict of dataframes containing all links and ids relevant to SOM calculations.</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>dict</code> <p>updated links and ids relevant to SOM calculations.</p> Source code in <code>src/som_app.py</code> <pre><code>def build_cases(data: dict[str, pd.DataFrame]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Builds cases.\n\n    Arguments:\n        data (dict): dict of dataframes containing all links and ids relevant to SOM calculations.\n\n    Returns:\n        data (dict): updated links and ids relevant to SOM calculations.\n    \"\"\"\n    cases = data['cases']\n    links = data['measure_effects']\n    # replace all zeros (0) in activity / pressure / state columns with full list of values\n    # filter those lists to only include relevant IDs (from links)\n    # finally explode to only have single IDs per row\n    cols = ['activity', 'pressure', 'state']\n    for col in cols:\n        cases[col] = cases[col].astype(object)\n    for i, row in cases.iterrows():\n        maps_links = links.loc[links['measure'] == row['measure'], cols]    # select relevant measure/activity/pressure/state links\n        if len(maps_links) == 0:\n            cases.drop(i, inplace=True) # drop rows where measure has no effect\n            continue\n        for col in cols:\n            cases.at[i, col] = maps_links[col].unique().tolist() if row[col] == 0 else row[col]\n    for col in cols:\n        cases = cases.explode(col)\n\n    cases = cases.reset_index(drop=True)\n\n    # filter out links that don't have associated reduction\n    m = cases['measure'].isin(links['measure'])\n    a = cases['activity'].isin(links['activity'])\n    p = cases['pressure'].isin(links['pressure'])\n    s = cases['state'].isin(links['state'])\n    existing_links = (m &amp; a &amp; p &amp; s)\n    cases = cases.loc[existing_links, :]\n\n    cases = cases.reset_index(drop=True)\n\n    # remove duplicate measures in areas, measure with highest coverage and implementation is chosen\n    cases = cases.sort_values(by=['coverage', 'implementation'], ascending=[False, False])\n    cases = cases.drop_duplicates(subset=['measure', 'activity', 'pressure', 'state', 'area_id'], keep='first')\n    cases = cases.reset_index(drop=True)\n\n    data['cases'] = cases\n\n    return data\n</code></pre>"},{"location":"modules/som_app/#src.som_app.build_changes","title":"<code>build_changes(data, time_steps=1, warnings=False)</code>","text":"<p>Main calculation method. Simulate the reduction in activities and pressures caused by measures and  return the change observed in state. </p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>dict of dataframes containing all links and ids relevant to SOM calculations.</p> required <code>time_steps</code> <code>int</code> <p>NOT IMPLEMENTED. amount of time steps to simulate, i.e. applications of measures several times.</p> <code>1</code> <code>warnings</code> <code>bool</code> <p>toggle for showing warnings related to calculations.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>data</code> <code>dict</code> <p>updated links and ids relevant to SOM calculations.</p> Source code in <code>src/som_app.py</code> <pre><code>def build_changes(data: dict[str, pd.DataFrame], time_steps: int = 1, warnings: bool = False) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Main calculation method. Simulate the reduction in activities and pressures caused by measures and \n    return the change observed in state. \n\n    Arguments:\n        data (dict): dict of dataframes containing all links and ids relevant to SOM calculations.\n        time_steps (int): NOT IMPLEMENTED. amount of time steps to simulate, i.e. applications of measures several times.\n        warnings (bool): toggle for showing warnings related to calculations.\n\n    Returns:\n        data (dict): updated links and ids relevant to SOM calculations.\n    \"\"\"\n    # this variable is used in assertions where float number error might affect comparisons\n    allowed_error = 0.00001     \n\n    cases = data['cases']\n    links = data['measure_effects']\n    areas = data['area']['ID']\n\n    # create dataframes to store changes in pressure and state, one column per area_id\n    # NOTE: the DataFrames are created on one line to avoid PerformanceWarning\n\n    # represents the amount of the pressure ('ID' column) that is left\n    # 1 = unchanged pressure, 0 = no pressure left\n    pressure_levels = pd.DataFrame(data['pressure']['ID']).reindex(columns=['ID']+areas.tolist()).fillna(1.0)\n    # represents the amount of the total pressure load that is left affecting the given state ('ID' column)\n    # 1 = unchanged pressure load, 0 = no pressure load left affecting the state\n    total_pressure_load_levels = pd.DataFrame(data['state']['ID']).reindex(columns=['ID']+areas.tolist()).fillna(1.0)\n\n    # represents the reduction observed in the total pressure load ('ID' column)\n    total_pressure_load_reductions = pd.DataFrame(data['state']['ID']).reindex(columns=['ID']+areas.tolist()).fillna(0.0)\n\n    # same as pressure_levels, but one dataframe for each separate state, so that state specific reductions on the pressures are captured\n    state_pressure_levels = {s: pd.DataFrame(data['pressure']['ID']).reindex(columns=['ID']+areas.tolist()).fillna(1.0) for s in data['state']['ID']}\n\n    # make sure activity contributions don't exceed 100 %\n    for area in areas:\n        for p_i, p in pressure_levels.iterrows():\n            mask = (data['activity_contributions']['area_id'] == area) &amp; (data['activity_contributions']['pressure'] == p['ID'])\n            relevant_contributions = data['activity_contributions'].loc[mask, :]\n            if len(relevant_contributions) &gt; 0:\n                contribution_sum = relevant_contributions['contribution'].sum()\n                if contribution_sum &gt; 1:\n                    data['activity_contributions'].loc[mask, 'contribution'] = relevant_contributions['contribution'] / contribution_sum\n            try: assert data['activity_contributions'].loc[mask, 'contribution'].sum() &lt;= 1 + allowed_error\n            except Exception as e: fail_with_message(f'Failed to verify that activity contributions do not exceed 100 % for area {area}, pressure {p[\"ID\"]} with contribution sum {data['activity_contributions'].loc[mask, 'contribution'].sum()}', e)\n\n    # make sure pressure contributions don't exceed 100 %\n    for area in areas:\n        for s_i, s in total_pressure_load_levels.iterrows():\n            mask = (data['pressure_contributions']['area_id'] == area) &amp; (data['pressure_contributions']['state'] == s['ID'])\n            relevant_contributions = data['pressure_contributions'].loc[mask, :]\n            if len(relevant_contributions) &gt; 0:\n                contribution_sum = relevant_contributions['contribution'].sum()\n                if contribution_sum &gt; 1:\n                    data['pressure_contributions'].loc[mask, 'contribution'] = relevant_contributions['contribution'] / contribution_sum\n            try: assert data['pressure_contributions'].loc[mask, 'contribution'].sum() &lt;= 1 + allowed_error\n            except Exception as e: fail_with_message(f'Failed to verify that pressure contributions do not exceed 100 % for area {area}, state {s[\"ID\"]} with contribution sum {data['pressure_contributions'].loc[mask, 'contribution'].sum()}', e)\n\n    #\n    # simulation loop\n    #\n\n    for time_step in range(time_steps):\n\n        #\n        # pressure reductions\n        #\n\n        # activity contributions\n        for area in areas:\n            c = cases.loc[cases['area_id'] == area, :]  # select cases for current area\n            for p_i, p in pressure_levels.iterrows():\n                relevant_measures = c.loc[c['pressure'] == p['ID'], :]  # select all measures affecting the current pressure in the current area\n                relevant_overlaps = data['overlaps'].loc[data['overlaps']['pressure'] == p['ID'], :]    # select all overlaps affecting current pressure\n                for m_i, m in relevant_measures.iterrows():\n                    #\n                    # get measure effect (= reduction), and apply modifiers\n                    #\n                    mask = (links['measure'] == m['measure']) &amp; (links['activity'] == m['activity']) &amp; (links['pressure'] == m['pressure']) &amp; (links['state'] == m['state'])\n                    row = links.loc[mask, :]    # find the reduction of the current measure implementation\n                    if len(row) == 0:\n                        if warnings: print(f'WARNING! Effect of measure {m[\"measure\"]} on activity {m[\"activity\"]} and pressure {m[\"pressure\"]} not known! Measure {m[\"measure\"]} will be skipped in area {area}.')\n                        continue    # skip measure if data on the effect is not known\n                    try: assert len(row) == 1\n                    except Exception as e: fail_with_message(f'ERROR! Multiple instances of measure {m[\"measure\"]} effect on activity {m[\"activity\"]} and pressure {m[\"pressure\"]} given in input data!', e)\n                    reduction = row['reduction'].values[0]\n                    for mod in ['coverage', 'implementation']:\n                        reduction = reduction * m[mod]\n                    #\n                    # overlaps (measure-measure interaction)\n                    #\n                    for o_i, o in relevant_overlaps.loc[(relevant_overlaps['overlapped'] == m['measure']) &amp; (relevant_overlaps['activity'] == m['activity']), :].iterrows():\n                        if o['overlapping'] in relevant_measures.loc[relevant_measures['activity'] == m['activity'], 'measure'].values: # ensure the overlapping measure is also for the current activity\n                            reduction = reduction * o['multiplier']\n                    #\n                    # contribution\n                    #\n                    if m['activity'] == 0:\n                        contribution = 1    # if activity is 0 (= straight to pressure), contribution will be 1\n                    else:\n                        cont_mask = (data['activity_contributions']['activity'] == m['activity']) &amp; (data['activity_contributions']['pressure'] == m['pressure']) &amp; (data['activity_contributions']['area_id'] == area)\n                        contribution = data['activity_contributions'].loc[cont_mask, 'contribution']\n                        if len(contribution) == 0:\n                            if warnings: print(f'WARNING! Contribution of activity {m[\"activity\"]} to pressure {m[\"pressure\"]} not known! Measure {m[\"measure\"]} will be skipped in area {area}.')\n                            continue    # skip measure if activity is not in contribution list\n                        else:\n                            try: assert len(contribution) == 1\n                            except Exception as e: fail_with_message(f'ERROR! Multiple instances of activity {m[\"activity\"]} contribution on pressure {m[\"pressure\"]} given in input data!', e)\n                            contribution = contribution.values[0]\n                    #\n                    # reduce pressure\n                    #\n                    pressure_levels.at[p_i, area] = pressure_levels.at[p_i, area] * (1 - reduction * contribution)\n                    if pressure_levels.at[p_i, area] &lt; 0:\n                        print(f'area {area}, pressure {p[\"ID\"]} =&gt; level = {pressure_levels.at[p_i, area]}, red = {reduction}, cont = {contribution}')\n                    #\n                    # normalize activity contributions to reflect pressure reduction\n                    #\n                    if abs(1 - contribution) &gt; allowed_error and contribution != 0:     # only normalize if there is change in contributions\n                        data['activity_contributions'].loc[cont_mask, 'contribution'] = contribution * (1 - reduction)   # reduce the current contribution before normalizing\n                        norm_mask = (data['activity_contributions']['area_id'] == area) &amp; (data['activity_contributions']['pressure'] == p['ID'])\n                        relevant_contributions = data['activity_contributions'].loc[norm_mask, 'contribution']\n                        data['activity_contributions'].loc[norm_mask, 'contribution'] = relevant_contributions / (1 - reduction * contribution)\n\n        #\n        # total pressure load reductions\n        #\n\n        # straight to state measures\n        for area in areas:\n            c = cases.loc[cases['area_id'] == area, :]  # select cases for current area\n            for s_i, s in total_pressure_load_levels.iterrows():\n                relevant_measures = c.loc[c['state'] == s['ID'], :] # select all measures affecting current state in current the area\n                for m_i, m in relevant_measures.iterrows():\n                    #\n                    # get measure effect (= reduction), and apply modifiers\n                    #\n                    mask = (links['measure'] == m['measure']) &amp; (links['activity'] == m['activity']) &amp; (links['pressure'] == m['pressure']) &amp; (links['state'] == m['state'])\n                    row = links.loc[mask, :]\n                    if len(row) == 0:\n                        continue\n                    reduction = row['reduction'].values[0]\n                    for mod in ['coverage', 'implementation']:\n                        reduction = reduction * m[mod]\n                    #\n                    # overlaps (measure-measure interaction)\n                    #\n                    for o_i, o in data['overlaps'].loc[(data['overlaps']['overlapped'] == m['measure']) &amp; (data['overlaps']['activity'] == m['activity']) &amp; (data['overlaps']['pressure'] == m['pressure']), :].iterrows():\n                        if o['overlapping'] in relevant_measures['measure'].values:\n                            reduction = reduction * o['multiplier']\n                    #\n                    # reduce pressure\n                    #\n                    total_pressure_load_levels.at[s_i, area] = total_pressure_load_levels.at[s_i, area] * (1 - reduction)\n\n        # update state pressures from pressure levels\n        for s_i, s in total_pressure_load_levels.iterrows():\n            state_pressure_levels[s['ID']].loc[:, :] = pressure_levels.loc[:, :]\n\n        # pressure contributions\n        for area in areas:\n            for s_i, s in total_pressure_load_levels.iterrows():    # for each state\n                a_i = pressure_levels.columns.get_loc(area)     # column index of current area column\n                relevant_pressures = data['pressure_contributions'].loc[(data['pressure_contributions']['area_id'] == area) &amp; (data['pressure_contributions']['state'] == s['ID']), :]  # select contributions of pressures affecting current state in current area\n                for p_i, p in relevant_pressures.iterrows():\n                    #\n                    # main pressure reduction\n                    #\n                    row_i = pressure_levels.loc[pressure_levels['ID'] == p['pressure']].index[0]\n                    reduction = 1 - pressure_levels.iloc[row_i, a_i]    # reduction = 100 % - the part that is left of the pressure\n                    contribution = data['pressure_contributions'].loc[(data['pressure_contributions']['area_id'] == area) &amp; (data['pressure_contributions']['state'] == s['ID']) &amp; (data['pressure_contributions']['pressure'] == p['pressure']), 'contribution'].values[0]\n                    #\n                    # subpressures\n                    #\n                    relevant_subpressures = data['subpressures'].loc[(data['subpressures']['state'] == s['ID']) &amp; (data['subpressures']['state pressure'] == p['pressure']), :]     # find all rows where the current pressure acts as a state pressure for the current state\n                    for sp_i, sp in relevant_subpressures.iterrows():   # for each subpressure of the current pressure\n                        sp_row_i = pressure_levels.loc[pressure_levels['ID'] == sp['reduced pressure']].index[0]\n                        multiplier = sp['multiplier']   # by how much does the subpressure affect the current pressure\n                        red = 1 - pressure_levels.iloc[sp_row_i, a_i]    # subpressure reduction = 100 % - the part that is left of the subpressure\n                        reduction = reduction + multiplier * red    # the new current pressure reduction is increased by the calculated subpressure reduction\n                    try: assert reduction &lt;= 1 + allowed_error\n                    except Exception as e: fail_with_message(f'Failed on area {area}, state {s[\"ID\"]}, pressure {p[\"pressure\"]} with reduction {reduction}', e)\n                    state_pressure_levels[s['ID']].iloc[row_i, a_i] = state_pressure_levels[s['ID']].iloc[row_i, a_i] * (1 - reduction)\n                    #\n                    # reduce total pressure load\n                    #\n                    total_pressure_load_levels.at[s_i, area] = total_pressure_load_levels.at[s_i, area] * (1 - reduction * contribution)\n                    #\n                    # normalize pressure contributions to reflect pressure reduction\n                    #\n                    if abs(1 - contribution) &gt; allowed_error and contribution != 0:     # only normalize if there is change in contributions\n                        data['pressure_contributions'].loc[(data['pressure_contributions']['area_id'] == area) &amp; (data['pressure_contributions']['state'] == s['ID']) &amp; (data['pressure_contributions']['pressure'] == p['pressure']), 'contribution'] = contribution * (1 - reduction)   # reduce the current contribution before normalizing\n                        norm_mask = (data['pressure_contributions']['area_id'] == area) &amp; (data['pressure_contributions']['state'] == s['ID'])\n                        relevant_contributions = data['pressure_contributions'].loc[norm_mask, 'contribution']\n                        data['pressure_contributions'].loc[norm_mask, 'contribution'] = relevant_contributions / (1 - reduction * contribution)\n                        try: assert abs(1 - data['pressure_contributions'].loc[norm_mask, 'contribution'].sum()) &lt;= allowed_error\n                        except Exception as e: fail_with_message(f'Failed on area {area}, state {s[\"ID\"]}, pressure {p[\"pressure\"]} with pressure contribution sum not equal to 1', e)\n\n    # total reduction observed in total pressure loads\n    for area in areas:\n        for s_i, s in total_pressure_load_levels.iterrows():\n            total_pressure_load_reductions.at[s_i, area] = 1 - total_pressure_load_levels.at[s_i, area]\n\n    # GES thresholds\n    cols = ['PR', '10', '25', '50']\n    thresholds = {}\n    for col in cols:\n        thresholds[col] = pd.DataFrame(data['state']['ID']).reindex(columns=['ID']+areas.tolist())\n    for area in areas:\n        a_i = total_pressure_load_levels.columns.get_loc(area)\n        for s_i, s in total_pressure_load_levels.iterrows():\n            row = data['thresholds'].loc[(data['thresholds']['state'] == s['ID']) &amp; (data['thresholds']['area_id'] == area), cols]\n            if len(row) == 0:\n                continue\n            for col in cols:\n                thresholds[col].iloc[s_i, a_i] = row.loc[:, col].values[0]\n\n    data.update({\n        'pressure_levels': pressure_levels, \n        'state_pressure_levels': state_pressure_levels, \n        'total_pressure_load_levels': total_pressure_load_levels, \n        'total_pressure_load_reductions': total_pressure_load_reductions, \n        'thresholds': thresholds\n    })\n\n    return data\n</code></pre>"},{"location":"modules/som_app/#src.som_app.build_input","title":"<code>build_input(config)</code>","text":"<p>Loads input data. If loading already processed data, probability distributions need to be converted back to arrays. </p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>configuration settings.</p> required <p>Returns:</p> Name Type Description <code>input_data</code> <code>dict</code> <p>SOM input data.</p> Source code in <code>src/som_app.py</code> <pre><code>def build_input(config: dict) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Loads input data. If loading already processed data, probability distributions need to be converted back to arrays. \n\n    Arguments:\n        config (dict): configuration settings.\n\n    Returns:\n        input_data (dict): SOM input data.\n    \"\"\"\n    # process legacy input data to be usable by the tool\n    if config['use_legacy_input_data']:\n        # process input data\n        input_data = process_input_data(config)\n\n        # load areas from layers and adjust area ids\n        if config['link_mpas_to_subbasins']:\n            print('Linking areas in input data...')\n            input_data = api_tools.link_areas(config, input_data)\n\n        # export input data to excel\n        path = os.path.realpath(config['input_data_legacy']['general_input'])\n        if not os.path.isfile(path): path = os.path.join(os.path.dirname(os.path.realpath(__file__)), config['input_data_legacy']['general_input'])\n        path = os.path.join(os.path.dirname(path), 'input_data.xlsx')\n        config['input_data']['path'] = path\n        with pd.ExcelWriter(path) as writer:\n            for key in input_data:\n                input_data[key].to_excel(writer, sheet_name=key, index=False)\n\n    # load processed input data used by the tool\n    path = os.path.realpath(config['input_data']['path'])\n    if not os.path.isfile(path): path = os.path.join(os.path.dirname(os.path.realpath(__file__)), config['input_data']['path'])\n    input_data = pd.read_excel(io=path, sheet_name=None)\n    conversion_sheet = [\n        ('measure_effects', 'reduction'), \n        ('activity_contributions', 'contribution'), \n        ('pressure_contributions', 'contribution'), \n        ('thresholds', 'PR'), \n        ('thresholds', '10'), \n        ('thresholds', '25'), \n        ('thresholds', '50')\n    ]\n    def str_to_arr(s):\n        if type(s) is float: return s\n        arr = []\n        for a in [x for x in s.replace('[', '').replace(']', '').split(' ')]:\n            if a != '':\n                arr.append(a)\n        arr = np.array(arr)\n        arr = arr.astype(float)\n        arr = arr / np.sum(arr)\n        return arr\n    for sheet in conversion_sheet:\n        input_data[sheet[0]][sheet[1]] = input_data[sheet[0]][sheet[1]].apply(str_to_arr)\n\n    # load areas from layers and adjust area ids (if not using legacy data)\n    # if done this way, input data file is not updated\n    if config['link_mpas_to_subbasins'] and not config['use_legacy_input_data']:\n        print('Linking areas in input data...')\n        input_data = api_tools.link_areas(config, input_data)\n\n    # make sure areas do not go over 32 characters\n    # input_data['area']['area'] = input_data['area']['area'].apply(lambda x: x if len(x) &lt;= 32 else x[:32])\n    # validate IDs\n    for key in ['measure', 'activity', 'pressure', 'state', 'area']:\n        input_data[key][key] = input_data[key][key].apply(sanitize_string)\n\n    return input_data\n</code></pre>"},{"location":"modules/som_app/#src.som_app.build_links","title":"<code>build_links(data)</code>","text":"<p>Build links by picking random samples using probability distributions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>dict of dataframes containing all links and ids relevant to SOM calculations.</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>dict</code> <p>updated links and ids relevant to SOM calculations.</p> Source code in <code>src/som_app.py</code> <pre><code>def build_links(data: dict[str, pd.DataFrame]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Build links by picking random samples using probability distributions.\n\n    Arguments:\n        data (dict): dict of dataframes containing all links and ids relevant to SOM calculations.\n\n    Returns:\n        data (dict): updated links and ids relevant to SOM calculations.\n    \"\"\"\n    #\n    # measure effects\n    #\n\n    # verify that there are no duplicate links\n    try: assert len(data['measure_effects'][data['measure_effects'].duplicated(['measure', 'activity', 'pressure', 'state'])]) == 0\n    except Exception as e: fail_with_message(f'Duplicate measure effects in input data!', e)\n\n    # get picks from cumulative distribution\n    data['measure_effects']['reduction'] = data['measure_effects']['reduction'].apply(get_pick)\n\n    #\n    # activity contributions\n    #\n\n    data['activity_contributions']['contribution'] = data['activity_contributions']['contribution'].apply(get_pick)\n\n    #\n    # pressure contributions\n    #\n\n    # get picks from cumulative distribution\n    data['pressure_contributions']['contribution'] = data['pressure_contributions']['contribution'].apply(lambda x: get_pick(x) if not np.any(np.isnan(x)) else np.nan)\n\n    data['pressure_contributions'] = data['pressure_contributions'].drop_duplicates(subset=['state', 'pressure', 'area_id'], keep='first').reset_index(drop=True)\n\n    # verify that there are no duplicate links\n    try: assert len(data['pressure_contributions'][data['pressure_contributions'].duplicated(['state', 'pressure', 'area_id'])]) == 0\n    except Exception as e: fail_with_message(f'Duplicate pressure contributions in input data!', e)\n\n    # make sure pressure contributions for each state / area are 100 %\n    for area in data['area']['ID']:\n        for state in data['state']['ID']:\n            mask = (data['pressure_contributions']['area_id'] == area) &amp; (data['pressure_contributions']['state'] == state)\n            relevant_contributions = data['pressure_contributions'].loc[mask, :]\n            if len(relevant_contributions) &gt; 0:\n                data['pressure_contributions'].loc[mask, 'contribution'] = relevant_contributions['contribution'] / relevant_contributions['contribution'].sum()\n\n    #\n    # thresholds\n    #\n\n    threshold_cols = ['PR', '10', '25', '50']   # target thresholds (PR=GES)\n\n    # get picks from cumulative distribution\n    for col in threshold_cols:\n        data['thresholds'][col] = data['thresholds'][col].apply(lambda x: get_pick(x) if not np.any(np.isnan(x)) else np.nan)\n\n    data['thresholds'] = data['thresholds'].drop_duplicates(subset=['state', 'area_id'], keep='first').reset_index(drop=True)\n\n    # verify that there are no duplicate links\n    try: assert len(data['thresholds'][data['thresholds'].duplicated(['state', 'area_id'])]) == 0\n    except Exception as e: fail_with_message(f'Duplicate GES targets in input data!', e)\n\n    return data\n</code></pre>"},{"location":"modules/som_app/#src.som_app.build_results","title":"<code>build_results(sim_res, input_data)</code>","text":"<p>Process the simulated results to calculate uncertainties.  Uncertainty is determined as standard error of the mean.</p> <p>Parameters:</p> Name Type Description Default <code>sim_res</code> <code>str</code> <p>path to directory holding individual simulation run results.</p> required <code>input_data</code> <code>dict</code> <p>SOM input data.</p> required <p>Returns:</p> Name Type Description <code>res</code> <code>dict</code> <p>SOM calculation results.</p> Source code in <code>src/som_app.py</code> <pre><code>def build_results(sim_res: str, input_data: dict[str, pd.DataFrame]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Process the simulated results to calculate uncertainties. \n    Uncertainty is determined as standard error of the mean.\n\n    Arguments:\n        sim_res (str): path to directory holding individual simulation run results.\n        input_data (dict): SOM input data.\n\n    Returns:\n        res (dict): SOM calculation results.\n    \"\"\"\n    files = [os.path.join(sim_res, x) for x in os.listdir(sim_res) if x.endswith('.pickle') and 'sim_res' in x]\n\n    areas = input_data['area']['ID']\n    pressures = input_data['pressure']['ID']\n    states = input_data['state']['ID']\n\n    res = {}\n\n    for key, val, ids in [\n        ('Pressure', 'pressure_levels', pressures), \n        ('TPL', 'total_pressure_load_levels', states), \n        ('TPLRed', 'total_pressure_load_reductions', states), \n        ('Thresholds', ('thresholds', 'PR'), states)\n    ]:\n        res[key] = {\n            'Mean': pd.DataFrame(ids).reindex(columns=['ID']+areas.tolist()).fillna(1.0), \n            'Error': pd.DataFrame(ids).reindex(columns=['ID']+areas.tolist()).fillna(1.0)\n        }\n        arr = np.empty(shape=(len(ids.tolist()), len(areas.tolist()), len(files)))\n        for i in range(len(files)):\n            with open(files[i], 'rb') as f:\n                data = pickle.load(f)\n            if type(val) == str:\n                arr[:, :, i] = data[val].values[:, 1:]\n            else:\n                arr[:, :, i] = data[val[0]][val[1]].values[:, 1:]\n        res[key]['Mean'].iloc[:, 1:] = np.mean(arr, axis=2)\n        res[key]['Error'].iloc[:, 1:] = np.std(arr, axis=2, ddof=1) / np.sqrt(arr.shape[2])    # calculate standard error\n\n    res['StatePressure'] = {\n        s: {\n            'Mean': pd.DataFrame(pressures).reindex(columns=['ID']+areas.tolist()).fillna(1.0), \n            'Error': pd.DataFrame(pressures).reindex(columns=['ID']+areas.tolist()).fillna(1.0)\n        } for s in states\n    }\n    for s in res['StatePressure']:\n        arr = np.empty(shape=(len(pressures.tolist()), len(areas.tolist()), len(files)))\n        for i in range(len(files)):\n            with open(files[i], 'rb') as f:\n                data = pickle.load(f)\n            arr[:, :, i] = data['state_pressure_levels'][s].values[:, 1:]\n        res['StatePressure'][s]['Mean'].iloc[:, 1:] = np.mean(arr, axis=2)\n        res['StatePressure'][s]['Error'].iloc[:, 1:] = np.std(arr, axis=2, ddof=1) / np.sqrt(arr.shape[2])\n\n    for key, val, col in [\n        ('MeasureEffects', 'measure_effects', 'reduction'), \n        ('ActivityContributions', 'activity_contributions', 'contribution'), \n        ('PressureContributions', 'pressure_contributions', 'contribution')\n    ]:\n        res[key] = {\n            'Mean': pd.DataFrame(input_data[val]), \n            'Error': pd.DataFrame(input_data[val])\n        }\n        arr = np.empty(shape=([x for x in input_data[val].values.shape]+[len(files)]))\n        for i in range(len(files)):\n            with open(files[i], 'rb') as f:\n                data = pickle.load(f)\n            arr[:, :, i] = data[val].values\n        res[key]['Mean'][col] = np.mean(arr[:, -1, :], axis=1)\n        res[key]['Error'][col] = np.std(arr[:, -1, :], axis=1, ddof=1) / np.sqrt(arr.shape[2])\n\n    return res\n</code></pre>"},{"location":"modules/som_app/#src.som_app.build_scenario","title":"<code>build_scenario(data, scenario)</code>","text":"<p>Build scenario. Updates activity contributions to pressures to reflect changes in the activities.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>dict of dataframes containing all links and ids relevant to SOM calculations.</p> required <code>scenario</code> <code>str</code> <p>name of scenario to be accessed from data.</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>dict</code> <p>updated links and ids relevant to SOM calculations.</p> Source code in <code>src/som_app.py</code> <pre><code>def build_scenario(data: dict[str, pd.DataFrame], scenario: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Build scenario. Updates activity contributions to pressures to reflect changes in the activities.\n\n    Arguments:\n        data (dict): dict of dataframes containing all links and ids relevant to SOM calculations.\n        scenario (str): name of scenario to be accessed from data.\n\n    Returns:\n        data (dict): updated links and ids relevant to SOM calculations.\n    \"\"\"\n    act_to_press = data['activity_contributions']\n    dev_scen = data['development_scenarios']\n\n    # for each pressure, save the total contribution of activities for later normalization\n    actual_sum = {}\n    for pressure_id in act_to_press['pressure'].unique():\n        actual_sum[pressure_id] = {}\n        activities = act_to_press.loc[act_to_press['pressure'] == pressure_id, :]\n        for area in activities['area_id'].unique():\n            actual_sum[pressure_id][area] = activities.loc[activities['area_id'] == area, 'contribution'].sum()\n\n    # multiply activities by scenario multiplier\n    def get_scenario(activity_id):\n        multiplier = dev_scen.loc[dev_scen['activity'] == activity_id, scenario]\n        if len(multiplier) == 0:\n            return 1\n        multiplier = multiplier.values[0]\n        return multiplier\n    act_to_press['contribution'] = act_to_press['contribution'] * act_to_press['activity'].apply(get_scenario)\n\n    # normalize\n    normalize_factor = {}\n    for pressure_id in act_to_press['pressure'].unique():\n        normalize_factor[pressure_id] = {}\n        activities = act_to_press.loc[act_to_press['pressure'] == pressure_id, :]\n        for area in activities['area_id'].unique():\n            scenario_sum = activities.loc[activities['area_id'] == area, 'contribution'].sum()\n            normalize_factor[pressure_id][area] = 1 + scenario_sum - actual_sum[pressure_id][area]\n\n    def normalize(value, pressure_id, area_id):\n        return value / normalize_factor[pressure_id][area_id]\n\n    act_to_press['contribution'] = act_to_press.apply(lambda x: normalize(x['contribution'], x['pressure'], x['area_id']), axis=1)\n\n    return act_to_press\n</code></pre>"},{"location":"modules/som_app/#src.som_app.export_results_to_excel","title":"<code>export_results_to_excel(res, input_data, export_path)</code>","text":"<p>Exports simulation results as excel file.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>dict</code> <p>SOM calculation results.</p> required <code>input_data</code> <code>dict</code> <p>SOM input data.</p> required <code>export_path</code> <code>str</code> <p>output path for exported results.</p> required Source code in <code>src/som_app.py</code> <pre><code>def export_results_to_excel(res: dict[str, pd.DataFrame], input_data: dict[str, pd.DataFrame], export_path: str):\n    \"\"\"\n    Exports simulation results as excel file.\n\n    Arguments:\n        res (dict): SOM calculation results.\n        input_data (dict): SOM input data.\n        export_path (str): output path for exported results.\n    \"\"\"\n    with pd.ExcelWriter(export_path) as writer:\n        new_res = set_id_columns(res, input_data)\n        for key in new_res:\n            if key != 'StatePressure':\n                for r in ['Mean', 'Error']:\n                    new_res[key][r].to_excel(writer, sheet_name=key+r, index=False)\n</code></pre>"},{"location":"modules/som_app/#src.som_app.set_id_columns","title":"<code>set_id_columns(res, data)</code>","text":"<p>Replaces id column values with the name of the corresponding measure/activity/pressure/state in the result dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>dict</code> <p>SOM calculation results.</p> required <p>Returns:</p> Name Type Description <code>res</code> <code>dict</code> <p>updated results.</p> Source code in <code>src/som_app.py</code> <pre><code>def set_id_columns(res: dict[str, pd.DataFrame], data: dict[str, pd.DataFrame]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Replaces id column values with the name of the corresponding measure/activity/pressure/state in the result dataframes.\n\n    Arguments:\n        res (dict): SOM calculation results.\n\n    Returns:\n        res (dict): updated results.\n    \"\"\"\n    res = copy.deepcopy(res)\n    relations = {\n        'Pressure': 'pressure', \n        'StatePressure': 'pressure', \n        'TPL': 'state', \n        'TPLRed': 'state', \n        'Thresholds': 'state', \n    }\n    def replace_ids(id, k):\n        return data[k].loc[data[k]['ID'] == id, k].values[0]\n    for key in relations:\n        if key == 'StatePressure':\n            for s in data['state']['ID']:\n                for r in ['Mean', 'Error']:\n                    res[key][s][r]['ID'] = res[key][s][r]['ID'].apply(lambda x: replace_ids(x, relations[key]))\n                    res[key][s][r] = res[key][s][r].rename(columns={col: data['area'].loc[data['area']['ID'] == col, 'area'].values[0] for col in [c for c in res[key][s][r].columns if c != 'ID']})\n        else:\n            for r in ['Mean', 'Error']:\n                res[key][r]['ID'] = res[key][r]['ID'].apply(lambda x: replace_ids(x, relations[key]))\n                res[key][r] = res[key][r].rename(columns={col: data['area'].loc[data['area']['ID'] == col, 'area'].values[0] for col in [c for c in res[key][r].columns if c != 'ID']})\n    relations = {\n        'MeasureEffects': ['measure', 'activity', 'pressure', 'state'], \n        'ActivityContributions': ['activity', 'pressure', 'area_id'], \n        'PressureContributions': ['state', 'pressure', 'area_id']\n    }\n    conversions = {\n        'activity': 'activity', \n        'pressure': 'pressure', \n        'state': 'state', \n        'area_id': 'area'\n    }\n    for key in relations:\n        for r in ['Mean', 'Error']:\n            for col in relations[key]:\n                k = conversions[col] if col in conversions else col\n                res[key][r][col] = res[key][r][col].apply(lambda id: data[k].loc[data[k]['ID'] == id, k].values[0] if id != 0 else '-')\n\n    return res\n</code></pre>"},{"location":"modules/som_plots/","title":"<code>som_plots</code>","text":"<p>Methods to plot SOM results.</p>"},{"location":"modules/som_plots/#src.som_plots.build_display","title":"<code>build_display(res, data, out_dir, use_parallel_processing=False, selection=None)</code>","text":"<p>Constructs plots to visualize results.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>dict</code> <p>SOM results.</p> required <code>data</code> <code>dict</code> <p>SOM input data.</p> required <code>out_dir</code> <code>str</code> <p>plot output directory.</p> required <code>use_parallel_processing</code> <code>bool</code> <p>toggle for multiprocessing.</p> <code>False</code> <code>selection</code> <code>dict</code> <p>filtering options.</p> <code>None</code> Source code in <code>src/som_plots.py</code> <pre><code>def build_display(res: dict[str, dict[str, pd.DataFrame]], data: dict[str, pd.DataFrame], out_dir: str, use_parallel_processing: bool = False, selection: dict[str, list] = None):\n    \"\"\"\n    Constructs plots to visualize results.\n\n    Arguments:\n        res (dict): SOM results.\n        data (dict): SOM input data.\n        out_dir (str): plot output directory.\n        use_parallel_processing (bool): toggle for multiprocessing.\n        selection (dict): filtering options.\n    \"\"\"\n    res = copy.deepcopy(res)\n    data = copy.deepcopy(data)\n\n    if selection is not None:\n        print('\\t\\tFiltering results...')\n        res = filter_results(res, selection)\n        data = filter_ids(data, selection)\n\n    areas = data['area']['ID']\n\n    cpu_count = multiprocessing.cpu_count()     # available cpu cores\n    with multiprocessing.Manager() as manager:\n        progress = manager.Namespace()\n        progress.current = 0\n        progress.total = len(areas)\n        lock = manager.Lock()\n        if use_parallel_processing:\n            with multiprocessing.Pool(processes=(min(cpu_count - 2, len(areas)))) as pool:\n                jobs = [(area, res, data, out_dir, progress, lock) for area in areas]\n                progress.current = 0\n                display_progress(progress.current / progress.total, text='\\t\\tTPL: ')\n                pool.starmap(plot_total_pressure_load_levels, jobs)\n                display_progress(progress.current / progress.total, text='\\t\\tTPL: ')\n                progress.current = 0\n                display_progress(progress.current / progress.total, text='\\n\\t\\tPressures: ')\n                pool.starmap(plot_pressure_levels, jobs)\n                display_progress(progress.current / progress.total, text='\\t\\tPressures: ')\n                progress.current = 0\n                display_progress(progress.current / progress.total, text='\\n\\t\\tThresholds: ')\n                pool.starmap(plot_thresholds, jobs)\n                display_progress(progress.current / progress.total, text='\\t\\tThresholds: ')\n                progress.current = 0\n                display_progress(progress.current / progress.total, text='\\n\\t\\tStatePressures: ')\n                pool.starmap(plot_state_pressure_levels, jobs)\n                display_progress(progress.current / progress.total, text='\\t\\tStatePressures: ')\n                progress.current = 0\n                display_progress(progress.current / progress.total, text='\\n\\t\\tActivityContributions: ')\n                pool.starmap(plot_activity_contributions, jobs)\n                display_progress(progress.current / progress.total, text='\\t\\tActivityContributions: ')\n                progress.current = 0\n                display_progress(progress.current / progress.total, text='\\n\\t\\tPressureContributions: ')\n                pool.starmap(plot_pressure_contributions, jobs)\n                display_progress(progress.current / progress.total, text='\\t\\tPressureContributions: ')\n        else:\n            progress.current = 0\n            display_progress(progress.current / progress.total, text='\\t\\tTPL: ')\n            for area in areas:\n                plot_total_pressure_load_levels(area, res, data, out_dir, progress, lock)\n            display_progress(progress.current / progress.total, text='\\t\\tTPL: ')\n            progress.current = 0\n            display_progress(progress.current / progress.total, text='\\n\\t\\tPressures: ')\n            for area in areas:\n                plot_pressure_levels(area, res, data, out_dir, progress, lock)\n            display_progress(progress.current / progress.total, text='\\t\\tPressures: ')\n            progress.current = 0\n            display_progress(progress.current / progress.total, text='\\n\\t\\tThresholds: ')\n            for area in areas:\n                plot_thresholds(area, res, data, out_dir, progress, lock)\n            display_progress(progress.current / progress.total, text='\\t\\tThresholds: ')\n            progress.current = 0\n            display_progress(progress.current / progress.total, text='\\n\\t\\tStatePressures: ')\n            for area in areas:\n                plot_state_pressure_levels(area, res, data, out_dir, progress, lock)\n            display_progress(progress.current / progress.total, text='\\t\\tStatePressures: ')\n            progress.current = 0\n            display_progress(progress.current / progress.total, text='\\n\\t\\tActivityContributions: ')\n            for area in areas:\n                plot_activity_contributions(area, res, data, out_dir, progress, lock)\n            display_progress(progress.current / progress.total, text='\\t\\tActivityContributions: ')\n            progress.current = 0\n            display_progress(progress.current / progress.total, text='\\n\\t\\tPressureContributions: ')\n            for area in areas:\n                plot_pressure_contributions(area, res, data, out_dir, progress, lock)\n            display_progress(progress.current / progress.total, text='\\t\\tPressureContributions: ')\n\n    #\n    # Measure effects\n    #\n\n    print('\\n\\t\\tMeasure effects...')\n\n    # plot settings\n    capsize = 3\n    capthick = 1\n    elinewidth = 1\n    ecolor = 'salmon'\n    label_angle = 60\n    char_limit = 25\n    bar_width = 0.4\n    edge_color = 'black'\n\n    fig, ax = plt.subplots(figsize=(100, 14), constrained_layout=True)\n\n    bar_width = 0.8\n    edge_color = 'black'\n    activity_font_size = 8\n\n    # adjust data\n    df = res['MeasureEffects']['Mean'].merge(res['MeasureEffects']['Error'], on=['measure', 'pressure', 'state', 'activity'], how='left', suffixes=('_mean', '_error'))\n    df = df.sort_values(by=['measure', 'pressure', 'state', 'activity'])\n    suffixes = ('', '_name')\n    for col in ['measure', 'activity', 'pressure', 'state']:\n        df = df.merge(data[col].loc[:, [col, 'ID']], left_on=col, right_on='ID', how='left', suffixes=suffixes)\n        df = df.drop(columns=[col, 'ID'])\n        df = df.rename(columns={col+'_name': col})\n        df.loc[:, col] = np.array([(x[:char_limit]+'...' if len(x) &gt; char_limit else x) if type(x) == str else 'All' for x in df.loc[:, col].values])\n    df['index'] = np.arange(len(df))\n    x_ticks = {x: df[df['measure'] == x]['index'].mean() for x in df['measure'].unique()}\n\n    # set colors\n    df['color_key'] = df['pressure'].astype(str) + '_' + df['state'].astype(str)\n    unique_keys = df['color_key'].unique()\n    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_keys)))\n    color_map = {key: colors[i] for i, key in enumerate(unique_keys)}\n\n    # create plot\n    for key in unique_keys:\n        subset = df[df['color_key'] == key]\n        bars = ax.bar(subset['index'], subset['reduction_mean'] * 100, width=bar_width, color=color_map[key], label=key if key not in ax.get_legend_handles_labels()[1] else '', edgecolor=edge_color)\n        ax.errorbar(subset['index'], subset['reduction_mean'] * 100, yerr=subset['reduction_error'] * 100, linestyle='None', marker='None', capsize=capsize, capthick=capthick, elinewidth=elinewidth, ecolor=ecolor)\n        for bar, (_, row) in zip(bars, subset.iterrows()):\n            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2, str(row['activity']), \n                    ha='center', va='center', rotation=90, fontsize=activity_font_size, color='white')\n\n    ax.set_xlabel('Measure')\n    ax.set_ylabel('Reduction effect (%)')\n    ax.set_title(f'Measure Reduction Effects')\n    ax.set_xticks(list(x_ticks.values()), list(x_ticks.keys()), rotation=label_angle, ha='right')\n    ax.yaxis.grid(True, linestyle='--', color='lavender')\n    ax.legend(title='Pressure/State', bbox_to_anchor=(1.05, 1), loc='upper left')\n\n    # adjust axis limits\n    x_lim = [- 0.5, len(df) - 0.5]\n    ax.set_xlim(x_lim)\n    y_lim = [0, 100]\n    ax.set_ylim(y_lim)\n\n    # export\n    for area in areas:\n        area_name = data['area'].loc[areas == area, 'area'].values[0]\n        temp_dir = os.path.join(out_dir, f'area_{area}_{area_name}')\n        plt.savefig(os.path.join(temp_dir, f'area_{area}_{area_name}_MeasureEffects.png'), dpi=200)\n\n    plt.close(fig)\n</code></pre>"},{"location":"modules/som_plots/#src.som_plots.filter_ids","title":"<code>filter_ids(input_data, selection)</code>","text":"<p>Filter input data id dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict</code> <p>SOM input data.</p> required <code>selection</code> <code>dict</code> <p>filtering options.</p> required <p>Returns:</p> Name Type Description <code>input_data</code> <code>dict</code> <p>filtered input data.</p> Source code in <code>src/som_plots.py</code> <pre><code>def filter_ids(input_data: dict[str, pd.DataFrame], selection: dict[str, list]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Filter input data id dataframes.\n\n    Arguments:\n        input_data (dict): SOM input data.\n        selection (dict): filtering options.\n\n    Returns:\n        input_data (dict): filtered input data.\n    \"\"\"\n    for key, values in [\n        ('measure', selection['measure']), \n        ('activity', selection['activity']), \n        ('pressure', selection['pressure']), \n        ('state', selection['state']), \n        ('area', selection['area'])\n    ]:\n        if values != []:\n            input_data[key] = input_data[key].loc[input_data[key]['ID'].isin(values), :]\n    return input_data\n</code></pre>"},{"location":"modules/som_plots/#src.som_plots.filter_results","title":"<code>filter_results(res, selection)</code>","text":"<p>Filter results for more selective output.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>dict</code> <p>SOM results.</p> required <code>selection</code> <code>dict</code> <p>filtering options.</p> required <p>Returns:</p> Name Type Description <code>res</code> <code>dict</code> <p>filtered results.</p> Source code in <code>src/som_plots.py</code> <pre><code>def filter_results(res: dict[str, pd.DataFrame], selection: dict[str, list]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Filter results for more selective output.\n\n    Arguments:\n        res (dict): SOM results.\n        selection (dict): filtering options.\n\n    Returns:\n        res (dict): filtered results.\n    \"\"\"\n    # Pressure, TPL, TPLRed, Thresholds\n    for key, values in [\n        ('Pressure', selection['pressure']), \n        ('TPL', selection['state']), \n        ('TPLRed', selection['state']), \n        ('Thresholds', selection['state'])\n    ]:\n        for r in ['Mean', 'Error']:\n            if values != []:\n                if selection['area'] != []:\n                    res[key][r] = res[key][r].loc[res[key][r]['ID'].isin(values), ['ID'] + selection['area']]\n                else:\n                    res[key][r] = res[key][r].loc[res[key][r]['ID'].isin(values), :]\n    # StatePressure\n    if selection['pressure'] != []:\n        for s in res['StatePressure']:\n            for r in ['Mean', 'Error']:\n                if selection['area'] != []:\n                    res['StatePressure'][s][r] = res['StatePressure'][s][r].loc[res['StatePressure'][s][r]['ID'].isin(selection['pressure']), ['ID'] + selection['area']]\n                else:\n                    res['StatePressure'][s][r] = res['StatePressure'][s][r].loc[res['StatePressure'][s][r]['ID'].isin(selection['pressure']), :]\n    # MeasureEffects, ActivityContributions, PressureContributions\n    for key, cols in {\n        'MeasureEffects': {\n            'measure': selection['measure'], \n            'activity': selection['activity'], \n            'pressure': selection['pressure'], \n            'state': selection['state']\n        }, \n        'ActivityContributions': {\n            'activity': selection['activity'], \n            'pressure': selection['pressure'], \n            'area_id': selection['area']\n        }, \n        'PressureContributions': {\n            'state': selection['state'], \n            'pressure': selection['pressure'], \n            'area_id': selection['area']\n        }\n    }.items():\n        for r in ['Mean', 'Error']:\n            for col, values in cols.items():\n                if values != []:\n                    res[key][r] = res[key][r].loc[res[key][r][col].isin(values), :]\n    return res\n</code></pre>"},{"location":"modules/som_plots/#src.som_plots.plot_activity_contributions","title":"<code>plot_activity_contributions(area, res, data, out_dir, progress, lock)</code>","text":"<p>Plots activity contributions.</p> Source code in <code>src/som_plots.py</code> <pre><code>def plot_activity_contributions(area, res, data, out_dir, progress, lock):\n    \"\"\"\n    Plots activity contributions.\n    \"\"\"\n    # create new directory for the plots\n    area_name = data['area'].loc[data['area']['ID'] == area, 'area'].values[0]\n    out_dir = os.path.join(out_dir, f'area_{area}_{area_name}', 'contributions', 'activity')\n    os.makedirs(out_dir, exist_ok=True)\n\n    # plot settings\n    char_limit = 25\n\n    for pressure in res['ActivityContributions']['Mean']['pressure'].unique():\n        pressure_name = data['pressure'].loc[data['pressure']['ID'] == pressure, 'pressure'].values[0]\n\n        out_path = os.path.join(out_dir, f'area_{area}_{area_name}_pressure_{pressure}_ActivityContributions.png')\n\n        fig, ax = plt.subplots(figsize=(12, 9), constrained_layout=True)\n\n        # adjust data\n        suffixes = ('_mean', '_error')\n        df = pd.merge(res['ActivityContributions']['Mean'].loc[:, :], res['ActivityContributions']['Error'].loc[:, :], on=['activity', 'pressure', 'area_id'], suffixes=suffixes)\n        contributions = df.loc[(df['area_id'] == area) &amp; (df['pressure'] == pressure), :]\n        labels = contributions['activity']\n        labels = contributions['activity'].apply(lambda x: data['activity'].loc[data['activity']['ID'] == x, 'activity'].values[0])\n        labels = np.array([x[:char_limit]+'...' if len(x) &gt; char_limit else x for x in labels])     # limit characters to char_limit\n        vals = contributions['contribution_mean'] * 100    # convert to %\n\n        # create plot\n        ax.pie(vals, labels=labels, autopct='%1.1f%%')\n        ax.set_title(f'Activity Contributions to Pressure ({pressure_name})\\n({area_name})')\n\n        # export\n        plt.savefig(out_path, dpi=200)\n        plt.close(fig)\n\n    with lock:\n        progress.current += 1\n        display_progress(progress.current / progress.total, text='\\t\\tActivityContributions: ')\n</code></pre>"},{"location":"modules/som_plots/#src.som_plots.plot_pressure_contributions","title":"<code>plot_pressure_contributions(area, res, data, out_dir, progress, lock)</code>","text":"<p>Plots pressure contributions.</p> Source code in <code>src/som_plots.py</code> <pre><code>def plot_pressure_contributions(area, res, data, out_dir, progress, lock):\n    \"\"\"\n    Plots pressure contributions.\n    \"\"\"\n    # create new directory for the plots\n    area_name = data['area'].loc[data['area']['ID'] == area, 'area'].values[0]\n    out_dir = os.path.join(out_dir, f'area_{area}_{area_name}', 'contributions', 'pressure')\n    os.makedirs(out_dir, exist_ok=True)\n\n    # plot settings\n    char_limit = 25\n\n    for state in res['PressureContributions']['Mean']['state'].unique():\n        state_name = data['state'].loc[data['state']['ID'] == state, 'state'].values[0]\n\n        out_path = os.path.join(out_dir, f'area_{area}_{area_name}_state_{state}_PressureContributions.png')\n\n        fig, ax = plt.subplots(figsize=(12, 9), constrained_layout=True)\n\n        # adjust data\n        suffixes = ('_mean', '_error')\n        df = pd.merge(res['PressureContributions']['Mean'].loc[:, :], res['PressureContributions']['Error'].loc[:, :], on=['state', 'pressure', 'area_id'], suffixes=suffixes)\n        contributions = df.loc[(df['area_id'] == area) &amp; (df['state'] == state) &amp; (pd.notna(df['contribution_mean'])), :]\n        if len(contributions) &gt; 0:\n            labels = contributions['pressure']\n            labels = contributions['pressure'].apply(lambda x: data['pressure'].loc[data['pressure']['ID'] == x, 'pressure'].values[0])\n            labels = np.array([x[:char_limit]+'...' if len(x) &gt; char_limit else x for x in labels])     # limit characters to char_limit\n            vals = contributions['contribution_mean'] * 100    # convert to %\n\n            # create plot\n            ax.pie(vals, labels=labels, autopct='%1.1f%%')\n            ax.set_title(f'Pressure Contributions to State ({state_name})\\n({area_name})')\n\n            # export\n            plt.savefig(out_path, dpi=200)\n        plt.close(fig)\n\n    with lock:\n        progress.current += 1\n        display_progress(progress.current / progress.total, text='\\t\\tPressureContributions: ')\n</code></pre>"},{"location":"modules/som_plots/#src.som_plots.plot_pressure_levels","title":"<code>plot_pressure_levels(area, res, data, out_dir, progress, lock)</code>","text":"<p>Plots pressures.</p> Source code in <code>src/som_plots.py</code> <pre><code>def plot_pressure_levels(area, res, data, out_dir, progress, lock):\n    \"\"\"\n    Plots pressures.\n    \"\"\"\n    # create new directory for the plots\n    area_name = data['area'].loc[data['area']['ID'] == area, 'area'].values[0]\n    out_path = os.path.join(out_dir, f'area_{area}_{area_name}', f'area_{area}_{area_name}_PressureLevels.png')\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # plot settings\n    marker = 's'\n    markersize = 5\n    markercolor = 'black'\n    capsize = 3\n    capthick = 1\n    elinewidth = 1\n    ecolor = 'salmon'\n    label_angle = 60\n    char_limit = 25\n\n    fig, ax = plt.subplots(figsize=(25, 12), constrained_layout=True)\n\n    # adjust data\n    suffixes = ('_mean', '_error')\n    df = pd.merge(res['Pressure']['Mean'].loc[:, ['ID', area]], res['Pressure']['Error'].loc[:, ['ID', area]], on='ID', suffixes=suffixes)\n    x_vals = data['pressure'].loc[:, 'pressure'].values\n    x_vals = np.array([x[:char_limit]+'...' if len(x) &gt; char_limit else x for x in x_vals])     # limit characters to char_limit\n    y_vals = df[str(area)+'_mean'] * 100    # convert to %\n    y_err = df[str(area)+'_error'] * 100    # conver to %\n\n    # create plot\n    ax.errorbar(np.arange(len(x_vals)), y_vals, yerr=y_err, linestyle='None', marker=marker, capsize=capsize, capthick=capthick, elinewidth=elinewidth, markersize=markersize, color=markercolor, ecolor=ecolor)\n    ax.set_xlabel('Pressure')\n    ax.set_ylabel('Level (%)')\n    ax.set_title(f'Pressure Levels\\n({area_name})')\n    ax.set_xticks(np.arange(len(x_vals)), x_vals, rotation=label_angle, ha='right')\n    ax.yaxis.grid(True, linestyle='--', color='lavender')\n\n    # adjust axis limits\n    x_lim = [- 0.5, len(x_vals) - 0.5]\n    ax.set_xlim(x_lim)\n    y_lim = [-5, 105]\n    ax.set_ylim(y_lim)\n\n    # export\n    plt.savefig(out_path, dpi=200)\n    plt.close(fig)\n\n    with lock:\n        progress.current += 1\n        display_progress(progress.current / progress.total, text='\\t\\tPressures: ')\n</code></pre>"},{"location":"modules/som_plots/#src.som_plots.plot_state_pressure_levels","title":"<code>plot_state_pressure_levels(area, res, data, out_dir, progress, lock)</code>","text":"<p>Plots state pressures.</p> Source code in <code>src/som_plots.py</code> <pre><code>def plot_state_pressure_levels(area, res, data, out_dir, progress, lock):\n    \"\"\"\n    Plots state pressures.\n    \"\"\"\n    # create new directory for the plots\n    area_name = data['area'].loc[data['area']['ID'] == area, 'area'].values[0]\n    out_dir = os.path.join(out_dir, f'area_{area}_{area_name}', 'state')\n    os.makedirs(out_dir, exist_ok=True)\n\n    # plot settings\n    marker = 's'\n    markersize = 5\n    markercolor = 'black'\n    capsize = 3\n    capthick = 1\n    elinewidth = 1\n    ecolor = 'salmon'\n    label_angle = 60\n    char_limit = 25\n\n    for state in res['StatePressure']:\n        state_name = data['state'].loc[data['state']['ID'] == state, 'state'].values[0]\n\n        out_path = os.path.join(out_dir, f'area_{area}_{area_name}_state_{state}_PressureLevels.png')\n\n        fig, ax = plt.subplots(figsize=(25, 12), constrained_layout=True)\n\n        # adjust data\n        suffixes = ('_mean', '_error')\n        df = pd.merge(res['StatePressure'][state]['Mean'].loc[:, ['ID', area]], res['StatePressure'][state]['Error'].loc[:, ['ID', area]], on='ID', suffixes=suffixes)\n        x_vals = data['pressure'].loc[:, 'pressure'].values\n        x_vals = np.array([x[:char_limit]+'...' if len(x) &gt; char_limit else x for x in x_vals])     # limit characters to char_limit\n        y_vals = df[str(area)+'_mean'] * 100    # convert to %\n        y_err = df[str(area)+'_error'] * 100    # conver to %\n\n        # create plot\n        ax.errorbar(np.arange(len(x_vals)), y_vals, yerr=y_err, linestyle='None', marker=marker, capsize=capsize, capthick=capthick, elinewidth=elinewidth, markersize=markersize, color=markercolor, ecolor=ecolor)\n        ax.set_xlabel('Pressure')\n        ax.set_ylabel('Level (%)')\n        ax.set_title(f'Pressure Levels ({state_name})\\n({area_name})')\n        ax.set_xticks(np.arange(len(x_vals)), x_vals, rotation=label_angle, ha='right')\n        ax.yaxis.grid(True, linestyle='--', color='lavender')\n\n        # adjust axis limits\n        x_lim = [- 0.5, len(x_vals) - 0.5]\n        ax.set_xlim(x_lim)\n        y_lim = [-5, 105]\n        ax.set_ylim(y_lim)\n\n        # export\n        plt.savefig(out_path, dpi=200)\n        plt.close(fig)\n\n    with lock:\n        progress.current += 1\n        display_progress(progress.current / progress.total, text='\\t\\tStatePressures: ')\n</code></pre>"},{"location":"modules/som_plots/#src.som_plots.plot_thresholds","title":"<code>plot_thresholds(area, res, data, out_dir, progress, lock)</code>","text":"<p>Plots thresholds comparison.</p> Source code in <code>src/som_plots.py</code> <pre><code>def plot_thresholds(area, res, data, out_dir, progress, lock):\n    \"\"\"\n    Plots thresholds comparison.\n    \"\"\"\n    # create new directory for the plots\n    area_name = data['area'].loc[data['area']['ID'] == area, 'area'].values[0]\n    out_path = os.path.join(out_dir, f'area_{area}_{area_name}', f'area_{area}_{area_name}_Thresholds.png')\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # plot settings\n    capsize = 3\n    capthick = 1\n    elinewidth = 1\n    ecolor = 'salmon'\n    label_angle = 60\n    char_limit = 25\n    bar_width = 0.4\n    bar_color_1 = 'turquoise'\n    bar_color_2 = 'seagreen'\n    edge_color = 'black'\n\n    fig, ax = plt.subplots(figsize=(16, 12), constrained_layout=True)\n\n    # adjust data\n    x_labels = np.array([x[:char_limit]+'...' if len(x) &gt; char_limit else x for x in data['state'].loc[:, 'state'].values])     # limit characters to char_limit\n    x_vals = np.arange(len(x_labels))\n    suffixes = ('_mean', '_error')\n    df = pd.merge(res['TPLRed']['Mean'].loc[:, ['ID', area]], res['TPLRed']['Error'].loc[:, ['ID', area]], on='ID', suffixes=suffixes)\n    y_vals_tpl = df[str(area)+'_mean'] * 100    # convert to %\n    y_err_tpl = df[str(area)+'_error'] * 100    # conver to %\n    df = pd.merge(res['Thresholds']['Mean'].loc[:, ['ID', area]], res['Thresholds']['Error'].loc[:, ['ID', area]], on='ID', suffixes=suffixes)\n    y_vals_ges = df[str(area)+'_mean'] * 100    # convert to %\n    y_err_ges = df[str(area)+'_error'] * 100    # convert to %\n\n    # create plot\n    label_tpl = 'Reduction with measures'\n    ax.bar(x_vals-bar_width/2, y_vals_tpl, width=bar_width, align='center', color=bar_color_1, label=label_tpl, edgecolor=edge_color)\n    ax.errorbar(x_vals-bar_width/2, y_vals_tpl, yerr=y_err_tpl, linestyle='None', marker='None', capsize=capsize, capthick=capthick, elinewidth=elinewidth, ecolor=ecolor)\n    label_ges = 'Target'\n    ax.bar(x_vals+bar_width/2, y_vals_ges, width=bar_width, align='center', color=bar_color_2, label=label_ges, edgecolor=edge_color)\n    ax.errorbar(x_vals+bar_width/2, y_vals_ges, yerr=y_err_ges, linestyle='None', marker='None', capsize=capsize, capthick=capthick, elinewidth=elinewidth, ecolor=ecolor)\n    ax.set_xlabel('Environmental State')\n    ax.set_ylabel('Reduction (%)')\n    ax.set_title(f'Total Pressure Load Reduction vs. GES Reduction Thresholds\\n({area_name})')\n    ax.set_xticks(x_vals, x_labels, rotation=label_angle, ha='right')\n    ax.yaxis.grid(True, linestyle='--', color='lavender')\n    ax.legend()\n\n    # adjust axis limits\n    x_lim = [- 0.5, len(x_vals) - 0.5]\n    ax.set_xlim(x_lim)\n    y_lim = [0, 100]\n    ax.set_ylim(y_lim)\n\n    # export\n    plt.savefig(out_path, dpi=200)\n    plt.close(fig)\n\n    with lock:\n        progress.current += 1\n        display_progress(progress.current / progress.total, text='\\t\\tThresholds: ')\n</code></pre>"},{"location":"modules/som_plots/#src.som_plots.plot_total_pressure_load_levels","title":"<code>plot_total_pressure_load_levels(area, res, data, out_dir, progress, lock)</code>","text":"<p>Plots Total Pressure Load.</p> Source code in <code>src/som_plots.py</code> <pre><code>def plot_total_pressure_load_levels(area, res, data, out_dir, progress, lock):\n    \"\"\"\n    Plots Total Pressure Load.\n    \"\"\"\n    # create new directory for the plots\n    area_name = data['area'].loc[data['area']['ID'] == area, 'area'].values[0]\n    out_path = os.path.join(out_dir, f'area_{area}_{area_name}', f'area_{area}_{area_name}_TotalPressureLoadLevels.png')\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # plot settings\n    marker = 's'\n    markersize = 5\n    markercolor = 'black'\n    capsize = 3\n    capthick = 1\n    elinewidth = 1\n    ecolor = 'salmon'\n    label_angle = 60\n    char_limit = 25\n\n    fig, ax = plt.subplots(figsize=(16, 12), constrained_layout=True)\n\n    # adjust data\n    suffixes = ('_mean', '_error')\n    df = pd.merge(res['TPL']['Mean'].loc[:, ['ID', area]], res['TPL']['Error'].loc[:, ['ID', area]], on='ID', suffixes=suffixes)\n    x_vals = data['state'].loc[:, 'state'].values\n    x_vals = np.array([x[:char_limit]+'...' if len(x) &gt; char_limit else x for x in x_vals])     # limit characters to char_limit\n    y_vals = df[str(area)+'_mean'] * 100    # convert to %\n    y_err = df[str(area)+'_error'] * 100    # conver to %\n\n    # create plot\n    ax.errorbar(np.arange(len(x_vals)), y_vals, yerr=y_err, linestyle='None', marker=marker, capsize=capsize, capthick=capthick, elinewidth=elinewidth, markersize=markersize, color=markercolor, ecolor=ecolor)\n    ax.set_xlabel('Environmental State')\n    ax.set_ylabel('Level (%)')\n    ax.set_title(f'Total Pressure Load on Environmental States\\n({area_name})')\n    ax.set_xticks(np.arange(len(x_vals)), x_vals, rotation=label_angle, ha='right')\n    ax.yaxis.grid(True, linestyle='--', color='lavender')\n\n    # adjust axis limits\n    x_lim = [- 0.5, len(x_vals) - 0.5]\n    ax.set_xlim(x_lim)\n    y_lim = [-5, 105]\n    ax.set_ylim(y_lim)\n\n    # export\n    plt.savefig(out_path, dpi=200)\n    plt.close(fig)\n\n    with lock:\n        progress.current += 1\n        display_progress(progress.current / progress.total, text='\\t\\tTPL: ')\n</code></pre>"},{"location":"modules/som_tools/","title":"<code>som_tools</code>","text":"<p>Methods to preprocess input data for SOM.</p>"},{"location":"modules/som_tools/#src.som_tools.get_expert_ids","title":"<code>get_expert_ids(df)</code>","text":"<p>Returns list of expert id column names from dataframe using regex.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>dataframe containing expert idc columns.</p> required <p>Returns:</p> Type Description <code>list</code> <p>list</p> Source code in <code>src/som_tools.py</code> <pre><code>def get_expert_ids(df: pd.DataFrame) -&gt; list:\n    '''\n    Returns list of expert id column names from dataframe using regex.\n\n    Arguments:\n        df (DataFrame): dataframe containing expert idc columns.\n\n    Returns:\n        list\n    '''\n    return df.filter(regex='^(100|[1-9]?[0-9])$').columns\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.process_input_data","title":"<code>process_input_data(config)</code>","text":"<p>Reads in data and processes raw input data to usable form.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>dictionary loaded from configuration file.</p> required <p>Returns:</p> Name Type Description <code>measure_survey_df</code> <code>DataFrame</code> <p>contains the measure survey data of expert panels.</p> <code>pressure_survey_df</code> <code>DataFrame</code> <p>contains the pressure survey data of expert panels.</p> <code>data</code> <code>dict</code> <p>container for general data dataframes:</p> <ul> <li> <p>measure (DataFrame):</p> <ul> <li>ID: unique measure identifier.</li> <li>measure: name / description column.</li> </ul> </li> <li> <p>activity (DataFrame):</p> <ul> <li>ID: unique activity identifier.</li> <li>activity: name / description column.</li> </ul> </li> <li> <p>pressure (DataFrame):</p> <ul> <li>ID: unique pressure identifier.</li> <li>pressure: name / description column.</li> </ul> </li> <li> <p>state (DataFrame):</p> <ul> <li>ID: unique state identifier.</li> <li>state: name / description column.</li> </ul> </li> <li> <p>area (DataFrame):</p> <ul> <li>ID: unique area identifier.</li> <li>area: name / description column.</li> </ul> </li> <li> <p>measure_effects (DataFrame): measure effects on activities, pressures, states.</p> </li> <li>pressure_contributions (DataFrame): pressure contributions to states.</li> <li>thresholds (DataFrame): changes in states required to meet specific target thresholds.</li> <li>cases (DataFrame): measure implementations in areas.</li> <li>activity_contributions (DataFrame): activity contributions to pressures.</li> <li>overlaps (DataFrame): measure-measure interactions.</li> <li>development_scenarios (DataFrame): changes in human activities.</li> <li>subpressures (DataFrame): pressure-pressure interactions.</li> </ul> Source code in <code>src/som_tools.py</code> <pre><code>def process_input_data(config: dict) -&gt; dict[str, pd.DataFrame | dict[str, pd.DataFrame]]:\n    \"\"\"\n    Reads in data and processes raw input data to usable form.\n\n    Arguments:\n        config (dict): dictionary loaded from configuration file.\n\n    Returns:\n        measure_survey_df (DataFrame): contains the measure survey data of expert panels.\n        pressure_survey_df (DataFrame): contains the pressure survey data of expert panels.\n        data (dict): container for general data dataframes:\n\n            - measure (DataFrame):\n                - ID: unique measure identifier.\n                - measure: name / description column.\n\n            - activity (DataFrame):\n                - ID: unique activity identifier.\n                - activity: name / description column.\n\n            - pressure (DataFrame):\n                - ID: unique pressure identifier.\n                - pressure: name / description column.\n\n            - state (DataFrame):\n                - ID: unique state identifier.\n                - state: name / description column.\n\n            - area (DataFrame):\n                - ID: unique area identifier.\n                - area: name / description column.\n\n            - measure_effects (DataFrame): measure effects on activities, pressures, states.\n            - pressure_contributions (DataFrame): pressure contributions to states.\n            - thresholds (DataFrame): changes in states required to meet specific target thresholds.\n            - cases (DataFrame): measure implementations in areas.\n            - activity_contributions (DataFrame): activity contributions to pressures.\n            - overlaps (DataFrame): measure-measure interactions.\n            - development_scenarios (DataFrame): changes in human activities.\n            - subpressures (DataFrame): pressure-pressure interactions.\n    \"\"\"\n    #\n    # measure survey data\n    #\n\n    file_name = os.path.realpath(config['input_data_legacy']['measure_effect_input'])\n    if not os.path.isfile(file_name): file_name = os.path.join(os.path.dirname(os.path.realpath(__file__)), config['input_data_legacy']['measure_effect_input'])\n    measure_effects = process_measure_survey_data(file_name)\n\n    #\n    # pressure survey data (combined pressure contributions and GES threshold)\n    #\n\n    file_name = os.path.realpath(config['input_data_legacy']['pressure_state_input'])\n    if not os.path.isfile(file_name): file_name = os.path.join(os.path.dirname(os.path.realpath(__file__)), config['input_data_legacy']['pressure_state_input'])\n    pressure_contributions, thresholds = process_pressure_survey_data(file_name)\n\n    #\n    # measure / pressure / activity / state links\n    #\n\n    # read core object descriptions\n    # i.e. ids for measures, activities, pressures and states\n    file_name = os.path.realpath(config['input_data_legacy']['general_input'])\n    if not os.path.isfile(file_name): file_name = os.path.join(os.path.dirname(os.path.realpath(__file__)), config['input_data_legacy']['general_input'])\n    id_sheets = config['input_data_legacy']['general_input_sheets']['ID']\n    data = read_ids(file_name=file_name, id_sheets=id_sheets)\n\n    #\n    # read case input\n    #\n\n    sheet_name = config['input_data_legacy']['general_input_sheets']['case']\n    cases = read_cases(file_name=file_name, sheet_name=sheet_name)\n\n    #\n    # read activity contribution data\n    #\n\n    sheet_name = config['input_data_legacy']['general_input_sheets']['postprocess']\n    activity_contributions = read_activity_contributions(file_name=file_name, sheet_name=sheet_name)\n\n    #\n    # read overlap data\n    #\n\n    sheet_name = config['input_data_legacy']['general_input_sheets']['overlaps']\n    overlaps = read_overlaps(file_name=file_name, sheet_name=sheet_name)\n\n    #\n    # read activity development scenario data\n    #\n\n    sheet_name = config['input_data_legacy']['general_input_sheets']['development_scenarios']\n    development_scenarios = read_development_scenarios(file_name=file_name, sheet_name=sheet_name)\n\n    #\n    # read subpressures links\n    #\n\n    sheet_name = config['input_data_legacy']['general_input_sheets']['subpressures']\n    subpressures = read_subpressures(file_name=file_name, sheet_name=sheet_name)\n\n    data.update({\n        'measure_effects': measure_effects, \n        'pressure_contributions': pressure_contributions, \n        'thresholds': thresholds, \n        'cases': cases, \n        'activity_contributions': activity_contributions, \n        'overlaps': overlaps, \n        'development_scenarios': development_scenarios, \n        'subpressures': subpressures\n    })\n\n    return data\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.process_measure_survey_data","title":"<code>process_measure_survey_data(file_name)</code>","text":"<p>This method reads input from the excel file containing data about measure reduction efficiencies  on activities, pressures and states.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>path of survey excel file.</p> required <p>Returns:</p> Name Type Description <code>survey_df</code> <code>DataFrame</code> <p>processed survey data information:</p> <ul> <li>measure (int): measure id.</li> <li>activity (int): activity id.</li> <li>pressure (int): pressure id.</li> <li>state (int): state id (if defined, [nan] if no state).</li> <li>reduction (list[float]): probability distribution represented as list.</li> </ul> Source code in <code>src/som_tools.py</code> <pre><code>def process_measure_survey_data(file_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    This method reads input from the excel file containing data about measure reduction efficiencies \n    on activities, pressures and states.\n\n    Arguments:\n        file_name (str): path of survey excel file.\n\n    Returns:\n        survey_df (DataFrame): processed survey data information:\n\n            - measure (int): measure id.\n            - activity (int): activity id.\n            - pressure (int): pressure id.\n            - state (int): state id (if defined, [nan] if no state).\n            - reduction (list[float]): probability distribution represented as list.\n    \"\"\"\n    #\n    # read information sheet from input Excel file\n    #\n\n    data = pd.read_excel(io=file_name, sheet_name=None, header=None)\n    sheet_names = list(data.keys())\n\n    mteq = data[sheet_names[0]]\n    mteq.columns = mteq.iloc[0].values\n    mteq = mteq[1:]\n\n    measure_survey_data = {}\n    for id in range(1, len(sheet_names)):\n        measure_survey_data[id] = data[sheet_names[id]]\n\n    #\n    # preprocess values\n    #\n\n    mteq.loc[:, 'State'] = [x.split(';') if type(x) == str else x for x in mteq['State']]\n\n    #\n    # create new dataframe\n    #\n\n    cols = ['survey_id', 'title', 'block', 'measure', 'activity', 'pressure', 'state']\n    survey_df = pd.DataFrame(columns=cols)\n\n    block_number = 0    # represents the survey block\n\n    # for every survey sheet\n    for survey_id in measure_survey_data:\n\n        survey_info = mteq[mteq['Survey ID'] == survey_id]  # select the rows linked to current survey\n\n        end = 0     # represents last column index of the question set\n        for row, amt in enumerate(survey_info['AMT']):  # for each set of questions (row in MTEQ)\n\n            end = end + (2 * amt + 1)     # end column index for data\n            start = end - (2 * amt)     # start column index for data\n\n            # create list to describe the data on each row\n            titles = ['expected value', 'variance'] * amt\n            titles.append('max effectiveness')\n            titles.append('expert weights')\n\n            # select current question column names as measure ids\n            measures = measure_survey_data[survey_id].iloc[0, start:end].tolist()\n            measures.append(np.nan)\n            measures.append(np.nan)\n\n            # create lists to hold ids and format each row as a list\n            ids = {}\n            for category in ['Activity', 'Pressure', 'State']:\n                category_ids = survey_info[category].iloc[row]\n                if isinstance(category_ids, str):\n                    ids[category] = [[int(x) for x in category_ids.split(';') if x != '']] * amt * 2\n                elif isinstance(category_ids, list):\n                    ids[category] = [[int(x) for x in category_ids if x != '']] * amt * 2\n                elif isinstance(category_ids, float) or isinstance(category_ids, int):\n                    ids[category] = [[category_ids if not np.isnan(category_ids) else np.nan]] * amt * 2\n                else:\n                    ids[category] = [category_ids] * amt * 2\n                ids[category].append(np.nan)\n                ids[category].append(np.nan)\n\n            # in MTEQ sheet, find all expert weight columns, get the values for the current row, set empty cells to 1\n            expert_cols = [True if 'exp' in col.lower() else False for col in survey_info.columns]\n            expert_weights = survey_info.loc[:, expert_cols].iloc[row]\n            expert_weights = expert_weights.astype(float).fillna(1).astype(int) # convert to float first so fillna() works without warning\n\n            data = measure_survey_data[survey_id].loc[1:, start:end]    # select current question answers\n            data[end+1] = expert_weights  # create column for expert weights\n            for expert, weight in enumerate(expert_weights, 1): # for each row (expert) in weights\n                data.loc[expert, end+1] = weight    # set the weight as the value\n            data = data.transpose() # transpose so that experts are columns and measures are rows\n\n            # add survey info to each entry in the data \n            data['survey_id'] = [survey_id] * len(data) # new column with survey_id for every row\n            data['title'] = titles\n            data['block'] = [block_number] * len(data)  # new column with block_number for every row\n            data['measure'] = measures\n            data['activity'] = ids['Activity']\n            data['pressure'] = ids['Pressure']\n            data['state'] = ids['State']\n\n            with warnings.catch_warnings(action='ignore'):\n                survey_df = pd.concat([survey_df, data], ignore_index=True, sort=False)\n            block_number = block_number + 1\n\n    # select column names corresponding to expert ids (any number between 1 and 100)\n    expert_ids = get_expert_ids(survey_df)\n\n    #\n    # Adjust answers by scaling factor\n    #\n\n    block_ids = survey_df.loc[:,'block'].unique()   # find unique block ids\n    for b_id in block_ids:  # for each block\n        block = survey_df.loc[survey_df['block'] == b_id, :]    # select all rows with current block id\n        for col in block:   # for each column\n            if isinstance(col, int):    # if it is an expert answer\n                # from the column, select the expected values and variances\n                expected_value = block.loc[block['title']=='expected value', col]\n                variance = block.loc[block['title']=='variance', col]\n                # skip if no questions were answered\n                if expected_value.isnull().all():\n                    block.loc[block['title']=='variance', col] = np.nan     # also set all variances to null\n                    continue\n                if variance.isnull().all():\n                    block.loc[block['title']=='expected value', col] = np.nan     # also set all expected values to null\n                    continue\n                # find the highest value of the answers\n                max_expected_value = expected_value.max()\n                # find the max effectiveness estimated by the expert\n                max_effectiveness = block.loc[block['title']=='max effectiveness', col].values[0]\n                # calculate scaling factor\n                if np.isnan(max_effectiveness):\n                    # set all values to null if no max effectiveness (in column, for current block)\n                    survey_df.loc[survey_df['block'] == b_id, col] = np.nan\n                elif max_effectiveness == 0 or max_expected_value == 0:\n                    # scale all expected values to 0 if max effectiveness is zero or all expected values are zero\n                    survey_df.loc[(survey_df['block'] == b_id) &amp; (survey_df['title'] == 'expected value'), col] = 0\n                else:\n                    # get the scaling factor\n                    scaling_factor = np.divide(max_expected_value, max_effectiveness)\n                    # divide the expected values by the new scaling factor\n                    survey_df.loc[(survey_df['block'] == b_id) &amp; (survey_df['title'] == 'expected value'), col] = np.divide(expected_value, scaling_factor)\n\n    #\n    # Calculate effectiveness range boundaries\n    #\n\n    # create new rows for 'effectiveness lower' and 'effectiveness upper' bounds after variance rows\n    new_rows = []\n    for i, row in survey_df.iterrows():\n        new_rows.append(row)\n        if row['title'] == 'variance':\n            # create lower bound\n            min_row = survey_df.loc[i].copy()\n            min_row['title'] = 'effectiveness lower'\n            min_row[expert_ids] = np.nan\n            new_rows.append(min_row)\n            # create upper bound\n            max_row = survey_df.loc[i].copy()\n            max_row['title'] = 'effectiveness upper'\n            max_row[expert_ids] = np.nan\n            new_rows.append(max_row)\n    survey_df = pd.DataFrame(new_rows, columns=survey_df.columns)\n    survey_df.reset_index(drop=True, inplace=True)\n    # set values for 'effectiveness lower' and 'effectiveness upper' bounds rows\n    # calculated as follows:\n    #   lower boundary:\n    #       if expected_value + variance / 2 &gt; 100:\n    #           boundary = 100 - variance\n    #       else:\n    #           if expected_value - variance / 2 &lt; 0:\n    #               boundary = 0\n    #           else:\n    #               boundary = expected_value - variance / 2\n    #   upper boundary:\n    #       if expected_value - variance / 2 &lt; 0:\n    #           boundary = variance\n    #       else:\n    #           if expected_value + variance / 2 &gt; 100:\n    #               boundary = 100\n    #           else:\n    #               boundary = expected_value + variance / 2\n    for i, row in survey_df.iterrows():\n        if row['title'] == 'effectiveness lower':\n            expected_value = survey_df.iloc[i-2][expert_ids]\n            variance = survey_df.iloc[i-1][expert_ids]\n            reach_upper_limit = expected_value + variance / 2 &gt; 100 # boolean array\n            row_values = survey_df.loc[i, expert_ids]\n            row_values[reach_upper_limit] = 100 - variance\n            row_values[~reach_upper_limit] = expected_value - variance / 2\n            row_values[row_values &lt; 0] = 0\n            survey_df.loc[i, expert_ids] = row_values\n        if row['title'] == 'effectiveness upper':\n            expected_value = survey_df.iloc[i-3][expert_ids]\n            variance = survey_df.iloc[i-2][expert_ids]\n            reach_lower_limit = expected_value - variance / 2 &lt; 0   # boolean array\n            row_values = survey_df.loc[i, expert_ids]\n            row_values[reach_lower_limit] = variance\n            row_values[~reach_lower_limit] = expected_value + variance / 2\n            row_values[row_values &gt; 100] = 100\n            survey_df.loc[i, expert_ids] = row_values\n\n    #\n    # Calculate probability distributions\n    #\n\n    # add a new column for the probability\n    survey_df['reduction'] = pd.Series([np.nan] * len(survey_df), dtype='object')\n\n    # access expert answer columns, separate rows by type of answer\n    expecteds = survey_df[expert_ids].loc[survey_df['title'] == 'expected value']\n    lower_boundaries = survey_df[expert_ids].loc[survey_df['title'] == 'effectiveness lower']\n    upper_boundaries = survey_df[expert_ids].loc[survey_df['title'] == 'effectiveness upper']\n    weights = survey_df.loc[survey_df['title'] == 'expert weights', np.insert(expert_ids, 0, 'block')]\n    blocks = survey_df['block'].loc[(survey_df['title'] == 'expected value')]\n    # go through each measure-activity-pressure link\n    for num in expecteds.index:\n        # access current row data and convert to 1-D arrays\n        b_id = blocks.loc[num]\n        e = expecteds.loc[num].to_numpy().astype(float)\n        l = lower_boundaries.loc[num+2].to_numpy().astype(float)\n        u = upper_boundaries.loc[num+3].to_numpy().astype(float)\n        w = weights.loc[weights['block'] == b_id, expert_ids].to_numpy().astype(float).flatten()\n        # get expert probability distribution\n        prob_dist = get_prob_dist(expecteds=e, \n                                  lower_boundaries=l, \n                                  upper_boundaries=u, \n                                  weights=w)\n\n        survey_df.at[num, 'reduction'] = prob_dist\n\n    #\n    # Remove rows and columns that are not needed anymore\n    #\n\n    for title in ['max effectiveness', 'variance', 'effectiveness lower', 'effectiveness upper', 'expert weights']:\n        survey_df = survey_df.loc[survey_df['title'] != title]\n    survey_df = survey_df.drop(columns=expert_ids)\n    survey_df = survey_df.drop(columns=['survey_id', 'title', 'block'])\n\n    #\n    # Split activity / pressure / state lists into separate rows, and reset index\n    #\n\n    for col in ['activity', 'pressure', 'state']:\n        survey_df = survey_df.explode(column=col)\n        survey_df = survey_df.reset_index(drop=True)\n\n    #\n    # Replace nan values with zeros and convert columns to integers\n    #\n\n    for column in ['measure', 'activity', 'pressure', 'state']:\n        with warnings.catch_warnings(action='ignore'):\n            survey_df[column] = survey_df[column].fillna(0)\n        survey_df[column] = survey_df[column].astype(int)\n\n    return survey_df\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.process_pressure_survey_data","title":"<code>process_pressure_survey_data(file_name)</code>","text":"<p>This method reads input from the excel file containing data about pressure contributions to states  and the changes in state required to reach required thresholds of improvement.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>path of survey excel file.</p> required <p>Returns:</p> Name Type Description <code>pressure_contributions</code> <code>DataFrame</code> <ul> <li>State: state id.</li> <li>pressure: pressure id.</li> <li>area_id: area id.</li> <li>average: average contribution of pressure.</li> <li>uncertainty: standard deviation of pressure contribution.</li> </ul> <code>thresholds</code> <code>DataFrame</code> <ul> <li>state: state id.</li> <li>area_id: area id.</li> <li>PR: reduction in state required to reach GES target.</li> <li>10: reduction in state required to reach 10 % improvement.</li> <li>25: reduction in state required to reach 25 % improvement.</li> <li>50: reduction in state required to reach 50 % improvement.</li> </ul> Source code in <code>src/som_tools.py</code> <pre><code>def process_pressure_survey_data(file_name: str) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    This method reads input from the excel file containing data about pressure contributions to states \n    and the changes in state required to reach required thresholds of improvement.\n\n    Arguments:\n        file_name (str): path of survey excel file.\n\n    Returns:\n        pressure_contributions (DataFrame):\n\n            - State: state id.\n            - pressure: pressure id.\n            - area_id: area id.\n            - average: average contribution of pressure.\n            - uncertainty: standard deviation of pressure contribution.\n\n        thresholds (DataFrame):\n\n            - state: state id.\n            - area_id: area id.\n            - PR: reduction in state required to reach GES target.\n            - 10: reduction in state required to reach 10 % improvement.\n            - 25: reduction in state required to reach 25 % improvement.\n            - 50: reduction in state required to reach 50 % improvement.\n    \"\"\"\n    #\n    # set parameter values\n    #\n\n    expert_number = 6   # max number of experts per question\n    threshold_cols = ['PR', '10', '25', '50']   # target thresholds (PR=GES)\n\n    #\n    # read information sheet from input Excel file\n    #\n\n    data = pd.read_excel(io=file_name, sheet_name=None)\n    sheet_names = list(data.keys())\n\n    psq = data[sheet_names[0]]\n\n    pressure_survey_data = {}\n    for id in range(1, len(sheet_names)):\n        pressure_survey_data[id] = data[sheet_names[id]]\n\n    #\n    # preprocess values\n    #\n\n    psq['area_id'] = [x.split(';') if type(x) == str else x for x in psq['area_id']]\n\n    # add question id column\n    psq['question_id'] = list(range(len(psq)))\n\n    #\n    # create new dataframe\n    #\n\n    survey_df = pd.DataFrame(columns=['survey_id', 'question_id', 'State', 'area_id', 'GES known', 'Weight'])\n\n    # survey columns from which to take data\n    cols = ['Expert']\n    cols += [x + str(i+1) for x in ['P', 'S'] for i in range(expert_number)]    # up to expert_number different pressures related to state, and their significance\n    cols += [x + y for x in ['MIN', 'MAX', 'ML'] for y in threshold_cols]     # required pressure reduction to reach GES (if known) or X % improvement in state\n\n    start = 0    # keep track of where to access data in psq\n\n    # for every survey sheet\n    for survey_id in pressure_survey_data:\n\n        # identify amount of experts in survey\n        expert_ids = pressure_survey_data[survey_id]['Expert'].unique()\n        # identify amount of questions in survey\n        questions = np.sum(pressure_survey_data[survey_id]['Expert'] == expert_ids[0])\n\n        # use number of questions to get state, area and GES known\n        question_id = psq['question_id'].iloc[start:start+questions].reset_index(drop=True)\n        state = psq['State'].iloc[start:start+questions].reset_index(drop=True)\n        areas = psq['area_id'].iloc[start:start+questions].reset_index(drop=True)\n        ges_known = psq['GES known'].iloc[start:start+questions].reset_index(drop=True)\n\n        # find all expert weight columns and values\n        expert_cols = [True if 'exp' in col.lower() else False for col in psq.columns]\n        expert_weights = psq.loc[start:start+questions, expert_cols].reset_index(drop=True)\n        expert_weights = expert_weights.fillna(1)\n\n        survey_answers = 0\n        for expert in expert_ids:\n\n            # select expert answers\n            data = pressure_survey_data[survey_id][cols].loc[pressure_survey_data[survey_id][cols]['Expert'] == expert].reset_index(drop=True)\n\n            # verify that the amount of answers is correct\n            if len(data) != questions: raise Exception('Not same amount of answers for each expert in survey sheet!')\n\n            survey_answers += len(data)\n\n            # set survey id, state, area and GES known for data\n            data['survey_id'] = survey_id\n            data['question_id'] = question_id\n            data['State'] = state\n            data['area_id'] = areas\n            data['GES known'] = ges_known\n\n            # set expert weights\n            data['Weight'] = expert_weights['Exp' + str(int(expert))]\n\n            # add data to final dataframe\n            with warnings.catch_warnings(action='ignore'):\n                survey_df = pd.concat([survey_df, data], ignore_index=True, sort=False)\n\n        # verify that the correct number of answers was saved\n        if survey_answers != len(expert_ids) * questions: raise Exception('Incorrect amount of answers found for survey!')\n\n        # increase counter\n        start += questions\n\n    # create new dataframe for merged rows\n    cols = ['survey_id', 'question_id', 'State', 'area_id', 'GES known']\n    new_df = pd.DataFrame(columns=cols+['Pressures', 'Contribution']+threshold_cols)\n    # remove empty elements from areas, and convert ids to integers\n    survey_df['area_id'] = survey_df['area_id'].apply(lambda x: [int(area) for area in x if area != ''])\n    # identify all unique questions\n    questions = survey_df['question_id'].unique()\n    # process each state\n    for question in questions:\n        # select current question rows\n        data = survey_df.loc[survey_df['question_id'] == question].reset_index(drop=True)\n        #\n        # pressure contributions and uncertainties\n        #\n        # select pressures and significances, and find non nan values\n        pressures = data[['P'+str(x+1) for x in range(expert_number)]].to_numpy().astype(float)\n        significances = data[['S'+str(x+1) for x in range(expert_number)]].to_numpy().astype(float)\n        mask = ~np.isnan(pressures)\n        # weigh significances by amount of participating experts\n        w = data[['Weight']].to_numpy().astype(float)\n        significances = significances * w\n        # go through each expert answer and calculate weights\n        weights = {}\n        for i, e in enumerate(pressures):\n            s_tot = np.sum(significances[i][mask[i]])\n            for p, s in zip(pressures[i][mask[i]], significances[i][mask[i]]):\n                if int(p) not in weights:\n                    weights[int(p)] = []\n                weights[int(p)].append(s / s_tot)\n        # using weights, calculate contributions and uncertainties\n        average = {p: np.mean(weights[p]) for p in weights}\n        stddev = {p: np.std(weights[p]) for p in weights}\n        # create probability distributions\n        minimum, maximum = {}, {}\n        for p in average:\n            if average[p] - stddev[p] &gt; 0:\n                if average[p] + stddev[p] &gt; 1:\n                    minimum[p] = 1.0 - stddev[p] * 2\n                    maximum[p] = 1.0\n                else:\n                    minimum[p] = average[p] - stddev[p]\n                    maximum[p] = average[p] + stddev[p]\n            else:\n                minimum[p] = 0.0\n                maximum[p] = stddev[p] * 2\n        average = {p: np.array([average[p] * 100]) for p in average}\n        minimum = {p: np.array([minimum[p] * 100]) for p in minimum}\n        maximum = {p: np.array([maximum[p] * 100]) for p in maximum}\n        contribution = {p: get_prob_dist(average[p], minimum[p], maximum[p], np.ones(len(average[p]))) for p in average}\n        # convert to lists\n        pressures = list(average.keys())\n        contribution = [contribution[p] for p in pressures]\n        #\n        # probability distributions for GES thresholds\n        #\n        reductions = {}\n        for r in threshold_cols:\n            # get min, max and ml data\n            r_min = data['MIN'+r].to_numpy().astype(float)\n            r_max = data['MAX'+r].to_numpy().astype(float)\n            r_ml = data['ML'+r].to_numpy().astype(float)\n            # get weighted cumulative probability distribution\n            dist = get_prob_dist(r_ml, r_min, r_max, w.flatten())\n            reductions[r] = dist\n        #\n        # merge processed data with dataframe\n        #\n        # create a new dataframe row and merge with new dataframe\n        data = survey_df[cols].loc[survey_df['question_id'] == question].reset_index(drop=True).iloc[0]\n        data = pd.DataFrame([data])\n        # initialize new columns\n        for c in ['Pressures', 'Contribution']+threshold_cols:\n            data[c] = np.nan\n            data[c] = data[c].astype(object)\n        # change data type to allow for lists\n        data.at[0, 'Pressures'] = pressures\n        data.at[0, 'Contribution'] = contribution\n        for r in threshold_cols:\n            data.at[0, r] = reductions[r]\n        with warnings.catch_warnings(action='ignore'):\n            new_df = pd.concat([new_df, data], ignore_index=True, sort=False)\n    #\n    # split pressures into separate rows\n    #\n    new_df = new_df.assign(pressure=[list(zip(*row)) for row in zip(new_df['Pressures'], new_df['Contribution'])])\n    new_df = new_df.explode('pressure')\n    new_df = new_df.reset_index(drop=True)\n    new_df = new_df.drop(columns=['Pressures', 'Contribution'])\n    new_df[['pressure', 'contribution']] = pd.DataFrame(new_df['pressure'].tolist())\n    #\n    # remove rows with missing data (no pressure or no thresholds)\n    #\n    new_df = new_df.loc[new_df['pressure'].notna(), :]\n    new_df = new_df[new_df[threshold_cols].notna().any(axis=1)]\n    new_df = new_df.reset_index(drop=True)\n    #\n    # make sure pressure ids are integers, remove unnecessary columns\n    #\n    new_df['pressure'] = new_df['pressure'].astype(int)\n    new_df = new_df.drop(columns=['survey_id', 'question_id', 'GES known'])\n    #\n    # split areas into separate rows\n    #\n    new_df = new_df.explode('area_id')\n    new_df = new_df.reset_index(drop=True)\n    #\n    # split new_df into two dataframes, one for pressure contributions and one for thresholds\n    #\n    pressure_contributions = pd.DataFrame(new_df.loc[:, ['State', 'pressure', 'area_id', 'contribution']])\n    thresholds = pd.DataFrame(new_df.loc[:, ['State', 'area_id'] + threshold_cols])\n    #\n    # rename capitalized columns\n    #\n    pressure_contributions = pressure_contributions.rename(columns={'State': 'state'})\n    thresholds = thresholds.rename(columns={'State': 'state'})\n\n    return pressure_contributions, thresholds\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.read_activity_contributions","title":"<code>read_activity_contributions(file_name, sheet_name)</code>","text":"<p>Reads input data of activity contributions to pressures in areas. </p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>name of source excel file name.</p> required <code>sheet_name</code> <code>str</code> <p>name of excel sheet.</p> required <p>Returns:</p> Name Type Description <code>act_to_press</code> <code>DataFrame</code> <p>dataframe containing mappings between activities and pressures.</p> Source code in <code>src/som_tools.py</code> <pre><code>def read_activity_contributions(file_name: str, sheet_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads input data of activity contributions to pressures in areas. \n\n    Arguments:\n        file_name (str): name of source excel file name.\n        sheet_name (str): name of excel sheet.\n\n    Returns:\n        act_to_press (DataFrame): dataframe containing mappings between activities and pressures.\n    \"\"\"\n    act_to_press = pd.read_excel(file_name, sheet_name=sheet_name)\n\n    # read all most likely, min and max column values into lists in new columns\n    for col, regex_str in zip(['expected', 'minimum', 'maximum'], ['ML[1-6]', 'Min[1-6]', 'Max[1-6]']):\n        act_to_press[col] = act_to_press.filter(regex=regex_str).values.tolist()\n\n    # remove all most likely, min and max columns\n    for regex_str in ['ML[1-6]', 'Min[1-6]', 'Max[1-6]']:\n        act_to_press.drop(act_to_press.filter(regex=regex_str).columns, axis=1, inplace=True)\n\n    # separate values grouped together in sheet on the same row with ';' into separate rows\n    for category in ['Activity', 'Pressure', 'area_id']:\n        act_to_press[category] = [list(filter(None, x.split(';'))) if type(x) == str else x for x in act_to_press[category]]\n        act_to_press = act_to_press.explode(category)\n        act_to_press[category] = act_to_press[category].astype(int)\n    act_to_press = act_to_press.reset_index(drop=True)\n\n    # calculate probability distributions\n    act_to_press['contribution'] = pd.Series([np.nan] * len(act_to_press), dtype='object')\n    for num in act_to_press.index:\n        # convert expert answers to array\n        expected = np.array(list(act_to_press.loc[num, ['expected']])).flatten()\n        lower = np.array(list(act_to_press.loc[num, ['minimum']])).flatten()\n        upper = np.array(list(act_to_press.loc[num, ['maximum']])).flatten()\n        weights = np.full(len(expected), 1)\n        # if boundaries are unknown, set to same as expected\n        lower[np.isnan(lower)] = expected[np.isnan(lower)]\n        upper[np.isnan(upper)] = expected[np.isnan(upper)]\n        # get probability distribution\n        act_to_press.at[num, 'contribution'] = get_prob_dist(expected, lower, upper, weights)\n\n    act_to_press = act_to_press.drop(columns=['expected', 'minimum', 'maximum'])\n\n    # rename columns\n    act_to_press = act_to_press.rename(columns={'Activity': 'activity', 'Pressure': 'pressure'})\n\n    return act_to_press\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.read_cases","title":"<code>read_cases(file_name, sheet_name)</code>","text":"<p>Reads input data for cases. Each row represents one case. </p> <p>In columns of 'ActMeas' sheet ('activities', 'pressure' and 'state') the value 0 == 'all relevant'.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>name of source excel file name.</p> required <code>sheet_name</code> <code>str</code> <p>name of excel sheet.</p> required <p>Returns:</p> Name Type Description <code>cases</code> <code>DataFrame</code> <p>case data.</p> Source code in <code>src/som_tools.py</code> <pre><code>def read_cases(file_name: str, sheet_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads input data for cases. Each row represents one case. \n\n    In columns of 'ActMeas' sheet ('activities', 'pressure' and 'state') the value 0 == 'all relevant'.\n\n    Arguments:\n        file_name (str): name of source excel file name.\n        sheet_name (str): name of excel sheet.\n\n    Returns:\n        cases (DataFrame): case data.\n    \"\"\"\n    cases = pd.read_excel(io=file_name, sheet_name=sheet_name)\n\n    assert len(cases[cases.duplicated(['ID'])]) == 0\n\n    for col in ['activity', 'pressure', 'state', 'area_id']:\n        # separate ids grouped together in sheet on the same row with ';' into separate rows\n        cases[col] = [list(filter(None, x.split(';'))) if type(x) == str else x for x in cases[col]]\n        cases = cases.explode(col)\n        # change types of split values from str to int\n        cases[col] = cases[col].astype(int)\n\n    for col in ['coverage', 'implementation']:\n        cases[col] = cases[col].astype(float)\n\n    cases = cases.reset_index(drop=True)\n\n    return cases\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.read_development_scenarios","title":"<code>read_development_scenarios(file_name, sheet_name)</code>","text":"<p>Reads input data of activity development scenarios. </p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>name of source excel file name.</p> required <code>sheet_name</code> <code>str</code> <p>name of sheet in excel file.</p> required <p>Returns:</p> Name Type Description <code>development_scenarios</code> <code>DataFrame</code> <p>dataframe containing activity development scenarios.</p> Source code in <code>src/som_tools.py</code> <pre><code>def read_development_scenarios(file_name: str, sheet_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads input data of activity development scenarios. \n\n    Arguments:\n        file_name (str): name of source excel file name.\n        sheet_name (str): name of sheet in excel file.\n\n    Returns:\n        development_scenarios (DataFrame): dataframe containing activity development scenarios.\n    \"\"\"\n    development_scenarios = pd.read_excel(file_name, sheet_name=sheet_name)\n    activity_id_col = 'Activity'\n\n    # replace nan values with 0, assuming that no value means no change\n    for category in [x for x in development_scenarios.columns if x != activity_id_col]:\n        development_scenarios.loc[np.isnan(development_scenarios[category]), category] = 0\n        development_scenarios[category] = development_scenarios[category].astype(float)\n\n    development_scenarios[activity_id_col] = development_scenarios[activity_id_col].astype(int)\n\n    # change values from percentual change to multiplier type by adding 1\n    for category in [x for x in development_scenarios.columns if x != activity_id_col]:\n        development_scenarios[category] = development_scenarios[category] + 1\n\n    # rename column\n    development_scenarios = development_scenarios.rename(columns={activity_id_col: 'activity'})\n\n    return development_scenarios\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.read_ids","title":"<code>read_ids(file_name, id_sheets)</code>","text":"<p>Reads in model component ids and descriptions from general input files.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>source excel file name containing measure, activity, pressure and state id sheets.</p> required <code>id_sheets</code> <code>dict</code> <p>should have structure {'measure': sheet_name, 'activity': sheet_name, ...}.</p> required <p>Returns:</p> Name Type Description <code>object_data</code> <code>dict</code> <p>dictionary containing measure, activity, pressure and state ids and descriptions in separate dataframes.</p> Source code in <code>src/som_tools.py</code> <pre><code>def read_ids(file_name: str, id_sheets: dict) -&gt; dict[str, dict]:\n    \"\"\"\n    Reads in model component ids and descriptions from general input files.\n\n    Arguments:\n        file_name (str): source excel file name containing measure, activity, pressure and state id sheets.\n        id_sheets (dict): should have structure {'measure': sheet_name, 'activity': sheet_name, ...}.\n\n    Returns:\n        object_data (dict): dictionary containing measure, activity, pressure and state ids and descriptions in separate dataframes.\n    \"\"\"\n    # create dicts for each category\n    object_data = {}\n    for category in id_sheets:\n        # read excel sheet into dataframe\n        df = pd.read_excel(io=file_name, sheet_name=id_sheets[category])\n        # remove non-necessary columns\n        df.drop(columns=[col for col in df.columns if col not in ['ID', category]])\n        # remove rows where id is nan or empty string\n        df = df.dropna(subset=['ID'])\n        df = df[df['ID'] != '']\n        # convert id column to integer (if not already)\n        df['ID'] = df['ID'].astype(int)\n        object_data[category] = df\n\n    return object_data\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.read_overlaps","title":"<code>read_overlaps(file_name, sheet_name)</code>","text":"<p>Reads input data of measure-measure interactions. </p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>name of source excel file name.</p> required <code>sheet_name</code> <code>str</code> <p>name of sheet in excel file.</p> required <p>Returns:</p> Name Type Description <code>overlaps</code> <code>DataFrame</code> <p>dataframe containing overlaps between individual measures.</p> Source code in <code>src/som_tools.py</code> <pre><code>def read_overlaps(file_name: str, sheet_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads input data of measure-measure interactions. \n\n    Arguments:\n        file_name (str): name of source excel file name.\n        sheet_name (str): name of sheet in excel file.\n\n    Returns:\n        overlaps (DataFrame): dataframe containing overlaps between individual measures.\n    \"\"\"\n    overlaps = pd.read_excel(file_name, sheet_name=sheet_name)\n\n    # replace nan values in ID columns with 0 and make sure they are integers\n    for category in ['Overlap', 'Pressure', 'Activity', 'Overlapping', 'Overlapped']:\n        overlaps.loc[np.isnan(overlaps[category]), category] = 0\n        overlaps[category] = overlaps[category].astype(int)\n\n    overlaps = overlaps.rename(columns={'Overlap': 'overlap', \n                                'Pressure': 'pressure', \n                                'Activity': 'activity', \n                                'Overlapping': 'overlapping', \n                                'Overlapped': 'overlapped', \n                                'Multiplier': 'multiplier'})\n\n    return overlaps\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.read_subpressures","title":"<code>read_subpressures(file_name, sheet_name)</code>","text":"<p>Reads input data of subpressures links to state pressures.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>name of source excel file name.</p> required <code>sheet_name</code> <code>str</code> <p>name of sheet in excel file.</p> required <p>Returns:</p> Name Type Description <code>subpressures</code> <code>DataFrame</code> <p>dataframe containing subpressure links.</p> Source code in <code>src/som_tools.py</code> <pre><code>def read_subpressures(file_name: str, sheet_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads input data of subpressures links to state pressures.\n\n    Arguments:\n        file_name (str): name of source excel file name.\n        sheet_name (str): name of sheet in excel file.\n\n    Returns:\n        subpressures (DataFrame): dataframe containing subpressure links.\n    \"\"\"\n    subpressures = pd.read_excel(file_name, sheet_name=sheet_name)\n\n    for col in ['Reduced pressure', 'State pressure', 'State']:\n        # separate ids grouped together in sheet on the same row with ';' into separate rows\n        subpressures[col] = [list(filter(None, x.split(';'))) if type(x) == str else x for x in subpressures[col]]\n        subpressures = subpressures.explode(col)\n        # change types of split values from str to int\n        subpressures[col] = subpressures[col].astype(int)\n\n    for col in subpressures.columns:\n        if col not in ['Reduced pressure', 'State pressure', 'State', 'Equivalence']:\n            subpressures = subpressures.drop(columns=[col])\n\n    subpressures = subpressures.reset_index(drop=True)\n\n    def assign_multiplier(equivalence):\n        if equivalence &lt;= 1:\n            return equivalence\n        elif equivalence == 2:\n            return 0\n        elif equivalence == 3:\n            return 0\n        else:\n            return 0\n\n    subpressures['Multiplier'] = subpressures['Equivalence'].apply(assign_multiplier)\n\n    # rename columns\n    subpressures = subpressures.rename(columns={'Reduced pressure': 'reduced pressure', \n                                        'State pressure': 'state pressure', \n                                        'Equivalence': 'equivalence', \n                                        'State': 'state', \n                                        'Multiplier': 'multiplier'})\n\n    return subpressures\n</code></pre>"},{"location":"modules/utilities/","title":"<code>utilities</code>","text":"<p>Small utility methods for convenience.</p>"},{"location":"modules/utilities/#src.utilities.Timer","title":"<code>Timer</code>","text":"<p>Simple timer class for determining execution time.</p> Source code in <code>src/utilities.py</code> <pre><code>class Timer:\n    \"\"\"\n    Simple timer class for determining execution time.\n    \"\"\"\n    def __init__(self) -&gt; None:\n        self.start = time.perf_counter()\n    def time_passed(self) -&gt; int:\n        \"\"\"Return time passed since start, in seconds.\"\"\"\n        return time.perf_counter() - self.start\n    def get_time(self) -&gt; tuple:\n        \"\"\"Returns durations in hours, minutes, seconds as a named tuple.\"\"\"\n        duration = self.time_passed()\n        hours = duration // 3600\n        minutes = (duration % 3600) // 60\n        seconds = duration % 60\n        PassedTime = namedtuple('PassedTime', 'hours minutes seconds')\n        return PassedTime(hours, minutes, seconds)\n    def get_duration(self) -&gt; str:\n        \"\"\"Returns a string of duration in hours, minutes, seconds.\"\"\"\n        t = self.get_time()\n        return '%d h %d min %d sec' % (t.hours, t.minutes, t.seconds)\n    def get_hhmmss(self) -&gt; str:\n        \"\"\"Returns a string of duration in hh:mm:ss format.\"\"\"\n        t = self.get_time()\n        return '[' + ':'.join(f'{int(value):02d}' for value in [t.hours, t.minutes, t.seconds]) + ']'\n    def reset(self) -&gt; None:\n        \"\"\"Reset timer to zero.\"\"\"\n        self.start = time.perf_counter()\n</code></pre>"},{"location":"modules/utilities/#src.utilities.Timer.get_duration","title":"<code>get_duration()</code>","text":"<p>Returns a string of duration in hours, minutes, seconds.</p> Source code in <code>src/utilities.py</code> <pre><code>def get_duration(self) -&gt; str:\n    \"\"\"Returns a string of duration in hours, minutes, seconds.\"\"\"\n    t = self.get_time()\n    return '%d h %d min %d sec' % (t.hours, t.minutes, t.seconds)\n</code></pre>"},{"location":"modules/utilities/#src.utilities.Timer.get_hhmmss","title":"<code>get_hhmmss()</code>","text":"<p>Returns a string of duration in hh:mm:ss format.</p> Source code in <code>src/utilities.py</code> <pre><code>def get_hhmmss(self) -&gt; str:\n    \"\"\"Returns a string of duration in hh:mm:ss format.\"\"\"\n    t = self.get_time()\n    return '[' + ':'.join(f'{int(value):02d}' for value in [t.hours, t.minutes, t.seconds]) + ']'\n</code></pre>"},{"location":"modules/utilities/#src.utilities.Timer.get_time","title":"<code>get_time()</code>","text":"<p>Returns durations in hours, minutes, seconds as a named tuple.</p> Source code in <code>src/utilities.py</code> <pre><code>def get_time(self) -&gt; tuple:\n    \"\"\"Returns durations in hours, minutes, seconds as a named tuple.\"\"\"\n    duration = self.time_passed()\n    hours = duration // 3600\n    minutes = (duration % 3600) // 60\n    seconds = duration % 60\n    PassedTime = namedtuple('PassedTime', 'hours minutes seconds')\n    return PassedTime(hours, minutes, seconds)\n</code></pre>"},{"location":"modules/utilities/#src.utilities.Timer.reset","title":"<code>reset()</code>","text":"<p>Reset timer to zero.</p> Source code in <code>src/utilities.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset timer to zero.\"\"\"\n    self.start = time.perf_counter()\n</code></pre>"},{"location":"modules/utilities/#src.utilities.Timer.time_passed","title":"<code>time_passed()</code>","text":"<p>Return time passed since start, in seconds.</p> Source code in <code>src/utilities.py</code> <pre><code>def time_passed(self) -&gt; int:\n    \"\"\"Return time passed since start, in seconds.\"\"\"\n    return time.perf_counter() - self.start\n</code></pre>"},{"location":"modules/utilities/#src.utilities.display_progress","title":"<code>display_progress(completion, size=20, text='Progress: ')</code>","text":"<p>Shows the current simulation progress as a percentage with a progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>completion</code> <code>float</code> <p>fraction representing completion.</p> required <code>size</code> <code>int</code> <p>total amount of simulations to run.</p> <code>20</code> <code>text</code> <code>str</code> <p>optional text to display before progress bas.</p> <code>'Progress: '</code> Source code in <code>src/utilities.py</code> <pre><code>def display_progress(completion: float, size: int = 20, text: str = 'Progress: '):\n    \"\"\"\n    Shows the current simulation progress as a percentage with a progress bar.\n\n    Arguments:\n        completion (float): fraction representing completion.\n        size (int): total amount of simulations to run.\n        text (str): optional text to display before progress bas.\n    \"\"\"\n    x = int(size*completion)\n    sys.stdout.write(\"%s[%s%s] %02d %%\\r\" % (text, \"#\"*x, \".\"*(size-x), completion*100))\n    sys.stdout.flush()\n</code></pre>"},{"location":"modules/utilities/#src.utilities.exception_traceback","title":"<code>exception_traceback(e, file=None)</code>","text":"<p>Format exception traceback and print it.</p> <p>Parameters:</p> Name Type Description Default <code>e</code> <code>Exception</code> <p>exception from which to print traceback.</p> required <code>file</code> <code>str</code> <p>optional file path to write to, otherwise defaults to sys.stdout.</p> <code>None</code> Source code in <code>src/utilities.py</code> <pre><code>def exception_traceback(e: Exception, file = None):\n    \"\"\"\n    Format exception traceback and print it.\n\n    Arguments:\n        e (Exception): exception from which to print traceback.\n        file (str): optional file path to write to, otherwise defaults to sys.stdout.\n    \"\"\"\n    tb = traceback.format_exception(type(e), e, e.__traceback__)\n    print(''.join(tb), file=file)\n</code></pre>"},{"location":"modules/utilities/#src.utilities.fail_with_message","title":"<code>fail_with_message(m=None, e=None, file=None, do_not_exit=False)</code>","text":"<p>Prints the given exception traceback along with given message, and exits.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>str</code> <p>optional message to print along with traceback.</p> <code>None</code> <code>e</code> <code>Exception</code> <p>exception from which to print traceback.</p> <code>None</code> <code>file</code> <code>str</code> <p>optional file path to write to, otherwise defaults to sys.stdout.</p> <code>None</code> <code>do_not_exit</code> <code>bool</code> <p>optional flag to not exit.</p> <code>False</code> Source code in <code>src/utilities.py</code> <pre><code>def fail_with_message(m: str = None, e: Exception = None, file = None, do_not_exit: bool = False):\n    \"\"\"\n    Prints the given exception traceback along with given message, and exits.\n\n    Arguments:\n        m (str): optional message to print along with traceback.\n        e (Exception): exception from which to print traceback.\n        file (str): optional file path to write to, otherwise defaults to sys.stdout.\n        do_not_exit (bool): optional flag to not exit.\n    \"\"\"\n    if e is not None:\n        exception_traceback(e, file)\n    if m is not None:\n        print(m, file=file)\n    print('Terminating.', file=file)\n    if not do_not_exit:\n        exit()\n</code></pre>"},{"location":"modules/utilities/#src.utilities.get_dist_from_picks","title":"<code>get_dist_from_picks(picks)</code>","text":"<p>Takes an array of picks and returns the probability distribution for each percentage unit. Picks need to be fractions in [0, 1].</p> <p>Parameters:</p> Name Type Description Default <code>picks</code> <code>ndarray</code> <p>array of random samples.</p> required <p>Returns:</p> Name Type Description <code>dist</code> <code>ndarray</code> <p>probability distribution.</p> Source code in <code>src/utilities.py</code> <pre><code>def get_dist_from_picks(picks: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Takes an array of picks and returns the probability distribution for each percentage unit. Picks need to be fractions in [0, 1].\n\n    Arguments:\n        picks (ndarray): array of random samples.\n\n    Returns:\n        dist (ndarray): probability distribution.\n    \"\"\"\n    picks = np.round(picks, decimals=2)\n    unique, count = np.unique(picks, return_counts=True)\n    dist = np.zeros(shape=101)  # probability distribution, each element represents a percentage from 0 - 100 %\n    # for each percentage, set its value to its frequency in the picks\n    for i in range(dist.size):\n        for k in range(unique.size):\n            if i / 100.0 == unique[k]:\n                dist[i] = count[k]\n    dist = dist / dist.sum()    # normalize frequencies to sum up to 1\n    return dist\n</code></pre>"},{"location":"modules/utilities/#src.utilities.get_pick","title":"<code>get_pick(dist)</code>","text":"<p>Makes a random pick within [0, 1] weighted by the given discrete distribution.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>ndarray</code> <p>probability distribution.</p> required <p>Returns:</p> Name Type Description <code>pick</code> <code>float</code> <p>sample from the distribution.</p> Source code in <code>src/utilities.py</code> <pre><code>def get_pick(dist: np.ndarray) -&gt; float:\n    \"\"\"\n    Makes a random pick within [0, 1] weighted by the given discrete distribution.\n\n    Arguments:\n        dist (ndarray): probability distribution.\n\n    Returns:\n        pick (float): sample from the distribution.\n    \"\"\"\n    if dist is not None:\n        step = 1 / (dist.size - 1)\n        a = np.arange(0, 1 + step, step)\n        pick = np.random.choice(a, p=dist)\n        return pick\n    else:\n        return np.nan\n</code></pre>"},{"location":"modules/utilities/#src.utilities.get_prob_dist","title":"<code>get_prob_dist(expecteds, lower_boundaries, upper_boundaries, weights)</code>","text":"<p>Returns an aggregated probability distribution from all the individual expert answers provided.  Each value in the argument arrays correspond to a PERT distribution characteristic (peak, high, low).  Each individual distribution has a weight which impacts its contribution to the final aggregated distribution.  All arguments should be 1D arrays with percentage as unit.</p> <p>Parameters:</p> Name Type Description Default <code>expecteds</code> <code>ndarray</code> <p>individual distribution peaks.</p> required <code>lower_boundaries</code> <code>ndarray</code> <p>individual distributions lows.</p> required <code>upper_boundaries</code> <code>ndarray</code> <p>individual distribution highs.</p> required <code>weights</code> <code>ndarray</code> <p>individual distribution weights.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy array</p> Source code in <code>src/utilities.py</code> <pre><code>def get_prob_dist(expecteds: np.ndarray, \n                  lower_boundaries: np.ndarray, \n                  upper_boundaries: np.ndarray, \n                  weights: np.ndarray) -&gt; np.ndarray:\n    '''\n    Returns an aggregated probability distribution from all the individual expert answers provided. \n    Each value in the argument arrays correspond to a PERT distribution characteristic (peak, high, low). \n    Each individual distribution has a weight which impacts its contribution to the final aggregated distribution. \n    All arguments should be 1D arrays with percentage as unit.\n\n    Arguments:\n        expecteds (ndarray): individual distribution peaks.\n        lower_boundaries (ndarray): individual distributions lows.\n        upper_boundaries (ndarray): individual distribution highs.\n        weights (ndarray): individual distribution weights.\n\n    Returns:\n        numpy array\n    '''\n    # verify that all arrays have the same size\n    assert expecteds.size == lower_boundaries.size == upper_boundaries.size == weights.size\n\n    #\n    # TODO: remove uncomment in future to not accept faulty data\n    # for now, sort arrays to have values in correct order\n    #\n    # # verify that all lower boundaries are lower than the upper boundaries\n    # assert np.sum(lower_boundaries &gt; upper_boundaries) == 0\n    # # verify that most likely values are between lower and upper boundaries\n    # assert np.sum((expecteds &lt; lower_boundaries) &amp; (expecteds &gt; upper_boundaries)) == 0\n    arr = np.full((len(expecteds), 3), np.nan)\n    arr[:, 0] = lower_boundaries\n    arr[:, 1] = expecteds\n    arr[:, 2] = upper_boundaries\n    arr = np.array([np.sort(row) for row in arr])\n    lower_boundaries = arr[:, 0]\n    expecteds = arr[:, 1]\n    upper_boundaries = arr[:, 2]\n\n    # select values that are not nan, bool matrix\n    non_nan = ~np.isnan(expecteds) &amp; ~np.isnan(lower_boundaries) &amp; ~np.isnan(upper_boundaries)\n    # multiply those values with weights, True = 1 and False = 0\n    weights_non_nan = (non_nan * weights)\n\n    # create a PERT distribution for each expert\n    # from each distribution, draw a large number of picks\n    # pool the picks together\n    number_of_picks = 5000\n    picks = []\n    for i in range(len(expecteds)):\n        peak = expecteds[i]\n        low = lower_boundaries[i]\n        high = upper_boundaries[i]\n        w = weights_non_nan[i]\n        if ~non_nan[i]: # note the tilde ~ to check for nan value\n            continue    # skip if any value is nan\n        dist = pert_dist(peak, low, high, w * number_of_picks)\n        picks += dist.tolist()\n\n    # return nan if no distributions (= no expert answers)\n    if len(picks) == 0:\n        return np.nan\n\n    # create final probability distribution\n    picks = np.array(picks) / 100.0   # convert percentages to fractions\n    prob_dist = get_dist_from_picks(picks)\n\n    return prob_dist\n</code></pre>"},{"location":"modules/utilities/#src.utilities.pert_dist","title":"<code>pert_dist(peak, low, high, size)</code>","text":"<p>Returns a set of random picks from a PERT distribution.</p> <p>Parameters:</p> Name Type Description Default <code>peak</code> <code>float</code> <p>distribution peak.</p> required <code>low</code> <code>float</code> <p>distribution lower tail.</p> required <code>high</code> <code>float</code> <p>distribution higher tail.</p> required <code>size</code> <code>int</code> <p>number of picks to return.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy array</p> Source code in <code>src/utilities.py</code> <pre><code>def pert_dist(peak: float, low: float, high: float, size: int) -&gt; np.ndarray:\n    '''\n    Returns a set of random picks from a PERT distribution.\n\n    Arguments:\n        peak (float): distribution peak.\n        low (float): distribution lower tail.\n        high (float): distribution higher tail.\n        size (int): number of picks to return.\n\n    Returns:\n        numpy array\n    '''\n    # weight, controls probability of edge values (higher -&gt; more emphasis on most likely, lower -&gt; extreme values more probable)\n    # 4 is standard used in unmodified PERT distributions\n    gamma = 4\n    # calculate expected value\n    # mu = ((low + gamma) * (peak + high)) / (gamma + 2)\n    if low == high and low == peak:\n        return np.full(int(size), peak)\n    r = high - low\n    alpha = 1 + gamma * (peak - low) / r\n    beta = 1 + gamma * (high - peak) / r\n    return low + np.random.default_rng().beta(alpha, beta, size=int(size)) * r\n</code></pre>"},{"location":"modules/utilities/#src.utilities.plot_dist","title":"<code>plot_dist(dist)</code>","text":"<p>Plot the given distribution</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>ndarray</code> <p>probability distribution.</p> required Source code in <code>src/utilities.py</code> <pre><code>def plot_dist(dist: np.ndarray):\n    \"\"\"\n    Plot the given distribution\n\n    Arguments:\n        dist (ndarray): probability distribution.\n    \"\"\"\n    # plot distribution\n    y_vals = dist\n    step = 1 / y_vals.size\n    x_vals = np.arange(0, 1, step)\n    plt.plot(x_vals, y_vals)\n    # verify that get_pick works\n    picks = np.array([get_pick(dist) for i in range(5000)])\n    y_vals = get_dist_from_picks(picks)\n    step = 1 / y_vals.size\n    x_vals = np.arange(0, 1, step)\n    plt.plot(x_vals, y_vals)\n    plt.show()\n</code></pre>"},{"location":"modules/utilities/#src.utilities.sanitize_string","title":"<code>sanitize_string(s)</code>","text":"<p>Makes a string valid for file and directory names.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>string to sanitize.</p> required Source code in <code>src/utilities.py</code> <pre><code>def sanitize_string(s: str):\n    \"\"\"\n    Makes a string valid for file and directory names.\n\n    Arguments:\n        s (str): string to sanitize.\n    \"\"\"\n    place_holder = '_'\n    for invalid in ['*', '\"', '/', '\\\\', '&lt;', '&gt;', ':', '|', '?']:\n        s = s.replace(invalid, place_holder)\n    s = s.strip()   # remove leading and trailing whitespace\n    return s\n</code></pre>"}]}