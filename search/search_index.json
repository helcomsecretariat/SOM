{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sufficiency of Measures (SOM)","text":"<p>Methodology developed at HELCOM. Tool available on GitHub.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>python main.py</code> - Run tool.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>src/                # Project source code.\npyproject.toml      # Project build requirements file.\nmkdocs.yml          # Documentation configuration file.\ndocs/               # Documentation resources.\nLICENSE             # Project license.\n</code></pre>"},{"location":"license/","title":"License","text":"<p>Please see the project license for further details.</p>"},{"location":"guide/configuration/","title":"Configuration","text":"<p>The <code>config.toml</code> file can be edited in a text editor and follows the TOML format. Please note that the options are case sensitive. </p> <ul> <li><code>export_path</code>: Path of the output excel file</li> <li><code>use_scenario</code>: Should an activity development scenario be applied? (true/false)</li> <li><code>scenario</code>: Which activity development scenario to use</li> <li><code>use_random_seed</code>: Should a custom seed be used for random values? (true/false)</li> <li><code>random_seed</code>: Custom random seed to use</li> <li><code>simulations</code>: Number of simulations to run</li> <li><code>use_parallel_processing</code>: Should multiprocessing be used for faster calculations? (true/false)</li> <li><code>use_legacy_input_data</code>: Choose between legacy/new input data (true/false), see Input data</li> <li><code>input_data:path</code>: Path of the new input data excel file</li> <li><code>input_data_legacy:general_input</code>: Path to legacy input data general data excel file</li> <li><code>input_data_legacy:measure_effect_input</code>: Path to legacy input data measure effects data excel file</li> <li><code>input_data_legacy:pressure_state_input</code>: Path to legacy input data pressure-state links and thresholds data excel file <code>input_data_legacy:general_input_sheets</code>: Links to the sheets in <code>input_data_legacy:general_input</code> in case of custom naming</li> </ul>"},{"location":"guide/file-structure/","title":"File structure","text":"<p>Model file structure (in <code>src/</code> directory):</p> <ul> <li><code>main.py</code>: Runs the tool</li> <li><code>config.toml</code>: Configuration settings</li> <li><code>som_app.py</code>: Main calculations are performed here</li> <li><code>som_tools.py</code>: Input data loading functions</li> <li><code>som_plots.py</code>: Plot functions for results</li> <li><code>utilities.py</code>: Small utility functions</li> </ul>"},{"location":"guide/input-data-legacy/","title":"Input data (legacy)","text":""},{"location":"guide/input-data-legacy/#intro","title":"Intro","text":"<p>(Note! This page details the previous format of the input data, that can be used with the SOM model by pre-processing it. To do so, make sure to set the <code>use_legacy_input_data</code> option to <code>true</code> in the configuration file)</p> <p>The input data consists of three files:</p> <ul> <li>exampleData.xlsx</li> <li>exampleEffect.xlsx</li> <li>examplePressureState.xlsx</li> </ul> <p>Example data has been provided in the <code>data</code> directory.</p> <p>Please note that most column names are case sensitive.</p>"},{"location":"guide/input-data-legacy/#general-input","title":"General Input","text":"<p><code>exampleData.xlsx</code> contains descriptions of the model domain:</p>"},{"location":"guide/input-data-legacy/#id-sheets","title":"ID sheets","text":""},{"location":"guide/input-data-legacy/#sheetmeasure-id","title":"<code>sheet:Measure ID</code>","text":"<ul> <li>Unique identifiers for measures</li> </ul>"},{"location":"guide/input-data-legacy/#sheetactivity-id","title":"<code>sheet:Activity ID</code>","text":"<ul> <li>Unique identifiers for activities</li> </ul>"},{"location":"guide/input-data-legacy/#sheetpressure-id","title":"<code>sheet:Pressure ID</code>","text":"<ul> <li>Unique identifiers for pressures</li> </ul>"},{"location":"guide/input-data-legacy/#sheetstate-id","title":"<code>sheet:State ID</code>","text":"<ul> <li>Unique identifiers for states</li> </ul>"},{"location":"guide/input-data-legacy/#sheetarea-id","title":"<code>sheet:Area ID</code>","text":"<ul> <li>Unique identifiers for areas</li> </ul>"},{"location":"guide/input-data-legacy/#sheetcase-id","title":"<code>sheet:Case ID</code>","text":"<ul> <li>Unique identifiers for cases</li> </ul>"},{"location":"guide/input-data-legacy/#sheetactmeas","title":"<code>sheet:ActMeas</code>","text":"<ul> <li>Implemented measure cases, all rows are independent, multiple IDs can be joined by a semi-colon.<ul> <li><code>column:ID</code>: Unique case id, linked to <code>sheet:Case ID</code></li> <li><code>column:measure</code>: Measure type ID, linked to <code>sheet:Measure ID</code></li> <li><code>column:activity</code>: Relevant Activities, linked to <code>sheet:Activity ID</code>, the value 0 (zero) means all relevant activities affected by the measure</li> <li><code>column:pressure</code>: Relevant Pressures, linked to <code>sheet:Pressure ID</code>, the value 0 (zero) means all relevant pressures affected by the measure</li> <li><code>column:state</code>: Relevant States, linked to <code>sheet:State ID</code>, the value 0 (zero) means all relevant states affected by the measure</li> <li><code>column:coverage</code>: Multiplier (fraction), represents how much of the area is covered by the measure</li> <li><code>column:implementation</code>: Multiplier (fraction), represents how much of the measure is implemented</li> <li><code>column:area_id</code>: Area ID, linked to <code>sheet:Area ID</code></li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#sheetactpres","title":"<code>sheet:ActPres</code>","text":"<ul> <li>Activity-Pressure links, how much the individual activities contribute to the pressures<ul> <li><code>column:Activity</code>: Activity ID, linked to <code>sheet:Activity ID</code></li> <li><code>column:Pressure</code>: Pressure ID, linked to <code>sheet:Pressure ID</code></li> <li><code>column:area_id</code>: Area ID, linked to <code>sheet:Area ID</code>, multiple IDs can be joined by a semi-colon</li> <li><code>column:Ml#</code>: Most likely contribution (%)</li> <li><code>column:Min#</code>: Lowest potential contribution (%)</li> <li><code>column:Max#</code>: Highest potential contribution (%)</li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#sheetdev_scenarios","title":"<code>sheet:DEV_scenarios</code>","text":"<ul> <li>Activity development scenarios<ul> <li><code>column:Activity</code>: Activity ID, linked to <code>sheet:Activity ID</code></li> <li><code>column:BAU</code>: Business As Usual, how much the activity will change without extra action (fraction)</li> <li><code>column:ChangeMin</code>: Lowest potential change (fraction)</li> <li><code>column:ChangeML</code>: Most likely change (fraction)</li> <li><code>column:ChangeMax</code>: Highest potential change (fraction)</li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#sheetoverlaps","title":"<code>sheet:Overlaps</code>","text":"<ul> <li>Interaction between separate measures, how joint implementation affects measure efficiency<ul> <li><code>column:Overlap</code>: Overlap ID</li> <li><code>column:Pressure</code>: Pressure ID, linked to <code>sheet:Pressure ID</code></li> <li><code>column:Activity</code>: Activity ID, linked to <code>sheet:Activity ID</code></li> <li><code>column:Overlapping</code>: Overlapping measure ID, linked to <code>sheet:Measure ID</code></li> <li><code>column:Overlapped</code>: Overlapped measure ID, linked to <code>sheet:Measure ID</code></li> <li><code>column:Multiplier</code>: Multiplier (fraction), how much of the <code>column:Overlapped</code> measure's effect will be observed if <code>column:Overlapping</code> is also implemented</li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#sheetsubpres","title":"<code>sheet:SubPres</code>","text":"<ul> <li>Links between separate pressures, where subpressures make up part of state pressures<ul> <li><code>column:Reduced pressure</code>: Subpressure ID, linked to <code>sheet:Pressure ID</code></li> <li><code>column:State pressure</code>: State pressure ID, linked to <code>sheet:Pressure ID</code></li> <li><code>column:Equivalence</code>: Equivalence between <code>column:Reduced pressure</code> and <code>column:State pressure</code>, i.e. how much of the state pressure is made up of the subpressure, where values between 0 and 1 are treated as fractions, and other values as either no quantified equivalence or no reduction from pressures</li> <li><code>column:State</code>: State ID, linked to <code>sheet:State ID</code></li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#measure-efficiencies","title":"Measure efficiencies","text":"<p><code>exampleEffect.xslx</code> contains survey data on the effects of measures on activity-pressure pairs as surved by expert panels:</p>"},{"location":"guide/input-data-legacy/#sheetmteq","title":"<code>sheet:MTEQ</code>","text":"<ul> <li>General information on the survey questions, each row corresponds to a unique activity-pressure pair, the value 0 (zero) for the Activity, Pressure and State columns is used to denote no value, used for direct to pressure / direct to state measures<ul> <li><code>column:Survey ID</code>: Survey ID, each unique id corresponds to a specific sheet in <code>exampleEffect.xslx</code></li> <li><code>column:Activity</code>: Activity ID, linked to <code>exampleData.xlsx:Activity ID</code></li> <li><code>column:Pressure</code>: Pressure ID, linked to <code>exampleData.xlsx:Pressure ID</code></li> <li><code>column:State</code>: State ID, linked to <code>exampleData.xlsx:State ID</code></li> <li><code>column:AMT</code>: Amount of measures linked to the activity-pressure pair in the corresponding survey sheet</li> <li><code>column:Exp#</code>: Expert columns, details the number of experts that gave each answer, used for weighting</li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#sheetsurveys","title":"<code>sheet:Surveys</code>","text":"<ul> <li>Survey sheets detailing the effects of the measures on the activity-pressure pairs in <code>sheet:MTEQ</code><ul> <li><code>column:expert ID</code>: Expert ID, linked to the corresponding expert columns in <code>sheet:MTEQ</code></li> <li><code>column:#</code>: Measure IDs as columns, linked to <code>exampleData.xlsx:Measure ID</code>, each measure takes two columns<ul> <li>the first column describes the most likely reduction (%) of the measure on the activity-pressure pair</li> <li>the second column describes the potential uncertainty range (%) regarding the reduction</li> </ul> </li> <li><code>column:ME</code>: The actual effect of the most effective measure for the current activity-pressure pair</li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#pressure-contributions-and-ges-thresholds","title":"Pressure contributions and GES thresholds","text":"<p><code>examplePressureState.xlsx</code> contains survey data on pressure contributions to states and total pressure load reduction targets:</p>"},{"location":"guide/input-data-legacy/#sheetpsq","title":"<code>sheet:PSQ</code>","text":"<ul> <li>General information on the survey questions, each row corresponds to a unique state-area pair<ul> <li><code>column:State</code>: State ID, linked to <code>exampleData.xlsx:State ID</code></li> <li><code>column:area_id</code>: Area ID, linked to <code>exampleData.xlsx:Area ID</code>, multiple IDs can be joined by a semi-colon</li> <li><code>column:GES known</code>: Is the GES threshold known, 0 for no, 1 for yes</li> <li><code>column:Exp#</code>: Expert columns, details the number of experts that gave each answer, used for weighting</li> </ul> </li> </ul>"},{"location":"guide/input-data-legacy/#sheetsurveys_1","title":"<code>sheet:Surveys</code>","text":"<ul> <li>Survey sheets detailing the contributions of individual pressures to states and the total pressure load reduction targets for the state, the targets are for PR (=GES), 10 %, 25 % and 50 % improvement in state<ul> <li><code>column:Expert</code>: Expert ID, linked to the corresponding expert columns in <code>sheet:PSQ</code>, each expert's answers comprise a block of rows corresponding to the state-area pair rows in <code>sheet:PSQ</code></li> <li><code>column:P#</code>: Pressure IDs, linked to <code>exampleData.xlsx:Pressure ID</code></li> <li><code>column:S#</code>: Significance of corresponding <code>column:P#</code>, used when weighing contributions of each pressure</li> <li><code>column:MIN#</code>: Lowest potential threshold value (%)</li> <li><code>column:MAX#</code>: Highest potential threshold value (%)</li> <li><code>column:ML#</code>: Most likely threshold value (%)</li> </ul> </li> </ul>"},{"location":"guide/input-data/","title":"Input data","text":""},{"location":"guide/input-data/#intro","title":"Intro","text":"<p>The new input data consists of one file:</p> <ul> <li>exampleInputData.xlsx</li> </ul> <p>Example input data has been provided in the <code>data</code> directory.</p> <p>For the previous version of the input data, see Input data (legacy).</p> <p>Please note that most column names are case sensitive.</p>"},{"location":"guide/input-data/#probability-distributions","title":"Probability Distributions","text":"<p>When the input data contains probability distributions, they will follow this format:</p> <ul> <li>The distribution is represented as a string</li> <li>The string is enclosed by square brackets [ ]</li> <li>Entries are space-separated</li> <li>Each entry in the distribution is the probability for a random pick to be within that discrete interval within the range [0, 1], where the distance is determined by the total number of entries in the distribution, such that the first and last entries represent 0 % and 100 % and each step in between is 100 / (N - 1).</li> </ul>"},{"location":"guide/input-data/#data-structure","title":"Data Structure","text":""},{"location":"guide/input-data/#sheetmeasure","title":"<code>sheet:measure</code>","text":"<ul> <li>Unique identifiers for measures</li> </ul>"},{"location":"guide/input-data/#sheetactivity","title":"<code>sheet:activity</code>","text":"<ul> <li>Unique identifiers for activities</li> </ul>"},{"location":"guide/input-data/#sheetpressure","title":"<code>sheet:pressure</code>","text":"<ul> <li>Unique identifiers for pressures</li> </ul>"},{"location":"guide/input-data/#sheetstate","title":"<code>sheet:state</code>","text":"<ul> <li>Unique identifiers for states</li> </ul>"},{"location":"guide/input-data/#sheetarea","title":"<code>sheet:area</code>","text":"<ul> <li>Unique identifiers for areas</li> </ul>"},{"location":"guide/input-data/#sheetcases","title":"<code>sheet:cases</code>","text":"<ul> <li>Implemented measure cases, all rows are independent<ul> <li><code>column:ID</code>: Unique case id</li> <li><code>column:measure</code>: Measure type ID, linked to <code>sheet:measure</code></li> <li><code>column:activity</code>: Relevant Activities, linked to <code>sheet:activity</code>, the value 0 (zero) means all relevant activities affected by the measure</li> <li><code>column:pressure</code>: Relevant Pressures, linked to <code>sheet:pressure</code>, the value 0 (zero) means all relevant pressures affected by the measure</li> <li><code>column:state</code>: Relevant States, linked to <code>sheet:state</code>, the value 0 (zero) means all relevant states affected by the measure</li> <li><code>column:coverage</code>: Multiplier (fraction), represents how much of the area is covered by the measure</li> <li><code>column:implementation</code>: Multiplier (fraction), represents how much of the measure is implemented</li> <li><code>column:area_id</code>: Area ID, linked to <code>sheet:Area ID</code></li> </ul> </li> </ul>"},{"location":"guide/input-data/#sheetmeasure_effects","title":"<code>sheet:measure_effects</code>","text":"<ul> <li>Activity-Pressure links, how much the individual activities contribute to the pressures<ul> <li><code>column:Activity</code>: Activity ID, linked to <code>sheet:activity</code></li> <li><code>column:Pressure</code>: Pressure ID, linked to <code>sheet:pressure</code></li> <li><code>column:area_id</code>: Area ID, linked to <code>sheet:area</code></li> <li><code>column:contribution</code>: Measure reduction effect, probability distribution, see Probability Distributions</li> </ul> </li> </ul>"},{"location":"guide/input-data/#sheetactivity_contributions","title":"<code>sheet:activity_contributions</code>","text":"<ul> <li>Measure reduction effects on activity-pressure pairs<ul> <li><code>column:measure</code>: Activity ID, linked to <code>sheet:measure</code></li> <li><code>column:activity</code>: Activity ID, linked to <code>sheet:activity</code></li> <li><code>column:pressure</code>: Pressure ID, linked to <code>sheet:pressure</code></li> <li><code>column:state</code>: Pressure ID, linked to <code>sheet:state</code></li> <li><code>column:probability</code>: Activity contribution, probability distribution, see Probability Distributions</li> </ul> </li> </ul>"},{"location":"guide/input-data/#sheetpressure_contributions","title":"<code>sheet:pressure_contributions</code>","text":"<ul> <li>Pressure-State links, how much the individual pressures contribute to the states<ul> <li><code>column:State</code>: State ID, linked to <code>sheet:state</code></li> <li><code>column:pressure</code>: Pressure ID, linked to <code>sheet:pressure</code></li> <li><code>column:area_id</code>: Area ID, linked to <code>sheet:area</code></li> <li><code>column:contribution</code>: Pressure contribution, probability distribution, see Probability Distributions</li> </ul> </li> </ul>"},{"location":"guide/input-data/#sheetthresholds","title":"<code>sheet:thresholds</code>","text":"<ul> <li>Environmental target thresholds, how much the individual states need to be reduced to reach the set targets<ul> <li><code>column:State</code>: State ID, linked to <code>sheet:state</code></li> <li><code>column:area_id</code>: Area ID, linked to <code>sheet:area</code></li> <li><code>column:PR/10/25/50</code>: Probability distributions for GES (PR) and 10/25/50 % reduction thresholds, see Probability Distributions</li> </ul> </li> </ul>"},{"location":"guide/input-data/#sheetoverlaps","title":"<code>sheet:overlaps</code>","text":"<ul> <li>Interaction between separate measures, how joint implementation affects measure efficiency<ul> <li><code>column:Overlap</code>: Overlap ID</li> <li><code>column:Pressure</code>: Pressure ID, linked to <code>sheet:pressure</code></li> <li><code>column:Activity</code>: Activity ID, linked to <code>sheet:activity</code></li> <li><code>column:Overlapping</code>: Overlapping measure ID, linked to <code>sheet:measure</code></li> <li><code>column:Overlapped</code>: Overlapped measure ID, linked to <code>sheet:measure</code></li> <li><code>column:Multiplier</code>: Multiplier (fraction), how much of the <code>column:Overlapped</code> measure's effect will be observed if <code>column:Overlapping</code> is also implemented</li> </ul> </li> </ul>"},{"location":"guide/input-data/#sheetdevelopment_scenarios","title":"<code>sheet:development_scenarios</code>","text":"<ul> <li>Activity development scenarios, how much each activity is expected to change during various scenarios, each value is a multiplier<ul> <li><code>column:Activity</code>: Activity ID, linked to <code>sheet:activity</code></li> <li><code>column:BAU</code>: Business As Usual, activity change without extra action (fraction)</li> <li><code>column:ChangeMin</code>: Lowest potential change (fraction)</li> <li><code>column:ChangeML</code>: Most likely change (fraction)</li> <li><code>column:ChangeMax</code>: Highest potential change (fraction)</li> </ul> </li> </ul>"},{"location":"guide/input-data/#sheetsubpressures","title":"<code>sheet:subpressures</code>","text":"<ul> <li>Links between separate pressures, where subpressures make up part of state pressures<ul> <li><code>column:Reduced pressure</code>: Subpressure ID, linked to <code>sheet:pressure</code></li> <li><code>column:State pressure</code>: State pressure ID, linked to <code>sheet:pressure</code></li> <li><code>column:Equivalence</code>: Equivalence between <code>column:Reduced pressure</code> and <code>column:State pressure</code>, i.e. how much of the state pressure is made up of the subpressure, where values between 0 and 1 are treated as fractions, and other values as either no quantified equivalence or no reduction from pressures</li> <li><code>column:State</code>: State ID, linked to <code>sheet:state</code></li> <li><code>column:Multiplier</code>: Multiplier (fraction), how much of the reduction in <code>column:Reduced pressure</code> should be applied to <code>column:State pressure</code>, determined from <code>column:Equivalence</code></li> </ul> </li> </ul>"},{"location":"guide/installation/","title":"Installation","text":""},{"location":"guide/installation/#download-the-model","title":"Download the model","text":"<p>The code can be downloaded from the repository (steps highlighted in red).</p> <p> </p> <p>Once downloaded, unzip the archive into your project directory.</p>"},{"location":"guide/installation/#installing-requirements","title":"Installing requirements","text":""},{"location":"guide/installation/#python","title":"Python","text":"<p>Running the SOM tool requires Python version 3.12 or above, which can be downloaded here. Follow the installer instructions to set it up. </p>"},{"location":"guide/installation/#setting-up-the-environment","title":"Setting up the environment","text":"<p>Open up a terminal and enter the following:</p> <ol> <li> <p>Navigate to your directory</p> <p><code>cd \"/path/to/som\"</code></p> </li> <li> <p>Create a new python environment (optional):</p> <p><code>python -m venv . source bin/activate</code></p> </li> <li> <p>Install dependencies:</p> <p><code>python -m pip install .</code></p> </li> </ol>"},{"location":"guide/installation/#running-the-tool","title":"Running the tool","text":"<p>To run the tool from the terminal:</p> <pre><code>python \"/path/to/som/src\"\n</code></pre> <p>Alternatively, see Using the tool.</p>"},{"location":"guide/using-the-tool/","title":"Using the tool","text":""},{"location":"guide/using-the-tool/#preparation","title":"Preparation","text":"<p>Before running the tool, make sure you have all dependencies installed (see Installation).</p> <p>Your input data should be inside the <code>data</code> directory (or alternatively, edit the <code>config.toml</code> file).</p> <p>Set the settings in the <code>config.toml</code> file to your preferences, see Configuration.</p>"},{"location":"guide/using-the-tool/#running-the-tool","title":"Running the tool","text":"<p>The tool can be executed by running the <code>SOM_Launcher.bat</code> app from the main directory. </p> <p>Alternatively, navigate to the <code>src</code> directory and run the <code>main.py</code> directly:</p> <pre><code>cd \"/path/to/som/src\"\npython main.py\n</code></pre>"},{"location":"guide/using-the-tool/#results","title":"Results","text":"<p>The results matrices are saved to the excel file set in <code>config.toml</code>. The individual simulation round results are saved to the <code>sim_res</code> folder. Plots are saved to the <code>output</code> folder. </p>"},{"location":"guide/welcome/","title":"Welcome","text":"<p>Welcome to the SOM tool user guide!</p> <p>Feel free to explore the various topics from the table of contents.</p>"},{"location":"modules/som_app/","title":"<code>som_app</code>","text":"<p>Copyright (c) 2024 Baltic Marine Environment Protection Commission</p> <p>LICENSE available under  local: 'SOM/protect_baltic/LICENSE' url: 'https://github.com/helcomsecretariat/SOM/blob/main/protect_baltic/LICENCE'</p>"},{"location":"modules/som_app/#src.som_app.build_cases","title":"<code>build_cases(data)</code>","text":"<p>Builds cases.</p> Source code in <code>src/som_app.py</code> <pre><code>def build_cases(data: dict[str, pd.DataFrame]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Builds cases.\n    \"\"\"\n    cases = data['cases']\n    links = data['measure_effects']\n    # replace all zeros (0) in activity / pressure / state columns with full list of values\n    # filter those lists to only include relevant IDs (from links)\n    # finally explode to only have single IDs per row\n    cols = ['activity', 'pressure', 'state']\n    for col in cols:\n        cases[col] = cases[col].astype(object)\n    for i, row in cases.iterrows():\n        maps_links = links.loc[links['measure'] == row['measure'], cols]    # select relevant measure/activity/pressure/state links\n        if len(maps_links) == 0:\n            cases.drop(i, inplace=True) # drop rows where measure has no effect\n            continue\n        for col in cols:\n            cases.at[i, col] = maps_links[col].unique().tolist() if row[col] == 0 else row[col]\n    for col in cols:\n        cases = cases.explode(col)\n\n    cases = cases.reset_index(drop=True)\n\n    # filter out links that don't have associated reduction\n    m = cases['measure'].isin(links['measure'])\n    a = cases['activity'].isin(links['activity'])\n    p = cases['pressure'].isin(links['pressure'])\n    s = cases['state'].isin(links['state'])\n    existing_links = (m &amp; a &amp; p &amp; s)\n    cases = cases.loc[existing_links, :]\n\n    cases = cases.reset_index(drop=True)\n\n    # remove duplicate measures in areas, measure with highest coverage and implementation is chosen\n    cases = cases.sort_values(by=['coverage', 'implementation'], ascending=[False, False])\n    cases = cases.drop_duplicates(subset=['measure', 'activity', 'pressure', 'state', 'area_id'], keep='first')\n    cases = cases.reset_index(drop=True)\n\n    data['cases'] = cases\n\n    return data\n</code></pre>"},{"location":"modules/som_app/#src.som_app.build_changes","title":"<code>build_changes(data, time_steps=1, warnings=False)</code>","text":"<p>Simulate the reduction in activities and pressures caused by measures and  return the change observed in state.</p> Source code in <code>src/som_app.py</code> <pre><code>def build_changes(data: dict[str, pd.DataFrame], time_steps: int = 1, warnings = False) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Simulate the reduction in activities and pressures caused by measures and \n    return the change observed in state. \n    \"\"\"\n    # this variable is used in assertions where float number error might affect comparisons\n    allowed_error = 0.00001     \n\n    cases = data['cases']\n    links = data['measure_effects']\n    areas = data['area']['ID']\n\n    # create dataframes to store changes in pressure and state, one column per area_id\n    # NOTE: the DataFrames are created on one line to avoid PerformanceWarning\n\n    # represents the amount of the pressure ('ID' column) that is left\n    # 1 = unchanged pressure, 0 = no pressure left\n    pressure_levels = pd.DataFrame(data['pressure']['ID']).reindex(columns=['ID']+areas.tolist()).fillna(1.0)\n    # represents the amount of the total pressure load that is left affecting the given state ('ID' column)\n    # 1 = unchanged pressure load, 0 = no pressure load left affecting the state\n    total_pressure_load_levels = pd.DataFrame(data['state']['ID']).reindex(columns=['ID']+areas.tolist()).fillna(1.0)\n\n    # represents the reduction observed in the total pressure load ('ID' column)\n    total_pressure_load_reductions = pd.DataFrame(data['state']['ID']).reindex(columns=['ID']+areas.tolist()).fillna(0.0)\n\n    # same as pressure_levels, but one dataframe for each separate state, so that state specific reductions on the pressures are captured\n    state_pressure_levels = {s: pd.DataFrame(data['pressure']['ID']).reindex(columns=['ID']+areas.tolist()).fillna(1.0) for s in data['state']['ID']}\n\n    # make sure activity contributions don't exceed 100 %\n    for area in areas:\n        for p_i, p in pressure_levels.iterrows():\n            mask = (data['activity_contributions']['area_id'] == area) &amp; (data['activity_contributions']['Pressure'] == p['ID'])\n            relevant_contributions = data['activity_contributions'].loc[mask, :]\n            if len(relevant_contributions) &gt; 0:\n                contribution_sum = relevant_contributions['contribution'].sum()\n                if contribution_sum &gt; 1:\n                    data['activity_contributions'].loc[mask, 'contribution'] = relevant_contributions['contribution'] / contribution_sum\n            try: assert data['activity_contributions'].loc[mask, 'contribution'].sum() &lt;= 1 + allowed_error\n            except Exception as e: fail_with_message(f'Failed to verify that activity contributions do not exceed 100 % for area {area}, pressure {p[\"ID\"]} with contribution sum {data['activity_contributions'].loc[mask, 'contribution'].sum()}', e)\n\n    # make sure pressure contributions don't exceed 100 %\n    for area in areas:\n        for s_i, s in total_pressure_load_levels.iterrows():\n            mask = (data['pressure_contributions']['area_id'] == area) &amp; (data['pressure_contributions']['State'] == s['ID'])\n            relevant_contributions = data['pressure_contributions'].loc[mask, :]\n            if len(relevant_contributions) &gt; 0:\n                contribution_sum = relevant_contributions['contribution'].sum()\n                if contribution_sum &gt; 1:\n                    data['pressure_contributions'].loc[mask, 'contribution'] = relevant_contributions['contribution'] / contribution_sum\n            try: assert data['pressure_contributions'].loc[mask, 'contribution'].sum() &lt;= 1 + allowed_error\n            except Exception as e: fail_with_message(f'Failed to verify that pressure contributions do not exceed 100 % for area {area}, state {s[\"ID\"]} with contribution sum {data['pressure_contributions'].loc[mask, 'contribution'].sum()}', e)\n\n    #\n    # simulation loop\n    #\n\n    for time_step in range(time_steps):\n\n        #\n        # pressure reductions\n        #\n\n        # activity contributions\n        for area in areas:\n            c = cases.loc[cases['area_id'] == area, :]  # select cases for current area\n            for p_i, p in pressure_levels.iterrows():\n                relevant_measures = c.loc[c['pressure'] == p['ID'], :]  # select all measures affecting the current pressure in the current area\n                relevant_overlaps = data['overlaps'].loc[data['overlaps']['Pressure'] == p['ID'], :]    # select all overlaps affecting current pressure\n                for m_i, m in relevant_measures.iterrows():\n                    #\n                    # get measure effect (= reduction), and apply modifiers\n                    #\n                    mask = (links['measure'] == m['measure']) &amp; (links['activity'] == m['activity']) &amp; (links['pressure'] == m['pressure']) &amp; (links['state'] == m['state'])\n                    row = links.loc[mask, :]    # find the reduction of the current measure implementation\n                    if len(row) == 0:\n                        if warnings: print(f'WARNING! Effect of measure {m[\"measure\"]} on activity {m[\"activity\"]} and pressure {m[\"pressure\"]} not known! Measure {m[\"measure\"]} will be skipped in area {area}.')\n                        continue    # skip measure if data on the effect is not known\n                    try: assert len(row) == 1\n                    except Exception as e: fail_with_message(f'ERROR! Multiple instances of measure {m[\"measure\"]} effect on activity {m[\"activity\"]} and pressure {m[\"pressure\"]} given in input data!', e)\n                    reduction = row['reduction'].values[0]\n                    for mod in ['coverage', 'implementation']:\n                        reduction = reduction * m[mod]\n                    #\n                    # overlaps (measure-measure interaction)\n                    #\n                    for o_i, o in relevant_overlaps.loc[(relevant_overlaps['Overlapped'] == m['measure']) &amp; (relevant_overlaps['Activity'] == m['activity']), :].iterrows():\n                        if o['Overlapping'] in relevant_measures.loc[relevant_measures['activity'] == m['activity'], 'measure'].values: # ensure the overlapping measure is also for the current activity\n                            reduction = reduction * o['Multiplier']\n                    #\n                    # contribution\n                    #\n                    if m['activity'] == 0:\n                        contribution = 1    # if activity is 0 (= straight to pressure), contribution will be 1\n                    else:\n                        cont_mask = (data['activity_contributions']['Activity'] == m['activity']) &amp; (data['activity_contributions']['Pressure'] == m['pressure']) &amp; (data['activity_contributions']['area_id'] == area)\n                        contribution = data['activity_contributions'].loc[cont_mask, 'contribution']\n                        if len(contribution) == 0:\n                            if warnings: print(f'WARNING! Contribution of activity {m[\"activity\"]} to pressure {m[\"pressure\"]} not known! Measure {m[\"measure\"]} will be skipped in area {area}.')\n                            continue    # skip measure if activity is not in contribution list\n                        else:\n                            try: assert len(contribution) == 1\n                            except Exception as e: fail_with_message(f'ERROR! Multiple instances of activity {m[\"activity\"]} contribution on pressure {m[\"pressure\"]} given in input data!', e)\n                            contribution = contribution.values[0]\n                    #\n                    # reduce pressure\n                    #\n                    pressure_levels.at[p_i, area] = pressure_levels.at[p_i, area] * (1 - reduction * contribution)\n                    if pressure_levels.at[p_i, area] &lt; 0:\n                        print(f'area {area}, pressure {p[\"ID\"]} =&gt; level = {pressure_levels.at[p_i, area]}, red = {reduction}, cont = {contribution}')\n                    #\n                    # normalize activity contributions to reflect pressure reduction\n                    #\n                    if abs(1 - contribution) &gt; allowed_error and contribution != 0:     # only normalize if there is change in contributions\n                        data['activity_contributions'].loc[cont_mask, 'contribution'] = contribution * (1 - reduction)   # reduce the current contribution before normalizing\n                        norm_mask = (data['activity_contributions']['area_id'] == area) &amp; (data['activity_contributions']['Pressure'] == p['ID'])\n                        relevant_contributions = data['activity_contributions'].loc[norm_mask, 'contribution']\n                        data['activity_contributions'].loc[norm_mask, 'contribution'] = relevant_contributions / (1 - reduction * contribution)\n\n        #\n        # total pressure load reductions\n        #\n\n        # straight to state measures\n        for area in areas:\n            c = cases.loc[cases['area_id'] == area, :]  # select cases for current area\n            for s_i, s in total_pressure_load_levels.iterrows():\n                relevant_measures = c.loc[c['state'] == s['ID'], :] # select all measures affecting current state in current the area\n                for m_i, m in relevant_measures.iterrows():\n                    #\n                    # get measure effect (= reduction), and apply modifiers\n                    #\n                    mask = (links['measure'] == m['measure']) &amp; (links['activity'] == m['activity']) &amp; (links['pressure'] == m['pressure']) &amp; (links['state'] == m['state'])\n                    row = links.loc[mask, :]\n                    if len(row) == 0:\n                        continue\n                    reduction = row['reduction'].values[0]\n                    for mod in ['coverage', 'implementation']:\n                        reduction = reduction * m[mod]\n                    #\n                    # overlaps (measure-measure interaction)\n                    #\n                    for o_i, o in data['overlaps'].loc[(data['overlaps']['Overlapped'] == m['measure']) &amp; (data['overlaps']['Activity'] == m['activity']) &amp; (data['overlaps']['Pressure'] == m['pressure']), :].iterrows():\n                        if o['Overlapping'] in relevant_measures['measure'].values:\n                            reduction = reduction * o['Multiplier']\n                    #\n                    # reduce pressure\n                    #\n                    total_pressure_load_levels.at[s_i, area] = total_pressure_load_levels.at[s_i, area] * (1 - reduction)\n\n        # update state pressures from pressure levels\n        for s_i, s in total_pressure_load_levels.iterrows():\n            state_pressure_levels[s['ID']].loc[:, :] = pressure_levels.loc[:, :]\n\n        # pressure contributions\n        for area in areas:\n            for s_i, s in total_pressure_load_levels.iterrows():    # for each state\n                a_i = pressure_levels.columns.get_loc(area)     # column index of current area column\n                relevant_pressures = data['pressure_contributions'].loc[(data['pressure_contributions']['area_id'] == area) &amp; (data['pressure_contributions']['State'] == s['ID']), :]  # select contributions of pressures affecting current state in current area\n                for p_i, p in relevant_pressures.iterrows():\n                    #\n                    # main pressure reduction\n                    #\n                    row_i = pressure_levels.loc[pressure_levels['ID'] == p['pressure']].index[0]\n                    reduction = 1 - pressure_levels.iloc[row_i, a_i]    # reduction = 100 % - the part that is left of the pressure\n                    contribution = data['pressure_contributions'].loc[(data['pressure_contributions']['area_id'] == area) &amp; (data['pressure_contributions']['State'] == s['ID']) &amp; (data['pressure_contributions']['pressure'] == p['pressure']), 'contribution'].values[0]\n                    #\n                    # subpressures\n                    #\n                    relevant_subpressures = data['subpressures'].loc[(data['subpressures']['State'] == s['ID']) &amp; (data['subpressures']['State pressure'] == p['pressure']), :]     # find all rows where the current pressure acts as a state pressure for the current state\n                    for sp_i, sp in relevant_subpressures.iterrows():   # for each subpressure of the current pressure\n                        sp_row_i = pressure_levels.loc[pressure_levels['ID'] == sp['Reduced pressure']].index[0]\n                        multiplier = sp['Multiplier']   # by how much does the subpressure affect the current pressure\n                        red = 1 - pressure_levels.iloc[sp_row_i, a_i]    # subpressure reduction = 100 % - the part that is left of the subpressure\n                        reduction = reduction + multiplier * red    # the new current pressure reduction is increased by the calculated subpressure reduction\n                    try: assert reduction &lt;= 1 + allowed_error\n                    except Exception as e: fail_with_message(f'Failed on area {area}, state {s[\"ID\"]}, pressure {p[\"pressure\"]} with reduction {reduction}', e)\n                    state_pressure_levels[s['ID']].iloc[row_i, a_i] = state_pressure_levels[s['ID']].iloc[row_i, a_i] * (1 - reduction)\n                    #\n                    # reduce total pressure load\n                    #\n                    total_pressure_load_levels.at[s_i, area] = total_pressure_load_levels.at[s_i, area] * (1 - reduction * contribution)\n                    #\n                    # normalize pressure contributions to reflect pressure reduction\n                    #\n                    if abs(1 - contribution) &gt; allowed_error and contribution != 0:     # only normalize if there is change in contributions\n                        data['pressure_contributions'].loc[(data['pressure_contributions']['area_id'] == area) &amp; (data['pressure_contributions']['State'] == s['ID']) &amp; (data['pressure_contributions']['pressure'] == p['pressure']), 'contribution'] = contribution * (1 - reduction)   # reduce the current contribution before normalizing\n                        norm_mask = (data['pressure_contributions']['area_id'] == area) &amp; (data['pressure_contributions']['State'] == s['ID'])\n                        relevant_contributions = data['pressure_contributions'].loc[norm_mask, 'contribution']\n                        data['pressure_contributions'].loc[norm_mask, 'contribution'] = relevant_contributions / (1 - reduction * contribution)\n                        try: assert abs(1 - data['pressure_contributions'].loc[norm_mask, 'contribution'].sum()) &lt;= allowed_error\n                        except Exception as e: fail_with_message(f'Failed on area {area}, state {s[\"ID\"]}, pressure {p[\"pressure\"]} with pressure contribution sum not equal to 1', e)\n\n    # total reduction observed in total pressure loads\n    for area in areas:\n        for s_i, s in total_pressure_load_levels.iterrows():\n            total_pressure_load_reductions.at[s_i, area] = 1 - total_pressure_load_levels.at[s_i, area]\n\n    # GES thresholds\n    cols = ['PR', '10', '25', '50']\n    thresholds = {}\n    for col in cols:\n        thresholds[col] = pd.DataFrame(data['state']['ID']).reindex(columns=['ID']+areas.tolist())\n    for area in areas:\n        a_i = total_pressure_load_levels.columns.get_loc(area)\n        for s_i, s in total_pressure_load_levels.iterrows():\n            row = data['thresholds'].loc[(data['thresholds']['State'] == s['ID']) &amp; (data['thresholds']['area_id'] == area), cols]\n            if len(row) == 0:\n                continue\n            for col in cols:\n                thresholds[col].iloc[s_i, a_i] = row.loc[:, col].values[0]\n\n    data.update({\n        'pressure_levels': pressure_levels, \n        'state_pressure_levels': state_pressure_levels, \n        'total_pressure_load_levels': total_pressure_load_levels, \n        'total_pressure_load_reductions': total_pressure_load_reductions, \n        'thresholds': thresholds\n    })\n\n    return data\n</code></pre>"},{"location":"modules/som_app/#src.som_app.build_input","title":"<code>build_input(config)</code>","text":"<p>Loads input data. If loading already processed data, probability distributions need to be converted back to arrays.</p> Source code in <code>src/som_app.py</code> <pre><code>def build_input(config: dict) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Loads input data. If loading already processed data, probability distributions need to be converted back to arrays. \n    \"\"\"\n    path = os.path.realpath(config['input_data']['path'])\n    if not os.path.isfile(path): path = os.path.join(os.path.dirname(os.path.realpath(__file__)), config['input_data']['path'])\n    if config['use_legacy_input_data']:\n        input_data = process_input_data(config)\n        with pd.ExcelWriter(path) as writer:\n            for key in input_data:\n                input_data[key].to_excel(writer, sheet_name=key, index=False)\n    else:\n        input_data = pd.read_excel(io=path, sheet_name=None)\n        conversion_sheet = [\n            ('measure_effects', 'reduction'), \n            ('activity_contributions', 'contribution'), \n            ('pressure_contributions', 'contribution'), \n            ('thresholds', 'PR'), \n            ('thresholds', '10'), \n            ('thresholds', '25'), \n            ('thresholds', '50')\n        ]\n        def str_to_arr(s):\n            if type(s) is float: return s\n            arr = []\n            for a in [x for x in s.replace('[', '').replace(']', '').split(' ')]:\n                if a != '':\n                    arr.append(a)\n            arr = np.array(arr)\n            arr = arr.astype(float)\n            arr = arr / np.sum(arr)\n            return arr\n        for sheet in conversion_sheet:\n            input_data[sheet[0]][sheet[1]] = input_data[sheet[0]][sheet[1]].apply(str_to_arr)\n    return input_data\n</code></pre>"},{"location":"modules/som_app/#src.som_app.build_links","title":"<code>build_links(data)</code>","text":"<p>Build links by picking random samples using probability distributions.</p> Source code in <code>src/som_app.py</code> <pre><code>def build_links(data: dict[str, pd.DataFrame]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Build links by picking random samples using probability distributions.\n    \"\"\"\n    #\n    # measure effects\n    #\n\n    # verify that there are no duplicate links\n    try: assert len(data['measure_effects'][data['measure_effects'].duplicated(['measure', 'activity', 'pressure', 'state'])]) == 0\n    except Exception as e: fail_with_message(f'Duplicate measure effects in input data!', e)\n\n    # get picks from cumulative distribution\n    data['measure_effects']['reduction'] = data['measure_effects']['reduction'].apply(get_pick)\n\n    #\n    # activity contributions\n    #\n\n    data['activity_contributions']['contribution'] = data['activity_contributions']['contribution'].apply(get_pick)\n\n    #\n    # pressure contributions\n    #\n\n    # get picks from cumulative distribution\n    data['pressure_contributions']['contribution'] = data['pressure_contributions']['contribution'].apply(lambda x: get_pick(x) if not np.any(np.isnan(x)) else np.nan)\n\n    data['pressure_contributions'] = data['pressure_contributions'].drop_duplicates(subset=['State', 'pressure', 'area_id'], keep='first').reset_index(drop=True)\n\n    # verify that there are no duplicate links\n    try: assert len(data['pressure_contributions'][data['pressure_contributions'].duplicated(['State', 'pressure', 'area_id'])]) == 0\n    except Exception as e: fail_with_message(f'Duplicate pressure contributions in input data!', e)\n\n    # make sure pressure contributions for each state / area are 100 %\n    for area in data['area']['ID']:\n        for state in data['state']['ID']:\n            mask = (data['pressure_contributions']['area_id'] == area) &amp; (data['pressure_contributions']['State'] == state)\n            relevant_contributions = data['pressure_contributions'].loc[mask, :]\n            if len(relevant_contributions) &gt; 0:\n                data['pressure_contributions'].loc[mask, 'contribution'] = relevant_contributions['contribution'] / relevant_contributions['contribution'].sum()\n\n    #\n    # thresholds\n    #\n\n    threshold_cols = ['PR', '10', '25', '50']   # target thresholds (PR=GES)\n\n    # get picks from cumulative distribution\n    for col in threshold_cols:\n        data['thresholds'][col] = data['thresholds'][col].apply(lambda x: get_pick(x) if not np.any(np.isnan(x)) else np.nan)\n\n    data['thresholds'] = data['thresholds'].drop_duplicates(subset=['State', 'area_id'], keep='first').reset_index(drop=True)\n\n    # verify that there are no duplicate links\n    try: assert len(data['thresholds'][data['thresholds'].duplicated(['State', 'area_id'])]) == 0\n    except Exception as e: fail_with_message(f'Duplicate GES targets in input data!', e)\n\n    return data\n</code></pre>"},{"location":"modules/som_app/#src.som_app.build_results","title":"<code>build_results(sim_res, input_data)</code>","text":"<p>Process the simulated results to calculate uncertainties.</p> <p>Uncertainty is determined as standard error of the mean.</p> Source code in <code>src/som_app.py</code> <pre><code>def build_results(sim_res: str, input_data: dict[str, pd.DataFrame]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Process the simulated results to calculate uncertainties.\n\n    Uncertainty is determined as standard error of the mean.\n    \"\"\"\n    files = [os.path.join(sim_res, x) for x in os.listdir(sim_res) if x.endswith('.pickle') and 'sim_res' in x]\n\n    areas = input_data['area']['ID']\n    pressures = input_data['pressure']['ID']\n    states = input_data['state']['ID']\n\n    res = {}\n\n    for key, val, ids in [\n        ('Pressure', 'pressure_levels', pressures), \n        ('TPL', 'total_pressure_load_levels', states), \n        ('TPLRed', 'total_pressure_load_reductions', states), \n        ('Thresholds', ('thresholds', 'PR'), states)\n    ]:\n        res[key] = {\n            'Mean': pd.DataFrame(ids).reindex(columns=['ID']+areas.tolist()).fillna(1.0), \n            'Error': pd.DataFrame(ids).reindex(columns=['ID']+areas.tolist()).fillna(1.0)\n        }\n        arr = np.empty(shape=(len(ids.tolist()), len(areas.tolist()), len(files)))\n        for i in range(len(files)):\n            with open(files[i], 'rb') as f:\n                data = pickle.load(f)\n            if type(val) == str:\n                arr[:, :, i] = data[val].values[:, 1:]\n            else:\n                arr[:, :, i] = data[val[0]][val[1]].values[:, 1:]\n        res[key]['Mean'].iloc[:, 1:] = np.mean(arr, axis=2)\n        res[key]['Error'].iloc[:, 1:] = np.std(arr, axis=2, ddof=1) / np.sqrt(arr.shape[2])    # calculate standard error\n\n    res['StatePressure'] = {\n        s: {\n            'Mean': pd.DataFrame(pressures).reindex(columns=['ID']+areas.tolist()).fillna(1.0), \n            'Error': pd.DataFrame(pressures).reindex(columns=['ID']+areas.tolist()).fillna(1.0)\n        } for s in states\n    }\n    for s in res['StatePressure']:\n        arr = np.empty(shape=(len(pressures.tolist()), len(areas.tolist()), len(files)))\n        for i in range(len(files)):\n            with open(files[i], 'rb') as f:\n                data = pickle.load(f)\n            arr[:, :, i] = data['state_pressure_levels'][s].values[:, 1:]\n        res['StatePressure'][s]['Mean'].iloc[:, 1:] = np.mean(arr, axis=2)\n        res['StatePressure'][s]['Error'].iloc[:, 1:] = np.std(arr, axis=2, ddof=1) / np.sqrt(arr.shape[2])\n\n    for key, val, col in [\n        ('MeasureEffects', 'measure_effects', 'reduction'), \n        ('ActivityContributions', 'activity_contributions', 'contribution'), \n        ('PressureContributions', 'pressure_contributions', 'contribution')\n    ]:\n        res[key] = {\n            'Mean': pd.DataFrame(input_data[val]), \n            'Error': pd.DataFrame(input_data[val])\n        }\n        arr = np.empty(shape=([x for x in input_data[val].values.shape]+[len(files)]))\n        for i in range(len(files)):\n            with open(files[i], 'rb') as f:\n                data = pickle.load(f)\n            arr[:, :, i] = data[val].values\n        res[key]['Mean'][col] = np.mean(arr[:, -1, :], axis=1)\n        res[key]['Error'][col] = np.std(arr[:, -1, :], axis=1, ddof=1) / np.sqrt(arr.shape[2])\n\n    return res\n</code></pre>"},{"location":"modules/som_app/#src.som_app.build_scenario","title":"<code>build_scenario(data, scenario)</code>","text":"<p>Build scenario</p> Source code in <code>src/som_app.py</code> <pre><code>def build_scenario(data: dict[str, pd.DataFrame], scenario: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Build scenario\n    \"\"\"\n    act_to_press = data['activity_contributions']\n    dev_scen = data['development_scenarios']\n\n    # for each pressure, save the total contribution of activities for later normalization\n    actual_sum = {}\n    for pressure_id in act_to_press['Pressure'].unique():\n        actual_sum[pressure_id] = {}\n        activities = act_to_press.loc[act_to_press['Pressure'] == pressure_id, :]\n        for area in activities['area_id'].unique():\n            actual_sum[pressure_id][area] = activities.loc[activities['area_id'] == area, 'contribution'].sum()\n\n    # multiply activities by scenario multiplier\n    def get_scenario(activity_id):\n        multiplier = dev_scen.loc[dev_scen['Activity'] == activity_id, scenario]\n        if len(multiplier) == 0:\n            return 1\n        multiplier = multiplier.values[0]\n        return multiplier\n    act_to_press['contribution'] = act_to_press['contribution'] * act_to_press['Activity'].apply(get_scenario)\n\n    # normalize\n    normalize_factor = {}\n    for pressure_id in act_to_press['Pressure'].unique():\n        normalize_factor[pressure_id] = {}\n        activities = act_to_press.loc[act_to_press['Pressure'] == pressure_id, :]\n        for area in activities['area_id'].unique():\n            scenario_sum = activities.loc[activities['area_id'] == area, 'contribution'].sum()\n            normalize_factor[pressure_id][area] = 1 + scenario_sum - actual_sum[pressure_id][area]\n\n    def normalize(value, pressure_id, area_id):\n        return value * normalize_factor[pressure_id][area_id]\n\n    act_to_press['contribution'] = act_to_press.apply(lambda x: normalize(x['contribution'], x['Pressure'], x['area_id']), axis=1)\n\n    return act_to_press\n</code></pre>"},{"location":"modules/som_app/#src.som_app.export_results_to_excel","title":"<code>export_results_to_excel(res, input_data, export_path)</code>","text":"<p>Exports simulation results as excel file</p> Source code in <code>src/som_app.py</code> <pre><code>def export_results_to_excel(res: dict[str, pd.DataFrame], input_data: dict[str, pd.DataFrame], export_path: str):\n    \"\"\"\n    Exports simulation results as excel file\n    \"\"\"\n    with pd.ExcelWriter(export_path) as writer:\n        new_res = set_id_columns(res, input_data)\n        for key in new_res:\n            if key != 'StatePressure':\n                for r in ['Mean', 'Error']:\n                    new_res[key][r].to_excel(writer, sheet_name=key+r, index=False)\n</code></pre>"},{"location":"modules/som_app/#src.som_app.set_id_columns","title":"<code>set_id_columns(res, data)</code>","text":"<p>Replaces id column values with the name of the corresponding measure/activity/pressure/state in the result dataframes</p> Source code in <code>src/som_app.py</code> <pre><code>def set_id_columns(res: dict[str, pd.DataFrame], data: dict[str, pd.DataFrame]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Replaces id column values with the name of the corresponding measure/activity/pressure/state in the result dataframes\n    \"\"\"\n    res = copy.deepcopy(res)\n    relations = {\n        'Pressure': 'pressure', \n        'StatePressure': 'pressure', \n        'TPL': 'state', \n        'TPLRed': 'state', \n        'Thresholds': 'state', \n    }\n    def replace_ids(id, k):\n        return data[k].loc[data[k]['ID'] == id, k].values[0]\n    for key in relations:\n        if key == 'StatePressure':\n            for s in data['state']['ID']:\n                for r in ['Mean', 'Error']:\n                    res[key][s][r]['ID'] = res[key][s][r]['ID'].apply(lambda x: replace_ids(x, relations[key]))\n                    res[key][s][r] = res[key][s][r].rename(columns={col: data['area'].loc[data['area']['ID'] == col, 'area'].values[0] for col in [c for c in res[key][s][r].columns if c != 'ID']})\n        else:\n            for r in ['Mean', 'Error']:\n                res[key][r]['ID'] = res[key][r]['ID'].apply(lambda x: replace_ids(x, relations[key]))\n                res[key][r] = res[key][r].rename(columns={col: data['area'].loc[data['area']['ID'] == col, 'area'].values[0] for col in [c for c in res[key][r].columns if c != 'ID']})\n    relations = {\n        'MeasureEffects': ['measure', 'activity', 'pressure', 'state'], \n        'ActivityContributions': ['Activity', 'Pressure', 'area_id'], \n        'PressureContributions': ['State', 'pressure', 'area_id']\n    }\n    conversions = {\n        'Activity': 'activity', \n        'Pressure': 'pressure', \n        'State': 'state', \n        'area_id': 'area'\n    }\n    for key in relations:\n        for r in ['Mean', 'Error']:\n            for col in relations[key]:\n                k = conversions[col] if col in conversions else col\n                res[key][r][col] = res[key][r][col].apply(lambda id: data[k].loc[data[k]['ID'] == id, k].values[0] if id != 0 else '-')\n\n    return res\n</code></pre>"},{"location":"modules/som_plots/","title":"<code>som_plots</code>","text":"<p>Copyright (c) 2024 Baltic Marine Environment Protection Commission</p> <p>LICENSE available under  local: 'SOM/protect_baltic/LICENSE' url: 'https://github.com/helcomsecretariat/SOM/blob/main/protect_baltic/LICENCE'</p>"},{"location":"modules/som_plots/#src.som_plots.build_display","title":"<code>build_display(res, data, out_dir, use_parallel_processing=False, selection=None)</code>","text":"<p>Constructs plots to visualize results.</p> Source code in <code>src/som_plots.py</code> <pre><code>def build_display(res: dict[str, dict[str, pd.DataFrame]], data: dict[str, pd.DataFrame], out_dir: str, use_parallel_processing: bool = False, selection: dict[str, list] = None):\n    \"\"\"\n    Constructs plots to visualize results.\n    \"\"\"\n    res = copy.deepcopy(res)\n    data = copy.deepcopy(data)\n\n    if selection is not None:\n        print('\\t\\tFiltering results...')\n        res = filter_results(res, selection)\n        data = filter_ids(data, selection)\n\n    areas = data['area']['ID']\n\n    cpu_count = multiprocessing.cpu_count()     # available cpu cores\n    with multiprocessing.Manager() as manager:\n        progress = manager.Namespace()\n        progress.current = 0\n        progress.total = len(areas)\n        lock = manager.Lock()\n        if use_parallel_processing:\n            with multiprocessing.Pool(processes=(min(cpu_count - 2, len(areas)))) as pool:\n                jobs = [(area, res, data, out_dir, progress, lock) for area in areas]\n                progress.current = 0\n                display_progress(progress.current / progress.total, text='\\t\\tTPL: ')\n                pool.starmap(plot_total_pressure_load_levels, jobs)\n                display_progress(progress.current / progress.total, text='\\t\\tTPL: ')\n                progress.current = 0\n                display_progress(progress.current / progress.total, text='\\n\\t\\tPressures: ')\n                pool.starmap(plot_pressure_levels, jobs)\n                display_progress(progress.current / progress.total, text='\\t\\tPressures: ')\n                progress.current = 0\n                display_progress(progress.current / progress.total, text='\\n\\t\\tThresholds: ')\n                pool.starmap(plot_thresholds, jobs)\n                display_progress(progress.current / progress.total, text='\\t\\tThresholds: ')\n                progress.current = 0\n                display_progress(progress.current / progress.total, text='\\n\\t\\tStatePressures: ')\n                pool.starmap(plot_state_pressure_levels, jobs)\n                display_progress(progress.current / progress.total, text='\\t\\tStatePressures: ')\n        else:\n            progress.current = 0\n            display_progress(progress.current / progress.total, text='\\t\\tTPL: ')\n            for area in areas:\n                plot_total_pressure_load_levels(area, res, data, out_dir, progress, lock)\n            display_progress(progress.current / progress.total, text='\\t\\tTPL: ')\n            progress.current = 0\n            display_progress(progress.current / progress.total, text='\\n\\t\\tPressures: ')\n            for area in areas:\n                plot_pressure_levels(area, res, data, out_dir, progress, lock)\n            display_progress(progress.current / progress.total, text='\\t\\tPressures: ')\n            progress.current = 0\n            display_progress(progress.current / progress.total, text='\\n\\t\\tThresholds: ')\n            for area in areas:\n                plot_thresholds(area, res, data, out_dir, progress, lock)\n            display_progress(progress.current / progress.total, text='\\t\\tThresholds: ')\n            progress.current = 0\n            display_progress(progress.current / progress.total, text='\\n\\t\\tStatePressures: ')\n            for area in areas:\n                plot_state_pressure_levels(area, res, data, out_dir, progress, lock)\n            display_progress(progress.current / progress.total, text='\\t\\tStatePressures: ')\n\n    #\n    # Measure effects\n    #\n\n    print('\\n\\t\\tMeasure effects...')\n\n    # plot settings\n    capsize = 3\n    capthick = 1\n    elinewidth = 1\n    ecolor = 'salmon'\n    label_angle = 60\n    char_limit = 25\n    bar_width = 0.4\n    edge_color = 'black'\n\n    fig, ax = plt.subplots(figsize=(100, 14), constrained_layout=True)\n\n    bar_width = 0.8\n    edge_color = 'black'\n    activity_font_size = 8\n\n    # adjust data\n    df = res['MeasureEffects']['Mean'].merge(res['MeasureEffects']['Error'], on=['measure', 'pressure', 'state', 'activity'], how='left', suffixes=('_mean', '_error'))\n    df = df.sort_values(by=['measure', 'pressure', 'state', 'activity'])\n    suffixes = ('', '_name')\n    for col in ['measure', 'activity', 'pressure', 'state']:\n        df = df.merge(data[col].loc[:, [col, 'ID']], left_on=col, right_on='ID', how='left', suffixes=suffixes)\n        df = df.drop(columns=[col, 'ID'])\n        df = df.rename(columns={col+'_name': col})\n        df.loc[:, col] = np.array([(x[:char_limit]+'...' if len(x) &gt; char_limit else x) if type(x) == str else 'All' for x in df.loc[:, col].values])\n    df['index'] = np.arange(len(df))\n    x_ticks = {x: df[df['measure'] == x]['index'].mean() for x in df['measure'].unique()}\n\n    # set colors\n    df['color_key'] = df['pressure'].astype(str) + '_' + df['state'].astype(str)\n    unique_keys = df['color_key'].unique()\n    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_keys)))\n    color_map = {key: colors[i] for i, key in enumerate(unique_keys)}\n\n    # create plot\n    for key in unique_keys:\n        subset = df[df['color_key'] == key]\n        bars = ax.bar(subset['index'], subset['reduction_mean'] * 100, width=bar_width, color=color_map[key], label=key if key not in ax.get_legend_handles_labels()[1] else '', edgecolor=edge_color)\n        ax.errorbar(subset['index'], subset['reduction_mean'] * 100, yerr=subset['reduction_error'] * 100, linestyle='None', marker='None', capsize=capsize, capthick=capthick, elinewidth=elinewidth, ecolor=ecolor)\n        for bar, (_, row) in zip(bars, subset.iterrows()):\n            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() / 2, str(row['activity']), \n                    ha='center', va='center', rotation=90, fontsize=activity_font_size, color='white')\n\n    ax.set_xlabel('Measure')\n    ax.set_ylabel('Reduction effect (%)')\n    ax.set_title(f'Measure Reduction Effects')\n    ax.set_xticks(list(x_ticks.values()), list(x_ticks.keys()), rotation=label_angle, ha='right')\n    ax.yaxis.grid(True, linestyle='--', color='lavender')\n    ax.legend(title='Pressure/State', bbox_to_anchor=(1.05, 1), loc='upper left')\n\n    # adjust axis limits\n    x_lim = [- 0.5, len(df) - 0.5]\n    ax.set_xlim(x_lim)\n    y_lim = [0, 100]\n    ax.set_ylim(y_lim)\n\n    # export\n    for area in areas:\n        area_name = data['area'].loc[areas == area, 'area'].values[0]\n        temp_dir = os.path.join(out_dir, f'{area}_{area_name}')\n        plt.savefig(os.path.join(temp_dir, f'{area}_{area_name}_MeasureEffects.png'), dpi=200)\n\n    plt.close(fig)\n</code></pre>"},{"location":"modules/som_plots/#src.som_plots.filter_ids","title":"<code>filter_ids(input_data, selection)</code>","text":"<p>Filter input data id dataframes</p> Source code in <code>src/som_plots.py</code> <pre><code>def filter_ids(input_data: dict[str, pd.DataFrame], selection: dict[str, list]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Filter input data id dataframes\n    \"\"\"\n    for key, values in [\n        ('measure', selection['measure']), \n        ('activity', selection['activity']), \n        ('pressure', selection['pressure']), \n        ('state', selection['state']), \n        ('area', selection['area'])\n    ]:\n        if values != []:\n            input_data[key] = input_data[key].loc[input_data[key]['ID'].isin(values), :]\n    return input_data\n</code></pre>"},{"location":"modules/som_plots/#src.som_plots.filter_results","title":"<code>filter_results(res, selection)</code>","text":"<p>Filter results for more selective output</p> Source code in <code>src/som_plots.py</code> <pre><code>def filter_results(res: dict[str, pd.DataFrame], selection: dict[str, list]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Filter results for more selective output\n    \"\"\"\n    # Pressure, TPL, TPLRed, Thresholds\n    for key, values in [\n        ('Pressure', selection['pressure']), \n        ('TPL', selection['state']), \n        ('TPLRed', selection['state']), \n        ('Thresholds', selection['state'])\n    ]:\n        for r in ['Mean', 'Error']:\n            if values != []:\n                if selection['area'] != []:\n                    res[key][r] = res[key][r].loc[res[key][r]['ID'].isin(values), ['ID'] + selection['area']]\n                else:\n                    res[key][r] = res[key][r].loc[res[key][r]['ID'].isin(values), :]\n    # StatePressure\n    if selection['pressure'] != []:\n        for s in res['StatePressure']:\n            for r in ['Mean', 'Error']:\n                if selection['area'] != []:\n                    res['StatePressure'][s][r] = res['StatePressure'][s][r].loc[res['StatePressure'][s][r]['ID'].isin(selection['pressure']), ['ID'] + selection['area']]\n                else:\n                    res['StatePressure'][s][r] = res['StatePressure'][s][r].loc[res['StatePressure'][s][r]['ID'].isin(selection['pressure']), :]\n    # MeasureEffects, ActivityContributions, PressureContributions\n    for key, cols in {\n        'MeasureEffects': {\n            'measure': selection['measure'], \n            'activity': selection['activity'], \n            'pressure': selection['pressure'], \n            'state': selection['state']\n        }, \n        'ActivityContributions': {\n            'Activity': selection['activity'], \n            'Pressure': selection['pressure'], \n            'area_id': selection['area']\n        }, \n        'PressureContributions': {\n            'State': selection['state'], \n            'pressure': selection['pressure'], \n            'area_id': selection['area']\n        }\n    }.items():\n        for r in ['Mean', 'Error']:\n            for col, values in cols.items():\n                if values != []:\n                    res[key][r] = res[key][r].loc[res[key][r][col].isin(values), :]\n    return res\n</code></pre>"},{"location":"modules/som_plots/#src.som_plots.plot_pressure_levels","title":"<code>plot_pressure_levels(area, res, data, out_dir, progress, lock)</code>","text":"<p>Plots pressures</p> Source code in <code>src/som_plots.py</code> <pre><code>def plot_pressure_levels(area, res, data, out_dir, progress, lock):\n    \"\"\"\n    Plots pressures\n    \"\"\"\n    # create new directory for the plots\n    area_name = data['area'].loc[data['area']['ID'] == area, 'area'].values[0]\n    out_path = os.path.join(out_dir, f'{area}_{area_name}', f'{area}_{area_name}_PressureLevels.png')\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # plot settings\n    marker = 's'\n    markersize = 5\n    markercolor = 'black'\n    capsize = 3\n    capthick = 1\n    elinewidth = 1\n    ecolor = 'salmon'\n    label_angle = 60\n    char_limit = 25\n\n    fig, ax = plt.subplots(figsize=(25, 12), constrained_layout=True)\n\n    # adjust data\n    suffixes = ('_mean', '_error')\n    df = pd.merge(res['Pressure']['Mean'].loc[:, ['ID', area]], res['Pressure']['Error'].loc[:, ['ID', area]], on='ID', suffixes=suffixes)\n    x_vals = data['pressure'].loc[:, 'pressure'].values\n    x_vals = np.array([x[:char_limit]+'...' if len(x) &gt; char_limit else x for x in x_vals])     # limit characters to char_limit\n    y_vals = df[str(area)+'_mean'] * 100    # convert to %\n    y_err = df[str(area)+'_error'] * 100    # conver to %\n\n    # create plot\n    ax.errorbar(np.arange(len(x_vals)), y_vals, yerr=y_err, linestyle='None', marker=marker, capsize=capsize, capthick=capthick, elinewidth=elinewidth, markersize=markersize, color=markercolor, ecolor=ecolor)\n    ax.set_xlabel('Pressure')\n    ax.set_ylabel('Level (%)')\n    ax.set_title(f'Pressure Levels\\n({area_name})')\n    ax.set_xticks(np.arange(len(x_vals)), x_vals, rotation=label_angle, ha='right')\n    ax.yaxis.grid(True, linestyle='--', color='lavender')\n\n    # adjust axis limits\n    x_lim = [- 0.5, len(x_vals) - 0.5]\n    ax.set_xlim(x_lim)\n    y_lim = [-5, 105]\n    ax.set_ylim(y_lim)\n\n    # export\n    plt.savefig(out_path, dpi=200)\n    plt.close(fig)\n\n    with lock:\n        progress.current += 1\n        display_progress(progress.current / progress.total, text='\\t\\tPressures: ')\n</code></pre>"},{"location":"modules/som_plots/#src.som_plots.plot_state_pressure_levels","title":"<code>plot_state_pressure_levels(area, res, data, out_dir, progress, lock)</code>","text":"<p>Plots state pressures</p> Source code in <code>src/som_plots.py</code> <pre><code>def plot_state_pressure_levels(area, res, data, out_dir, progress, lock):\n    \"\"\"\n    Plots state pressures\n    \"\"\"\n    # create new directory for the plots\n    area_name = data['area'].loc[data['area']['ID'] == area, 'area'].values[0]\n    out_dir = os.path.join(out_dir, f'{area}_{area_name}', 'state')\n    os.makedirs(out_dir, exist_ok=True)\n\n    # plot settings\n    marker = 's'\n    markersize = 5\n    markercolor = 'black'\n    capsize = 3\n    capthick = 1\n    elinewidth = 1\n    ecolor = 'salmon'\n    label_angle = 60\n    char_limit = 25\n\n    for state in res['StatePressure']:\n        state_name = data['state'].loc[data['state']['ID'] == state, 'state'].values[0]\n\n        out_path = os.path.join(out_dir, f'{area}_{area_name}_state_{state}_PressureLevels.png')\n\n        fig, ax = plt.subplots(figsize=(25, 12), constrained_layout=True)\n\n        # adjust data\n        suffixes = ('_mean', '_error')\n        df = pd.merge(res['StatePressure'][state]['Mean'].loc[:, ['ID', area]], res['StatePressure'][state]['Error'].loc[:, ['ID', area]], on='ID', suffixes=suffixes)\n        x_vals = data['pressure'].loc[:, 'pressure'].values\n        x_vals = np.array([x[:char_limit]+'...' if len(x) &gt; char_limit else x for x in x_vals])     # limit characters to char_limit\n        y_vals = df[str(area)+'_mean'] * 100    # convert to %\n        y_err = df[str(area)+'_error'] * 100    # conver to %\n\n        # create plot\n        ax.errorbar(np.arange(len(x_vals)), y_vals, yerr=y_err, linestyle='None', marker=marker, capsize=capsize, capthick=capthick, elinewidth=elinewidth, markersize=markersize, color=markercolor, ecolor=ecolor)\n        ax.set_xlabel('Pressure')\n        ax.set_ylabel('Level (%)')\n        ax.set_title(f'Pressure Levels ({state_name})\\n({area_name})')\n        ax.set_xticks(np.arange(len(x_vals)), x_vals, rotation=label_angle, ha='right')\n        ax.yaxis.grid(True, linestyle='--', color='lavender')\n\n        # adjust axis limits\n        x_lim = [- 0.5, len(x_vals) - 0.5]\n        ax.set_xlim(x_lim)\n        y_lim = [-5, 105]\n        ax.set_ylim(y_lim)\n\n        # export\n        plt.savefig(out_path, dpi=200)\n        plt.close(fig)\n\n    with lock:\n        progress.current += 1\n        display_progress(progress.current / progress.total, text='\\t\\tStatePressures: ')\n</code></pre>"},{"location":"modules/som_plots/#src.som_plots.plot_thresholds","title":"<code>plot_thresholds(area, res, data, out_dir, progress, lock)</code>","text":"<p>Plots thresholds comparison</p> Source code in <code>src/som_plots.py</code> <pre><code>def plot_thresholds(area, res, data, out_dir, progress, lock):\n    \"\"\"\n    Plots thresholds comparison\n    \"\"\"\n    # create new directory for the plots\n    area_name = data['area'].loc[data['area']['ID'] == area, 'area'].values[0]\n    out_path = os.path.join(out_dir, f'{area}_{area_name}', f'{area}_{area_name}_Thresholds.png')\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # plot settings\n    capsize = 3\n    capthick = 1\n    elinewidth = 1\n    ecolor = 'salmon'\n    label_angle = 60\n    char_limit = 25\n    bar_width = 0.4\n    bar_color_1 = 'turquoise'\n    bar_color_2 = 'seagreen'\n    edge_color = 'black'\n\n    fig, ax = plt.subplots(figsize=(16, 12), constrained_layout=True)\n\n    # adjust data\n    x_labels = np.array([x[:char_limit]+'...' if len(x) &gt; char_limit else x for x in data['state'].loc[:, 'state'].values])     # limit characters to char_limit\n    x_vals = np.arange(len(x_labels))\n    suffixes = ('_mean', '_error')\n    df = pd.merge(res['TPLRed']['Mean'].loc[:, ['ID', area]], res['TPLRed']['Error'].loc[:, ['ID', area]], on='ID', suffixes=suffixes)\n    y_vals_tpl = df[str(area)+'_mean'] * 100    # convert to %\n    y_err_tpl = df[str(area)+'_error'] * 100    # conver to %\n    df = pd.merge(res['Thresholds']['Mean'].loc[:, ['ID', area]], res['Thresholds']['Error'].loc[:, ['ID', area]], on='ID', suffixes=suffixes)\n    y_vals_ges = df[str(area)+'_mean'] * 100    # convert to %\n    y_err_ges = df[str(area)+'_error'] * 100    # convert to %\n\n    # create plot\n    label_tpl = 'Reduction with measures'\n    ax.bar(x_vals-bar_width/2, y_vals_tpl, width=bar_width, align='center', color=bar_color_1, label=label_tpl, edgecolor=edge_color)\n    ax.errorbar(x_vals-bar_width/2, y_vals_tpl, yerr=y_err_tpl, linestyle='None', marker='None', capsize=capsize, capthick=capthick, elinewidth=elinewidth, ecolor=ecolor)\n    label_ges = 'Target'\n    ax.bar(x_vals+bar_width/2, y_vals_ges, width=bar_width, align='center', color=bar_color_2, label=label_ges, edgecolor=edge_color)\n    ax.errorbar(x_vals+bar_width/2, y_vals_ges, yerr=y_err_ges, linestyle='None', marker='None', capsize=capsize, capthick=capthick, elinewidth=elinewidth, ecolor=ecolor)\n    ax.set_xlabel('Environmental State')\n    ax.set_ylabel('Reduction (%)')\n    ax.set_title(f'Total Pressure Load Reduction vs. GES Reduction Thresholds\\n({area_name})')\n    ax.set_xticks(x_vals, x_labels, rotation=label_angle, ha='right')\n    ax.yaxis.grid(True, linestyle='--', color='lavender')\n    ax.legend()\n\n    # adjust axis limits\n    x_lim = [- 0.5, len(x_vals) - 0.5]\n    ax.set_xlim(x_lim)\n    y_lim = [0, 100]\n    ax.set_ylim(y_lim)\n\n    # export\n    plt.savefig(out_path, dpi=200)\n    plt.close(fig)\n\n    with lock:\n        progress.current += 1\n        display_progress(progress.current / progress.total, text='\\t\\tThresholds: ')\n</code></pre>"},{"location":"modules/som_plots/#src.som_plots.plot_total_pressure_load_levels","title":"<code>plot_total_pressure_load_levels(area, res, data, out_dir, progress, lock)</code>","text":"<p>Plots TPL</p> Source code in <code>src/som_plots.py</code> <pre><code>def plot_total_pressure_load_levels(area, res, data, out_dir, progress, lock):\n    \"\"\"\n    Plots TPL\n    \"\"\"\n    # create new directory for the plots\n    area_name = data['area'].loc[data['area']['ID'] == area, 'area'].values[0]\n    out_path = os.path.join(out_dir, f'{area}_{area_name}', f'{area}_{area_name}_TotalPressureLoadLevels.png')\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n\n    # plot settings\n    marker = 's'\n    markersize = 5\n    markercolor = 'black'\n    capsize = 3\n    capthick = 1\n    elinewidth = 1\n    ecolor = 'salmon'\n    label_angle = 60\n    char_limit = 25\n\n    fig, ax = plt.subplots(figsize=(16, 12), constrained_layout=True)\n\n    # adjust data\n    suffixes = ('_mean', '_error')\n    df = pd.merge(res['TPL']['Mean'].loc[:, ['ID', area]], res['TPL']['Error'].loc[:, ['ID', area]], on='ID', suffixes=suffixes)\n    x_vals = data['state'].loc[:, 'state'].values\n    x_vals = np.array([x[:char_limit]+'...' if len(x) &gt; char_limit else x for x in x_vals])     # limit characters to char_limit\n    y_vals = df[str(area)+'_mean'] * 100    # convert to %\n    y_err = df[str(area)+'_error'] * 100    # conver to %\n\n    # create plot\n    ax.errorbar(np.arange(len(x_vals)), y_vals, yerr=y_err, linestyle='None', marker=marker, capsize=capsize, capthick=capthick, elinewidth=elinewidth, markersize=markersize, color=markercolor, ecolor=ecolor)\n    ax.set_xlabel('Environmental State')\n    ax.set_ylabel('Level (%)')\n    ax.set_title(f'Total Pressure Load on Environmental States\\n({area_name})')\n    ax.set_xticks(np.arange(len(x_vals)), x_vals, rotation=label_angle, ha='right')\n    ax.yaxis.grid(True, linestyle='--', color='lavender')\n\n    # adjust axis limits\n    x_lim = [- 0.5, len(x_vals) - 0.5]\n    ax.set_xlim(x_lim)\n    y_lim = [-5, 105]\n    ax.set_ylim(y_lim)\n\n    # export\n    plt.savefig(out_path, dpi=200)\n    plt.close(fig)\n\n    with lock:\n        progress.current += 1\n        display_progress(progress.current / progress.total, text='\\t\\tTPL: ')\n</code></pre>"},{"location":"modules/som_tools/","title":"<code>som_tools</code>","text":"<p>Copyright (c) 2024 Baltic Marine Environment Protection Commission</p> <p>LICENSE available under  local: 'SOM/protect_baltic/LICENSE' url: 'https://github.com/helcomsecretariat/SOM/blob/main/protect_baltic/LICENCE'</p>"},{"location":"modules/som_tools/#src.som_tools.get_expert_ids","title":"<code>get_expert_ids(df)</code>","text":"<p>Returns list of expert id column names from dataframe using regex</p> Source code in <code>src/som_tools.py</code> <pre><code>def get_expert_ids(df: pd.DataFrame) -&gt; list:\n    '''\n    Returns list of expert id column names from dataframe using regex\n    '''\n    return df.filter(regex='^(100|[1-9]?[0-9])$').columns\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.process_input_data","title":"<code>process_input_data(config)</code>","text":"<p>Reads in data and processes to usable form.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>dictionary loaded from configuration file</p> required <p>Returns:</p> Name Type Description <code>measure_survey_df</code> <code>DataFrame</code> <p>contains the measure survey data of expert panels        </p> <code>pressure_survey_df</code> <code>DataFrame</code> <p>contains the pressure survey data of expert panels</p> <code>data</code> <code>dict</code> <p>container for general data dataframes: measure (DataFrame):     ID: unique measure identifier     measure: name / description column activity (DataFrame):     ID: unique activity identifier     activity: name / description column pressure (DataFrame):     ID: unique pressure identifier     pressure: name / description column state (DataFrame):     ID: unique state identifier     state: name / description column area (DataFrame):     ID: unique area identifier     area: name / description column measure_effects (DataFrame): measure effects on activities, pressures, states pressure_contributions (DataFrame): pressure contributions to states thresholds (DataFrame): changes in states required to meet specific target thresholds cases (DataFrame): measure implementations in areas activity_contributions (DataFrame): activity contributions to pressures overlaps (DataFrame): measure-measure interactions development_scenarios (DataFrame): changes in human activities subpressures (DataFrame): pressure-pressure interactions</p> Source code in <code>src/som_tools.py</code> <pre><code>def process_input_data(config: dict) -&gt; dict[str, pd.DataFrame | dict[str, pd.DataFrame]]:\n    \"\"\"\n    Reads in data and processes to usable form.\n\n    Arguments:\n        config (dict): dictionary loaded from configuration file\n\n    Returns:\n        measure_survey_df (DataFrame): contains the measure survey data of expert panels        \n        pressure_survey_df (DataFrame): contains the pressure survey data of expert panels\n        data (dict): container for general data dataframes:\n            measure (DataFrame):\n                ID: unique measure identifier\n                measure: name / description column\n            activity (DataFrame):\n                ID: unique activity identifier\n                activity: name / description column\n            pressure (DataFrame):\n                ID: unique pressure identifier\n                pressure: name / description column\n            state (DataFrame):\n                ID: unique state identifier\n                state: name / description column\n            area (DataFrame):\n                ID: unique area identifier\n                area: name / description column\n            measure_effects (DataFrame): measure effects on activities, pressures, states\n            pressure_contributions (DataFrame): pressure contributions to states\n            thresholds (DataFrame): changes in states required to meet specific target thresholds\n            cases (DataFrame): measure implementations in areas\n            activity_contributions (DataFrame): activity contributions to pressures\n            overlaps (DataFrame): measure-measure interactions\n            development_scenarios (DataFrame): changes in human activities\n            subpressures (DataFrame): pressure-pressure interactions\n    \"\"\"\n    #\n    # measure survey data\n    #\n\n    file_name = os.path.realpath(config['input_data_legacy']['measure_effect_input'])\n    if not os.path.isfile(file_name): file_name = os.path.join(os.path.dirname(os.path.realpath(__file__)), config['input_data_legacy']['measure_effect_input'])\n    measure_effects = process_measure_survey_data(file_name)\n\n    #\n    # pressure survey data (combined pressure contributions and GES threshold)\n    #\n\n    file_name = os.path.realpath(config['input_data_legacy']['pressure_state_input'])\n    if not os.path.isfile(file_name): file_name = os.path.join(os.path.dirname(os.path.realpath(__file__)), config['input_data_legacy']['pressure_state_input'])\n    pressure_contributions, thresholds = process_pressure_survey_data(file_name)\n\n    #\n    # measure / pressure / activity / state links\n    #\n\n    # read core object descriptions\n    # i.e. ids for measures, activities, pressures and states\n    file_name = os.path.realpath(config['input_data_legacy']['general_input'])\n    if not os.path.isfile(file_name): file_name = os.path.join(os.path.dirname(os.path.realpath(__file__)), config['input_data_legacy']['general_input'])\n    id_sheets = config['input_data_legacy']['general_input_sheets']['ID']\n    data = read_ids(file_name=file_name, id_sheets=id_sheets)\n\n    #\n    # read case input\n    #\n\n    sheet_name = config['input_data_legacy']['general_input_sheets']['case']\n    cases = read_cases(file_name=file_name, sheet_name=sheet_name)\n\n    #\n    # read activity contribution data\n    #\n\n    sheet_name = config['input_data_legacy']['general_input_sheets']['postprocess']\n    activity_contributions = read_activity_contributions(file_name=file_name, sheet_name=sheet_name)\n\n    #\n    # read overlap data\n    #\n\n    sheet_name = config['input_data_legacy']['general_input_sheets']['overlaps']\n    overlaps = read_overlaps(file_name=file_name, sheet_name=sheet_name)\n\n    #\n    # read activity development scenario data\n    #\n\n    sheet_name = config['input_data_legacy']['general_input_sheets']['development_scenarios']\n    development_scenarios = read_development_scenarios(file_name=file_name, sheet_name=sheet_name)\n\n    #\n    # read subpressures links\n    #\n\n    sheet_name = config['input_data_legacy']['general_input_sheets']['subpressures']\n    subpressures = read_subpressures(file_name=file_name, sheet_name=sheet_name)\n\n    data.update({\n        'measure_effects': measure_effects, \n        'pressure_contributions': pressure_contributions, \n        'thresholds': thresholds, \n        'cases': cases, \n        'activity_contributions': activity_contributions, \n        'overlaps': overlaps, \n        'development_scenarios': development_scenarios, \n        'subpressures': subpressures\n    })\n\n    return data\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.process_measure_survey_data","title":"<code>process_measure_survey_data(file_name)</code>","text":"<p>This method reads input from the excel file containing data about measure reduction efficiencies  on activities, pressures and states.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>path of survey excel file</p> required <code>sheet_names</code> <code>dict</code> <p>dict of survey sheet ids in file_name</p> required <p>Returns:</p> Name Type Description <code>survey_df</code> <code>DataFrame</code> <p>processed survey data information: measure (int): measure id activity (int): activity id pressure (int): pressure id state (int): state id (if defined, [nan] if no state) reduction (list[float]): probability distribution represented as list</p> Source code in <code>src/som_tools.py</code> <pre><code>def process_measure_survey_data(file_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    This method reads input from the excel file containing data about measure reduction efficiencies \n    on activities, pressures and states.\n\n    Arguments:\n        file_name (str): path of survey excel file\n        sheet_names (dict): dict of survey sheet ids in file_name\n\n    Returns:\n        survey_df (DataFrame): processed survey data information:\n            measure (int): measure id\n            activity (int): activity id\n            pressure (int): pressure id\n            state (int): state id (if defined, [nan] if no state)\n            reduction (list[float]): probability distribution represented as list\n    \"\"\"\n    #\n    # read information sheet from input Excel file\n    #\n\n    data = pd.read_excel(io=file_name, sheet_name=None, header=None)\n    sheet_names = list(data.keys())\n\n    mteq = data[sheet_names[0]]\n    mteq.columns = mteq.iloc[0].values\n    mteq = mteq[1:]\n\n    measure_survey_data = {}\n    for id in range(1, len(sheet_names)):\n        measure_survey_data[id] = data[sheet_names[id]]\n\n    #\n    # preprocess values\n    #\n\n    mteq.loc[:, 'State'] = [x.split(';') if type(x) == str else x for x in mteq['State']]\n\n    #\n    # create new dataframe\n    #\n\n    cols = ['survey_id', 'title', 'block', 'measure', 'activity', 'pressure', 'state']\n    survey_df = pd.DataFrame(columns=cols)\n\n    block_number = 0    # represents the survey block\n\n    # for every survey sheet\n    for survey_id in measure_survey_data:\n\n        survey_info = mteq[mteq['Survey ID'] == survey_id]  # select the rows linked to current survey\n\n        end = 0     # represents last column index of the question set\n        for row, amt in enumerate(survey_info['AMT']):  # for each set of questions (row in MTEQ)\n\n            end = end + (2 * amt + 1)     # end column index for data\n            start = end - (2 * amt)     # start column index for data\n\n            # create list to describe the data on each row\n            titles = ['expected value', 'variance'] * amt\n            titles.append('max effectiveness')\n            titles.append('expert weights')\n\n            # select current question column names as measure ids\n            measures = measure_survey_data[survey_id].iloc[0, start:end].tolist()\n            measures.append(np.nan)\n            measures.append(np.nan)\n\n            # create lists to hold ids and format each row as a list\n            ids = {}\n            for category in ['Activity', 'Pressure', 'State']:\n                category_ids = survey_info[category].iloc[row]\n                if isinstance(category_ids, str):\n                    ids[category] = [[int(x) for x in category_ids.split(';') if x != '']] * amt * 2\n                elif isinstance(category_ids, list):\n                    ids[category] = [[int(x) for x in category_ids if x != '']] * amt * 2\n                elif isinstance(category_ids, float) or isinstance(category_ids, int):\n                    ids[category] = [[category_ids if not np.isnan(category_ids) else np.nan]] * amt * 2\n                else:\n                    ids[category] = [category_ids] * amt * 2\n                ids[category].append(np.nan)\n                ids[category].append(np.nan)\n\n            # in MTEQ sheet, find all expert weight columns, get the values for the current row, set empty cells to 1\n            expert_cols = [True if 'exp' in col.lower() else False for col in survey_info.columns]\n            expert_weights = survey_info.loc[:, expert_cols].iloc[row]\n            expert_weights = expert_weights.astype(float).fillna(1).astype(int) # convert to float first so fillna() works without warning\n\n            data = measure_survey_data[survey_id].loc[1:, start:end]    # select current question answers\n            data[end+1] = expert_weights  # create column for expert weights\n            for expert, weight in enumerate(expert_weights, 1): # for each row (expert) in weights\n                data.loc[expert, end+1] = weight    # set the weight as the value\n            data = data.transpose() # transpose so that experts are columns and measures are rows\n\n            # add survey info to each entry in the data \n            data['survey_id'] = [survey_id] * len(data) # new column with survey_id for every row\n            data['title'] = titles\n            data['block'] = [block_number] * len(data)  # new column with block_number for every row\n            data['measure'] = measures\n            data['activity'] = ids['Activity']\n            data['pressure'] = ids['Pressure']\n            data['state'] = ids['State']\n\n            with warnings.catch_warnings(action='ignore'):\n                survey_df = pd.concat([survey_df, data], ignore_index=True, sort=False)\n            block_number = block_number + 1\n\n    # select column names corresponding to expert ids (any number between 1 and 100)\n    expert_ids = get_expert_ids(survey_df)\n\n    #\n    # Adjust answers by scaling factor\n    #\n\n    block_ids = survey_df.loc[:,'block'].unique()   # find unique block ids\n    for b_id in block_ids:  # for each block\n        block = survey_df.loc[survey_df['block'] == b_id, :]    # select all rows with current block id\n        for col in block:   # for each column\n            if isinstance(col, int):    # if it is an expert answer\n                # from the column, select the expected values and variances\n                expected_value = block.loc[block['title']=='expected value', col]\n                variance = block.loc[block['title']=='variance', col]\n                # skip if no questions were answered\n                if expected_value.isnull().all():\n                    block.loc[block['title']=='variance', col] = np.nan     # also set all variances to null\n                    continue\n                if variance.isnull().all():\n                    block.loc[block['title']=='expected value', col] = np.nan     # also set all expected values to null\n                    continue\n                # find the highest value of the answers\n                max_expected_value = expected_value.max()\n                # find the max effectiveness estimated by the expert\n                max_effectiveness = block.loc[block['title']=='max effectiveness', col].values[0]\n                # calculate scaling factor\n                if np.isnan(max_effectiveness):\n                    # set all values to null if no max effectiveness (in column, for current block)\n                    survey_df.loc[survey_df['block'] == b_id, col] = np.nan\n                elif max_effectiveness == 0 or max_expected_value == 0:\n                    # scale all expected values to 0 if max effectiveness is zero or all expected values are zero\n                    survey_df.loc[(survey_df['block'] == b_id) &amp; (survey_df['title'] == 'expected value'), col] = 0\n                else:\n                    # get the scaling factor\n                    scaling_factor = np.divide(max_expected_value, max_effectiveness)\n                    # divide the expected values by the new scaling factor\n                    survey_df.loc[(survey_df['block'] == b_id) &amp; (survey_df['title'] == 'expected value'), col] = np.divide(expected_value, scaling_factor)\n\n    #\n    # Calculate effectiveness range boundaries\n    #\n\n    # create new rows for 'effectiveness lower' and 'effectiveness upper' bounds after variance rows\n    new_rows = []\n    for i, row in survey_df.iterrows():\n        new_rows.append(row)\n        if row['title'] == 'variance':\n            # create lower bound\n            min_row = survey_df.loc[i].copy()\n            min_row['title'] = 'effectiveness lower'\n            min_row[expert_ids] = np.nan\n            new_rows.append(min_row)\n            # create upper bound\n            max_row = survey_df.loc[i].copy()\n            max_row['title'] = 'effectiveness upper'\n            max_row[expert_ids] = np.nan\n            new_rows.append(max_row)\n    survey_df = pd.DataFrame(new_rows, columns=survey_df.columns)\n    survey_df.reset_index(drop=True, inplace=True)\n    # set values for 'effectiveness lower' and 'effectiveness upper' bounds rows\n    # calculated as follows:\n    #   lower boundary:\n    #       if expected_value + variance / 2 &gt; 100:\n    #           boundary = 100 - variance\n    #       else:\n    #           if expected_value - variance / 2 &lt; 0:\n    #               boundary = 0\n    #           else:\n    #               boundary = expected_value - variance / 2\n    #   upper boundary:\n    #       if expected_value - variance / 2 &lt; 0:\n    #           boundary = variance\n    #       else:\n    #           if expected_value + variance / 2 &gt; 100:\n    #               boundary = 100\n    #           else:\n    #               boundary = expected_value + variance / 2\n    for i, row in survey_df.iterrows():\n        if row['title'] == 'effectiveness lower':\n            expected_value = survey_df.iloc[i-2][expert_ids]\n            variance = survey_df.iloc[i-1][expert_ids]\n            reach_upper_limit = expected_value + variance / 2 &gt; 100 # boolean array\n            row_values = survey_df.loc[i, expert_ids]\n            row_values[reach_upper_limit] = 100 - variance\n            row_values[~reach_upper_limit] = expected_value - variance / 2\n            row_values[row_values &lt; 0] = 0\n            survey_df.loc[i, expert_ids] = row_values\n        if row['title'] == 'effectiveness upper':\n            expected_value = survey_df.iloc[i-3][expert_ids]\n            variance = survey_df.iloc[i-2][expert_ids]\n            reach_lower_limit = expected_value - variance / 2 &lt; 0   # boolean array\n            row_values = survey_df.loc[i, expert_ids]\n            row_values[reach_lower_limit] = variance\n            row_values[~reach_lower_limit] = expected_value + variance / 2\n            row_values[row_values &gt; 100] = 100\n            survey_df.loc[i, expert_ids] = row_values\n\n    #\n    # Calculate probability distributions\n    #\n\n    # add a new column for the probability\n    survey_df['reduction'] = pd.Series([np.nan] * len(survey_df), dtype='object')\n\n    # access expert answer columns, separate rows by type of answer\n    expecteds = survey_df[expert_ids].loc[survey_df['title'] == 'expected value']\n    lower_boundaries = survey_df[expert_ids].loc[survey_df['title'] == 'effectiveness lower']\n    upper_boundaries = survey_df[expert_ids].loc[survey_df['title'] == 'effectiveness upper']\n    weights = survey_df.loc[survey_df['title'] == 'expert weights', np.insert(expert_ids, 0, 'block')]\n    blocks = survey_df['block'].loc[(survey_df['title'] == 'expected value')]\n    # go through each measure-activity-pressure link\n    for num in expecteds.index:\n        # access current row data and convert to 1-D arrays\n        b_id = blocks.loc[num]\n        e = expecteds.loc[num].to_numpy().astype(float)\n        l = lower_boundaries.loc[num+2].to_numpy().astype(float)\n        u = upper_boundaries.loc[num+3].to_numpy().astype(float)\n        w = weights.loc[weights['block'] == b_id, expert_ids].to_numpy().astype(float).flatten()\n        # get expert probability distribution\n        prob_dist = get_prob_dist(expecteds=e, \n                                  lower_boundaries=l, \n                                  upper_boundaries=u, \n                                  weights=w)\n\n        survey_df.at[num, 'reduction'] = prob_dist\n\n    #\n    # Remove rows and columns that are not needed anymore\n    #\n\n    for title in ['max effectiveness', 'variance', 'effectiveness lower', 'effectiveness upper', 'expert weights']:\n        survey_df = survey_df.loc[survey_df['title'] != title]\n    survey_df = survey_df.drop(columns=expert_ids)\n    survey_df = survey_df.drop(columns=['survey_id', 'title', 'block'])\n\n    #\n    # Split activity / pressure / state lists into separate rows, and reset index\n    #\n\n    for col in ['activity', 'pressure', 'state']:\n        survey_df = survey_df.explode(column=col)\n        survey_df = survey_df.reset_index(drop=True)\n\n    #\n    # Replace nan values with zeros and convert columns to integers\n    #\n\n    for column in ['measure', 'activity', 'pressure', 'state']:\n        with warnings.catch_warnings(action='ignore'):\n            survey_df[column] = survey_df[column].fillna(0)\n        survey_df[column] = survey_df[column].astype(int)\n\n    return survey_df\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.process_pressure_survey_data","title":"<code>process_pressure_survey_data(file_name)</code>","text":"<p>This method reads input from the excel file containing data about pressure contributions to states  and the changes in state required to reach required thresholds of improvement.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>path of survey excel file</p> required <code>sheet_names</code> <code>dict</code> <p>dict of survey sheet ids in file_name</p> required <p>Returns:</p> Name Type Description <code>pressure_contributions</code> <code>DataFrame</code> <p>State: state id pressure: pressure id area_id: area id average: average contribution of pressure uncertainty: standard deviation of pressure contribution</p> <code>thresholds</code> <code>DataFrame</code> <p>State: state id area_id: area id PR: reduction in state required to reach GES target 10: reduction in state required to reach 10 % improvement 25: reduction in state required to reach 25 % improvement 50: reduction in state required to reach 50 % improvement</p> Source code in <code>src/som_tools.py</code> <pre><code>def process_pressure_survey_data(file_name: str) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    This method reads input from the excel file containing data about pressure contributions to states \n    and the changes in state required to reach required thresholds of improvement.\n\n    Arguments:\n        file_name (str): path of survey excel file\n        sheet_names (dict): dict of survey sheet ids in file_name\n\n    Returns:\n        pressure_contributions (DataFrame):\n            State: state id\n            pressure: pressure id\n            area_id: area id\n            average: average contribution of pressure\n            uncertainty: standard deviation of pressure contribution\n        thresholds (DataFrame):\n            State: state id\n            area_id: area id\n            PR: reduction in state required to reach GES target\n            10: reduction in state required to reach 10 % improvement\n            25: reduction in state required to reach 25 % improvement\n            50: reduction in state required to reach 50 % improvement\n    \"\"\"\n    #\n    # set parameter values\n    #\n\n    expert_number = 6   # max number of experts per question\n    threshold_cols = ['PR', '10', '25', '50']   # target thresholds (PR=GES)\n\n    #\n    # read information sheet from input Excel file\n    #\n\n    data = pd.read_excel(io=file_name, sheet_name=None)\n    sheet_names = list(data.keys())\n\n    psq = data[sheet_names[0]]\n\n    pressure_survey_data = {}\n    for id in range(1, len(sheet_names)):\n        pressure_survey_data[id] = data[sheet_names[id]]\n\n    #\n    # preprocess values\n    #\n\n    psq['area_id'] = [x.split(';') if type(x) == str else x for x in psq['area_id']]\n\n    # add question id column\n    psq['question_id'] = list(range(len(psq)))\n\n    #\n    # create new dataframe\n    #\n\n    survey_df = pd.DataFrame(columns=['survey_id', 'question_id', 'State', 'area_id', 'GES known', 'Weight'])\n\n    # survey columns from which to take data\n    cols = ['Expert']\n    cols += [x + str(i+1) for x in ['P', 'S'] for i in range(expert_number)]    # up to expert_number different pressures related to state, and their significance\n    cols += [x + y for x in ['MIN', 'MAX', 'ML'] for y in threshold_cols]     # required pressure reduction to reach GES (if known) or X % improvement in state\n\n    start = 0    # keep track of where to access data in psq\n\n    # for every survey sheet\n    for survey_id in pressure_survey_data:\n\n        # identify amount of experts in survey\n        expert_ids = pressure_survey_data[survey_id]['Expert'].unique()\n        # identify amount of questions in survey\n        questions = np.sum(pressure_survey_data[survey_id]['Expert'] == expert_ids[0])\n\n        # use number of questions to get state, area and GES known\n        question_id = psq['question_id'].iloc[start:start+questions].reset_index(drop=True)\n        state = psq['State'].iloc[start:start+questions].reset_index(drop=True)\n        areas = psq['area_id'].iloc[start:start+questions].reset_index(drop=True)\n        ges_known = psq['GES known'].iloc[start:start+questions].reset_index(drop=True)\n\n        # find all expert weight columns and values\n        expert_cols = [True if 'exp' in col.lower() else False for col in psq.columns]\n        expert_weights = psq.loc[start:start+questions, expert_cols].reset_index(drop=True)\n        expert_weights = expert_weights.fillna(1)\n\n        survey_answers = 0\n        for expert in expert_ids:\n\n            # select expert answers\n            data = pressure_survey_data[survey_id][cols].loc[pressure_survey_data[survey_id][cols]['Expert'] == expert].reset_index(drop=True)\n\n            # verify that the amount of answers is correct\n            if len(data) != questions: raise Exception('Not same amount of answers for each expert in survey sheet!')\n\n            survey_answers += len(data)\n\n            # set survey id, state, area and GES known for data\n            data['survey_id'] = survey_id\n            data['question_id'] = question_id\n            data['State'] = state\n            data['area_id'] = areas\n            data['GES known'] = ges_known\n\n            # set expert weights\n            data['Weight'] = expert_weights['Exp' + str(int(expert))]\n\n            # add data to final dataframe\n            with warnings.catch_warnings(action='ignore'):\n                survey_df = pd.concat([survey_df, data], ignore_index=True, sort=False)\n\n        # verify that the correct number of answers was saved\n        if survey_answers != len(expert_ids) * questions: raise Exception('Incorrect amount of answers found for survey!')\n\n        # increase counter\n        start += questions\n\n    # create new dataframe for merged rows\n    cols = ['survey_id', 'question_id', 'State', 'area_id', 'GES known']\n    new_df = pd.DataFrame(columns=cols+['Pressures', 'Contribution']+threshold_cols)\n    # remove empty elements from areas, and convert ids to integers\n    survey_df['area_id'] = survey_df['area_id'].apply(lambda x: [int(area) for area in x if area != ''])\n    # identify all unique questions\n    questions = survey_df['question_id'].unique()\n    # process each state\n    for question in questions:\n        # select current question rows\n        data = survey_df.loc[survey_df['question_id'] == question].reset_index(drop=True)\n        #\n        # pressure contributions and uncertainties\n        #\n        # select pressures and significances, and find non nan values\n        pressures = data[['P'+str(x+1) for x in range(expert_number)]].to_numpy().astype(float)\n        significances = data[['S'+str(x+1) for x in range(expert_number)]].to_numpy().astype(float)\n        mask = ~np.isnan(pressures)\n        # weigh significances by amount of participating experts\n        w = data[['Weight']].to_numpy().astype(float)\n        significances = significances * w\n        # go through each expert answer and calculate weights\n        weights = {}\n        for i, e in enumerate(pressures):\n            s_tot = np.sum(significances[i][mask[i]])\n            for p, s in zip(pressures[i][mask[i]], significances[i][mask[i]]):\n                if int(p) not in weights:\n                    weights[int(p)] = []\n                weights[int(p)].append(s / s_tot)\n        # using weights, calculate contributions and uncertainties\n        average = {p: np.mean(weights[p]) for p in weights}\n        stddev = {p: np.std(weights[p]) for p in weights}\n        # create probability distributions\n        minimum, maximum = {}, {}\n        for p in average:\n            if average[p] - stddev[p] &gt; 0:\n                if average[p] + stddev[p] &gt; 1:\n                    minimum[p] = 1.0 - stddev[p] * 2\n                    maximum[p] = 1.0\n                else:\n                    minimum[p] = average[p] - stddev[p]\n                    maximum[p] = average[p] + stddev[p]\n            else:\n                minimum[p] = 0.0\n                maximum[p] = stddev[p] * 2\n        average = {p: np.array([average[p] * 100]) for p in average}\n        minimum = {p: np.array([minimum[p] * 100]) for p in minimum}\n        maximum = {p: np.array([maximum[p] * 100]) for p in maximum}\n        contribution = {p: get_prob_dist(average[p], minimum[p], maximum[p], np.ones(len(average[p]))) for p in average}\n        # convert to lists\n        pressures = list(average.keys())\n        contribution = [contribution[p] for p in pressures]\n        #\n        # probability distributions for GES thresholds\n        #\n        reductions = {}\n        for r in threshold_cols:\n            # get min, max and ml data\n            r_min = data['MIN'+r].to_numpy().astype(float)\n            r_max = data['MAX'+r].to_numpy().astype(float)\n            r_ml = data['ML'+r].to_numpy().astype(float)\n            # get weighted cumulative probability distribution\n            dist = get_prob_dist(r_ml, r_min, r_max, w.flatten())\n            reductions[r] = dist\n        #\n        # merge processed data with dataframe\n        #\n        # create a new dataframe row and merge with new dataframe\n        data = survey_df[cols].loc[survey_df['question_id'] == question].reset_index(drop=True).iloc[0]\n        data = pd.DataFrame([data])\n        # initialize new columns\n        for c in ['Pressures', 'Contribution']+threshold_cols:\n            data[c] = np.nan\n            data[c] = data[c].astype(object)\n        # change data type to allow for lists\n        data.at[0, 'Pressures'] = pressures\n        data.at[0, 'Contribution'] = contribution\n        for r in threshold_cols:\n            data.at[0, r] = reductions[r]\n        with warnings.catch_warnings(action='ignore'):\n            new_df = pd.concat([new_df, data], ignore_index=True, sort=False)\n    #\n    # split pressures into separate rows\n    #\n    new_df = new_df.assign(pressure=[list(zip(*row)) for row in zip(new_df['Pressures'], new_df['Contribution'])])\n    new_df = new_df.explode('pressure')\n    new_df = new_df.reset_index(drop=True)\n    new_df = new_df.drop(columns=['Pressures', 'Contribution'])\n    new_df[['pressure', 'contribution']] = pd.DataFrame(new_df['pressure'].tolist())\n    #\n    # remove rows with missing data (no pressure or no thresholds)\n    #\n    new_df = new_df.loc[new_df['pressure'].notna(), :]\n    new_df = new_df[new_df[threshold_cols].notna().any(axis=1)]\n    new_df = new_df.reset_index(drop=True)\n    #\n    # make sure pressure ids are integers, remove unnecessary columns\n    #\n    new_df['pressure'] = new_df['pressure'].astype(int)\n    new_df = new_df.drop(columns=['survey_id', 'question_id', 'GES known'])\n    #\n    # split areas into separate rows\n    #\n    new_df = new_df.explode('area_id')\n    new_df = new_df.reset_index(drop=True)\n    #\n    # split new_df into two dataframes, one for pressure contributions and one for thresholds\n    #\n    pressure_contributions = pd.DataFrame(new_df.loc[:, ['State', 'pressure', 'area_id', 'contribution']])\n    thresholds = pd.DataFrame(new_df.loc[:, ['State', 'area_id'] + threshold_cols])\n\n    return pressure_contributions, thresholds\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.read_activity_contributions","title":"<code>read_activity_contributions(file_name, sheet_name)</code>","text":"<p>Reads input data of activities to pressures in areas. </p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>name of source excel file name</p> required <code>sheet_name</code> <code>str</code> <p>name of excel sheet</p> required <p>Returns:</p> Name Type Description <code>act_to_press</code> <code>DataFrame</code> <p>dataframe containing mappings between activities and pressures</p> Source code in <code>src/som_tools.py</code> <pre><code>def read_activity_contributions(file_name: str, sheet_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads input data of activities to pressures in areas. \n\n    Arguments:\n        file_name (str): name of source excel file name\n        sheet_name (str): name of excel sheet\n\n    Returns:\n        act_to_press (DataFrame): dataframe containing mappings between activities and pressures\n    \"\"\"\n    act_to_press = pd.read_excel(file_name, sheet_name=sheet_name)\n\n    # read all most likely, min and max column values into lists in new columns\n    for col, regex_str in zip(['expected', 'minimum', 'maximum'], ['ML[1-6]', 'Min[1-6]', 'Max[1-6]']):\n        act_to_press[col] = act_to_press.filter(regex=regex_str).values.tolist()\n\n    # remove all most likely, min and max columns\n    for regex_str in ['ML[1-6]', 'Min[1-6]', 'Max[1-6]']:\n        act_to_press.drop(act_to_press.filter(regex=regex_str).columns, axis=1, inplace=True)\n\n    # separate values grouped together in sheet on the same row with ';' into separate rows\n    for category in ['Activity', 'Pressure', 'area_id']:\n        act_to_press[category] = [list(filter(None, x.split(';'))) if type(x) == str else x for x in act_to_press[category]]\n        act_to_press = act_to_press.explode(category)\n        act_to_press[category] = act_to_press[category].astype(int)\n    act_to_press = act_to_press.reset_index(drop=True)\n\n    # calculate probability distributions\n    act_to_press['contribution'] = pd.Series([np.nan] * len(act_to_press), dtype='object')\n    for num in act_to_press.index:\n        # convert expert answers to array\n        expected = np.array(list(act_to_press.loc[num, ['expected']])).flatten()\n        lower = np.array(list(act_to_press.loc[num, ['minimum']])).flatten()\n        upper = np.array(list(act_to_press.loc[num, ['maximum']])).flatten()\n        weights = np.full(len(expected), 1)\n        # if boundaries are unknown, set to same as expected\n        lower[np.isnan(lower)] = expected[np.isnan(lower)]\n        upper[np.isnan(upper)] = expected[np.isnan(upper)]\n        # get probability distribution\n        act_to_press.at[num, 'contribution'] = get_prob_dist(expected, lower, upper, weights)\n\n    act_to_press = act_to_press.drop(columns=['expected', 'minimum', 'maximum'])\n\n    return act_to_press\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.read_cases","title":"<code>read_cases(file_name, sheet_name)</code>","text":"<p>Reading in and processing data for cases. Each row represents one case. </p> <p>In columns of 'ActMeas' sheet ('activities', 'pressure' and 'state') the value 0 == 'all relevant'.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>name of source excel file name</p> required <code>sheet_name</code> <code>str</code> <p>name of excel sheet</p> required <p>Returns:</p> Name Type Description <code>cases</code> <code>DataFrame</code> <p>case data</p> Source code in <code>src/som_tools.py</code> <pre><code>def read_cases(file_name: str, sheet_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reading in and processing data for cases. Each row represents one case. \n\n    In columns of 'ActMeas' sheet ('activities', 'pressure' and 'state') the value 0 == 'all relevant'.\n\n    Arguments:\n        file_name (str): name of source excel file name\n        sheet_name (str): name of excel sheet\n\n    Returns:\n        cases (DataFrame): case data\n    \"\"\"\n    cases = pd.read_excel(io=file_name, sheet_name=sheet_name)\n\n    assert len(cases[cases.duplicated(['ID'])]) == 0\n\n    for col in ['activity', 'pressure', 'state', 'area_id']:\n        # separate ids grouped together in sheet on the same row with ';' into separate rows\n        cases[col] = [list(filter(None, x.split(';'))) if type(x) == str else x for x in cases[col]]\n        cases = cases.explode(col)\n        # change types of split values from str to int\n        cases[col] = cases[col].astype(int)\n\n    for col in ['coverage', 'implementation']:\n        cases[col] = cases[col].astype(float)\n\n    cases = cases.reset_index(drop=True)\n\n    return cases\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.read_development_scenarios","title":"<code>read_development_scenarios(file_name, sheet_name)</code>","text":"<p>Reads input data of activity development scnearios. </p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>name of source excel file name</p> required <code>sheet_name</code> <code>str</code> <p>name of sheet in excel file</p> required <p>Returns:</p> Name Type Description <code>development_scenarios</code> <code>DataFrame</code> <p>dataframe containing activity development scenarios</p> Source code in <code>src/som_tools.py</code> <pre><code>def read_development_scenarios(file_name: str, sheet_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads input data of activity development scnearios. \n\n    Arguments:\n        file_name (str): name of source excel file name\n        sheet_name (str): name of sheet in excel file\n\n    Returns:\n        development_scenarios (DataFrame): dataframe containing activity development scenarios\n    \"\"\"\n    development_scenarios = pd.read_excel(file_name, sheet_name=sheet_name)\n\n    # replace nan values with 0, assuming that no value means no change\n    for category in ['BAU', 'ChangeMin', 'ChangeML', 'ChangeMax']:\n        development_scenarios.loc[np.isnan(development_scenarios[category]), category] = 0\n        development_scenarios[category] = development_scenarios[category].astype(float)\n\n    development_scenarios['Activity'] = development_scenarios['Activity'].astype(int)\n\n    # change values from percentual change to multiplier type by adding 1\n    for category in ['BAU', 'ChangeMin', 'ChangeML', 'ChangeMax']:\n        development_scenarios[category] = development_scenarios[category] + 1\n\n    return development_scenarios\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.read_ids","title":"<code>read_ids(file_name, id_sheets)</code>","text":"<p>Reads in model object descriptions from general input files</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>source excel file name containing measure, activity, pressure and state id sheets</p> required <code>id_sheets</code> <code>dict</code> <p>should have structure {'measure': sheet_name, 'activity': sheet_name, ...}</p> required <p>Returns:</p> Name Type Description <code>object_data</code> <code>dict</code> <p>dictionary containing measure, activity, pressure and state ids and descriptions in separate dataframes</p> Source code in <code>src/som_tools.py</code> <pre><code>def read_ids(file_name: str, id_sheets: dict) -&gt; dict[str, dict]:\n    \"\"\"\n    Reads in model object descriptions from general input files\n\n    Arguments:\n        file_name (str): source excel file name containing measure, activity, pressure and state id sheets\n        id_sheets (dict): should have structure {'measure': sheet_name, 'activity': sheet_name, ...}\n\n    Returns:\n        object_data (dict): dictionary containing measure, activity, pressure and state ids and descriptions in separate dataframes\n    \"\"\"\n    # create dicts for each category\n    object_data = {}\n    for category in id_sheets:\n        # read excel sheet into dataframe\n        df = pd.read_excel(io=file_name, sheet_name=id_sheets[category])\n        # remove non-necessary columns\n        df.drop(columns=[col for col in df.columns if col not in ['ID', category]])\n        # remove rows where id is nan or empty string\n        df = df.dropna(subset=['ID'])\n        df = df[df['ID'] != '']\n        # convert id column to integer (if not already)\n        df['ID'] = df['ID'].astype(int)\n        object_data[category] = df\n\n    return object_data\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.read_overlaps","title":"<code>read_overlaps(file_name, sheet_name)</code>","text":"<p>Reads input data of measure-measure interactions. </p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>name of source excel file name</p> required <code>sheet_name</code> <code>str</code> <p>name of sheet in excel file</p> required <p>Returns:</p> Name Type Description <code>overlaps</code> <code>DataFrame</code> <p>dataframe containing overlaps between individual measures</p> Source code in <code>src/som_tools.py</code> <pre><code>def read_overlaps(file_name: str, sheet_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads input data of measure-measure interactions. \n\n    Arguments:\n        file_name (str): name of source excel file name\n        sheet_name (str): name of sheet in excel file\n\n    Returns:\n        overlaps (DataFrame): dataframe containing overlaps between individual measures\n    \"\"\"\n    overlaps = pd.read_excel(file_name, sheet_name=sheet_name)\n\n    # replace nan values in ID columns with 0 and make sure they are integers\n    for category in ['Overlap', 'Pressure', 'Activity', 'Overlapping', 'Overlapped']:\n        overlaps.loc[np.isnan(overlaps[category]), category] = 0\n        overlaps[category] = overlaps[category].astype(int)\n\n    return overlaps\n</code></pre>"},{"location":"modules/som_tools/#src.som_tools.read_subpressures","title":"<code>read_subpressures(file_name, sheet_name)</code>","text":"<p>Reads input data of subpressures links to state pressures</p> Source code in <code>src/som_tools.py</code> <pre><code>def read_subpressures(file_name: str, sheet_name: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads input data of subpressures links to state pressures\n    \"\"\"\n    subpressures = pd.read_excel(file_name, sheet_name=sheet_name)\n\n    for col in ['Reduced pressure', 'State pressure', 'State']:\n        # separate ids grouped together in sheet on the same row with ';' into separate rows\n        subpressures[col] = [list(filter(None, x.split(';'))) if type(x) == str else x for x in subpressures[col]]\n        subpressures = subpressures.explode(col)\n        # change types of split values from str to int\n        subpressures[col] = subpressures[col].astype(int)\n\n    for col in subpressures.columns:\n        if col not in ['Reduced pressure', 'State pressure', 'State', 'Equivalence']:\n            subpressures = subpressures.drop(columns=[col])\n\n    subpressures = subpressures.reset_index(drop=True)\n\n    def assign_multiplier(equivalence):\n        if equivalence &lt;= 1:\n            return equivalence\n        elif equivalence == 2:\n            return 0\n        elif equivalence == 3:\n            return 0\n        else:\n            return 0\n\n    subpressures['Multiplier'] = subpressures['Equivalence'].apply(assign_multiplier)\n\n    return subpressures\n</code></pre>"},{"location":"modules/utilities/","title":"<code>utilities</code>","text":"<p>Copyright (c) 2024 Baltic Marine Environment Protection Commission</p> <p>LICENSE available under  local: 'SOM/protect_baltic/LICENSE' url: 'https://github.com/helcomsecretariat/SOM/blob/main/protect_baltic/LICENCE'</p> <p>Small utility methods</p>"},{"location":"modules/utilities/#src.utilities.Timer","title":"<code>Timer</code>","text":"<p>Simple timer for determining execution time</p> Source code in <code>src/utilities.py</code> <pre><code>class Timer:\n    \"\"\"\n    Simple timer for determining execution time\n    \"\"\"\n    def __init__(self) -&gt; None:\n        self.start = time.perf_counter()\n    def time_passed(self) -&gt; int:\n        \"\"\"Return time passed since start, in seconds\"\"\"\n        return time.perf_counter() - self.start\n    def get_time(self) -&gt; tuple:\n        \"\"\"Returns durations in hours, minutes, seconds as a named tuple\"\"\"\n        duration = self.time_passed()\n        hours = duration // 3600\n        minutes = (duration % 3600) // 60\n        seconds = duration % 60\n        PassedTime = namedtuple('PassedTime', 'hours minutes seconds')\n        return PassedTime(hours, minutes, seconds)\n    def get_duration(self) -&gt; str:\n        \"\"\"Returns a string of duration in hours, minutes, seconds\"\"\"\n        t = self.get_time()\n        return '%d h %d min %d sec' % (t.hours, t.minutes, t.seconds)\n    def get_hhmmss(self) -&gt; str:\n        \"\"\"Returns a string of duration in hh:mm:ss format\"\"\"\n        t = self.get_time()\n        return '[' + ':'.join(f'{int(value):02d}' for value in [t.hours, t.minutes, t.seconds]) + ']'\n    def reset(self) -&gt; None:\n        \"\"\"Reset timer\"\"\"\n        self.start = time.perf_counter()\n</code></pre>"},{"location":"modules/utilities/#src.utilities.Timer.get_duration","title":"<code>get_duration()</code>","text":"<p>Returns a string of duration in hours, minutes, seconds</p> Source code in <code>src/utilities.py</code> <pre><code>def get_duration(self) -&gt; str:\n    \"\"\"Returns a string of duration in hours, minutes, seconds\"\"\"\n    t = self.get_time()\n    return '%d h %d min %d sec' % (t.hours, t.minutes, t.seconds)\n</code></pre>"},{"location":"modules/utilities/#src.utilities.Timer.get_hhmmss","title":"<code>get_hhmmss()</code>","text":"<p>Returns a string of duration in hh:mm:ss format</p> Source code in <code>src/utilities.py</code> <pre><code>def get_hhmmss(self) -&gt; str:\n    \"\"\"Returns a string of duration in hh:mm:ss format\"\"\"\n    t = self.get_time()\n    return '[' + ':'.join(f'{int(value):02d}' for value in [t.hours, t.minutes, t.seconds]) + ']'\n</code></pre>"},{"location":"modules/utilities/#src.utilities.Timer.get_time","title":"<code>get_time()</code>","text":"<p>Returns durations in hours, minutes, seconds as a named tuple</p> Source code in <code>src/utilities.py</code> <pre><code>def get_time(self) -&gt; tuple:\n    \"\"\"Returns durations in hours, minutes, seconds as a named tuple\"\"\"\n    duration = self.time_passed()\n    hours = duration // 3600\n    minutes = (duration % 3600) // 60\n    seconds = duration % 60\n    PassedTime = namedtuple('PassedTime', 'hours minutes seconds')\n    return PassedTime(hours, minutes, seconds)\n</code></pre>"},{"location":"modules/utilities/#src.utilities.Timer.reset","title":"<code>reset()</code>","text":"<p>Reset timer</p> Source code in <code>src/utilities.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset timer\"\"\"\n    self.start = time.perf_counter()\n</code></pre>"},{"location":"modules/utilities/#src.utilities.Timer.time_passed","title":"<code>time_passed()</code>","text":"<p>Return time passed since start, in seconds</p> Source code in <code>src/utilities.py</code> <pre><code>def time_passed(self) -&gt; int:\n    \"\"\"Return time passed since start, in seconds\"\"\"\n    return time.perf_counter() - self.start\n</code></pre>"},{"location":"modules/utilities/#src.utilities.exception_traceback","title":"<code>exception_traceback(e, file=None)</code>","text":"<p>Format exception traceback and print</p> Source code in <code>src/utilities.py</code> <pre><code>def exception_traceback(e: Exception, file = None):\n    \"\"\"\n    Format exception traceback and print\n    \"\"\"\n    tb = traceback.format_exception(type(e), e, e.__traceback__)\n    print(''.join(tb), file=file)\n</code></pre>"},{"location":"modules/utilities/#src.utilities.fail_with_message","title":"<code>fail_with_message(m=None, e=None, file=None, do_not_exit=False)</code>","text":"<p>Prints the given exception traceback along with given message, and exits.</p> Source code in <code>src/utilities.py</code> <pre><code>def fail_with_message(m: str = None, e: Exception = None, file = None, do_not_exit: bool = False):\n    \"\"\"\n    Prints the given exception traceback along with given message, and exits.\n    \"\"\"\n    if e is not None:\n        exception_traceback(e, file)\n    if m is not None:\n        print(m, file=file)\n    print('Terminating.', file=file)\n    if not do_not_exit:\n        exit()\n</code></pre>"},{"location":"modules/utilities/#src.utilities.get_dist_from_picks","title":"<code>get_dist_from_picks(picks)</code>","text":"<p>Takes an array of picks and returns the probability distribution for each percentage unit. Picks need to be fractions in [0, 1].</p> Source code in <code>src/utilities.py</code> <pre><code>def get_dist_from_picks(picks: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Takes an array of picks and returns the probability distribution for each percentage unit. Picks need to be fractions in [0, 1].\n    \"\"\"\n    picks = np.round(picks, decimals=2)\n    unique, count = np.unique(picks, return_counts=True)\n    dist = np.zeros(shape=101)  # probability distribution, each element represents a percentage from 0 - 100 %\n    # for each percentage, set its value to its frequency in the picks\n    for i in range(dist.size):\n        for k in range(unique.size):\n            if i / 100.0 == unique[k]:\n                dist[i] = count[k]\n    dist = dist / dist.sum()    # normalize frequencies to sum up to 1\n    return dist\n</code></pre>"},{"location":"modules/utilities/#src.utilities.get_pick","title":"<code>get_pick(dist)</code>","text":"<p>Makes a random pick within [0, 1] weighted by the given discrete distribution.</p> Source code in <code>src/utilities.py</code> <pre><code>def get_pick(dist: np.ndarray) -&gt; float:\n    \"\"\"\n    Makes a random pick within [0, 1] weighted by the given discrete distribution.\n    \"\"\"\n    if dist is not None:\n        step = 1 / (dist.size - 1)\n        a = np.arange(0, 1 + step, step)\n        pick = np.random.choice(a, p=dist)\n        return pick\n    else:\n        return np.nan\n</code></pre>"},{"location":"modules/utilities/#src.utilities.get_prob_dist","title":"<code>get_prob_dist(expecteds, lower_boundaries, upper_boundaries, weights)</code>","text":"<p>Returns a cumulative probability distribution. All arguments should be 1D arrays with percentage as unit.</p> Source code in <code>src/utilities.py</code> <pre><code>def get_prob_dist(expecteds: np.ndarray, \n                  lower_boundaries: np.ndarray, \n                  upper_boundaries: np.ndarray, \n                  weights: np.ndarray) -&gt; np.ndarray:\n    '''\n    Returns a cumulative probability distribution. All arguments should be 1D arrays with percentage as unit.\n    '''\n    # verify that all arrays have the same size\n    assert expecteds.size == lower_boundaries.size == upper_boundaries.size == weights.size\n\n    #\n    # TODO: remove uncomment in future to not accept faulty data\n    # for now, sort arrays to have values in correct order\n    #\n    # # verify that all lower boundaries are lower than the upper boundaries\n    # assert np.sum(lower_boundaries &gt; upper_boundaries) == 0\n    # # verify that most likely values are between lower and upper boundaries\n    # assert np.sum((expecteds &lt; lower_boundaries) &amp; (expecteds &gt; upper_boundaries)) == 0\n    arr = np.full((len(expecteds), 3), np.nan)\n    arr[:, 0] = lower_boundaries\n    arr[:, 1] = expecteds\n    arr[:, 2] = upper_boundaries\n    arr = np.array([np.sort(row) for row in arr])\n    lower_boundaries = arr[:, 0]\n    expecteds = arr[:, 1]\n    upper_boundaries = arr[:, 2]\n\n    # select values that are not nan, bool matrix\n    non_nan = ~np.isnan(expecteds) &amp; ~np.isnan(lower_boundaries) &amp; ~np.isnan(upper_boundaries)\n    # multiply those values with weights, True = 1 and False = 0\n    weights_non_nan = (non_nan * weights)\n\n    # create a PERT distribution for each expert\n    # from each distribution, draw a large number of picks\n    # pool the picks together\n    number_of_picks = 5000\n    picks = []\n    for i in range(len(expecteds)):\n        peak = expecteds[i]\n        low = lower_boundaries[i]\n        high = upper_boundaries[i]\n        w = weights_non_nan[i]\n        if ~non_nan[i]: # note the tilde ~ to check for nan value\n            continue    # skip if any value is nan\n        dist = pert_dist(peak, low, high, w * number_of_picks)\n        picks += dist.tolist()\n\n    # return nan if no distributions (= no expert answers)\n    if len(picks) == 0:\n        return np.nan\n\n    # create final probability distribution\n    picks = np.array(picks) / 100.0   # convert percentages to fractions\n    prob_dist = get_dist_from_picks(picks)\n    cum_dist = np.cumsum(prob_dist) # cumulative distribution, not used\n\n    return prob_dist\n</code></pre>"},{"location":"modules/utilities/#src.utilities.pert_dist","title":"<code>pert_dist(peak, low, high, size)</code>","text":"<p>Returns a set of random picks from a PERT distribution.</p> Source code in <code>src/utilities.py</code> <pre><code>def pert_dist(peak, low, high, size) -&gt; np.ndarray:\n    '''\n    Returns a set of random picks from a PERT distribution.\n    '''\n    # weight, controls probability of edge values (higher -&gt; more emphasis on most likely, lower -&gt; extreme values more probable)\n    # 4 is standard used in unmodified PERT distributions\n    gamma = 4\n    # calculate expected value\n    # mu = ((low + gamma) * (peak + high)) / (gamma + 2)\n    if low == high and low == peak:\n        return np.full(int(size), peak)\n    r = high - low\n    alpha = 1 + gamma * (peak - low) / r\n    beta = 1 + gamma * (high - peak) / r\n    return low + np.random.default_rng().beta(alpha, beta, size=int(size)) * r\n</code></pre>"},{"location":"modules/utilities/#src.utilities.plot_dist","title":"<code>plot_dist(dist)</code>","text":"<p>Plot the given distribution</p> Source code in <code>src/utilities.py</code> <pre><code>def plot_dist(dist):\n    \"\"\"\n    Plot the given distribution\n    \"\"\"\n    # plot distribution\n    y_vals = dist\n    step = 1 / y_vals.size\n    x_vals = np.arange(0, 1, step)\n    plt.plot(x_vals, y_vals)\n    # verify that get_pick works\n    picks = np.array([get_pick(dist) for i in range(5000)])\n    y_vals = get_dist_from_picks(picks)\n    step = 1 / y_vals.size\n    x_vals = np.arange(0, 1, step)\n    plt.plot(x_vals, y_vals)\n    plt.show()\n</code></pre>"}]}